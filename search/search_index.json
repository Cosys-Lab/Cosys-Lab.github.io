{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Cosys-AirSim AirSim is a simulator for drones, cars and more, built on Unreal Engine . It is open-source, cross platform, and supports hardware-in-loop with popular flight controllers such as PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Our goal is to develop AirSim as a platform for AI research to experiment with deep learning, computer vision and reinforcement learning algorithms for autonomous vehicles. For this purpose, AirSim also exposes APIs to retrieve data and control vehicles in a platform independent way. Cosys-Lab Modifications Cosys-Lab made extensive modifications to the AirSim platform to support multiple projects and research goals. Please contact a Cosys-Lab researcher to get more in depth information on our work or if you wish to colaborate. The original AirSim MIT license applies to all native AirSim source files. Please note that we use that same MIT license as which applies to all changes made by Cosys-Lab in case you plan to do anything within this repository. Do note that this repository is provided as is, will not be actively updated and comes without warranty or support. Associated publications Cosys-AirSim: A Real-Time Simulation Framework Expanded for Complex Industrial Applications @inproceedings{jansen2023cosysairsim, title={Cosys-AirSim: A Real-Time Simulation Framework Expanded for Complex Industrial Applications}, author={Jansen, Wouter and Verreycken, Erik and Schenck, Anthony and Blanquart, Jean-Edouard and Verhulst, Connor and Huebel, Nico and Steckel, Jan}, booktitle={Accepted at Annual Modeling and Simulation Conference, ANNSIM 202}, year={2023}, eprint={2303.13381}, archivePrefix={arXiv}, primaryClass={cs.RO} } Physical LiDAR Simulation in Real-Time Engine @inproceedings{lidarsim2022jansen, author={Jansen, Wouter and Huebel, Nico and Steckel, Jan}, booktitle={2022 IEEE Sensors}, title={Physical LiDAR Simulation in Real-Time Engine}, year={2022}, volume={}, number={}, pages={1-4}, doi={10.1109/SENSORS52175.2022.9967197}} } Simulation of Pulse-Echo Radar for Vehicle Control and SLAM @Article{echosim2021schouten, author={Schouten, Girmi and Jansen, Wouter and Steckel, Jan}, title={Simulation of Pulse-Echo Radar for Vehicle Control and SLAM}, JOURNAL={Sensors}, volume={21}, year={2021}, number={2}, article-number={523}, doi={10.3390/s21020523} } Main Modifications Updated the camera, Echo and (GPU)LiDAR sensors to be able to uncouple from the vehicle and be placed as external world sensors. Added more camera sensor distortion features such as chromatic aberration, motion blur and lens distortion. Updated Python ROS implementation with completely new implementation and feature set. C++ version is deprecated. Added Echo sensor type for simulation of sensors like sonar and radar. Added Instance Segmentation . Added experimental and undocumented WiFi and UWB sensor types. Added GPU LIDAR sensor type : Uses GPU acceleration to simulate a LiDAR sensor. Can support much higher point density then normal LiDAR and behaves more authentic and has realistic intensity generation. Updated ComputerVision mode : Now has full API and Simulation just like other vehicle types. It mostly means it can now have sensors attached (outside of IMU). Improved handling and camera operation. Updated LIDAR sensor type : Fixed not tracing correctly, added ground truth (point labels) generation, added range-noise generation. Improved API pointcloud delivery to be full scan instead of being frame-rate dependent and partial. Added option to hot-reload plugin through Unreal Editor (faster development). Added skid steering SimMode and vehicle type based on NVIDIA tank PhysX vehicle model. ClearPath Husky and Pioneer P3DX implemented as vehicle types using this new vehicle model. Added BoxCar vehicle model to the Car SimMode to have a smaller vehicle to use in indoor spaces. Updated standard camera render resolution target to 960x540. Updated standard uncompressed image format to RGB instead of BGR (this breaks OpenCV support but fixes ROS images). Added option to Cameras, EchoSensor and GPULiDAR to ignore certain objects with the MarkedIgnore Unreal tag and enabling the \"IgnoreMarked\" setting in the settings file . Updated Unreal to 4.24 (custom fork needed for instance segmentation: https://github.com/WouterJansen/UnrealEngine ) Dropped support for Unity Environments. How to Get It This branch uses a custom Unreal Engine version! Please read the documentation carefully. Windows Build it Linux Build it How to Use It Documentation View our detailed documentation on all aspects of AirSim. Manual drive If you have remote control (RC) as shown below, you can manually control the drone in the simulator. For cars, you can use arrow keys to drive manually. More details Programmatic control AirSim exposes APIs so you can interact with the vehicle in the simulation programmatically. You can use these APIs to retrieve images, get state, control the vehicle and so on. The APIs are exposed through the RPC, and are accessible via a variety of languages, including C++, Python, C# and Java. These APIs are also available as part of a separate, independent cross-platform library, so you can deploy them on a companion computer on your vehicle. This way you can write and test your code in the simulator, and later execute it on the real vehicles. Transfer learning and related research is one of our focus areas. Note that you can use SimMode setting to specify the default vehicle or the new ComputerVision mode so you don't get prompted each time you start AirSim. More details Gathering training data There are two ways you can generate training data from AirSim for deep learning. The easiest way is to simply press the record button in the lower right corner. This will start writing pose and images for each frame. The data logging code is pretty simple and you can modify it to your heart's content. A better way to generate training data exactly the way you want is by accessing the APIs. This allows you to be in full control of how, what, where and when you want to log data. Computer Vision mode Yet another way to use AirSim is the so-called \"Computer Vision\" mode. In this mode, you don't have vehicles or physics. You can use the keyboard to move around the scene, or use APIs to position available cameras in any arbitrary pose, and collect images such as depth, disparity, surface normals or object segmentation. More details Weather Effects Press F10 to see various options available for weather effects. You can also control the weather using APIs . Press F1 to see other options available. Tutorials Video - Setting up AirSim with Pixhawk Tutorial by Chris Lovett Video - Using AirSim with Pixhawk Tutorial by Chris Lovett Video - Using off-the-self environments with AirSim by Jim Piavis Reinforcement Learning with AirSim by Ashish Kapoor The Autonomous Driving Cookbook by Microsoft Deep Learning and Robotics Garage Chapter Using TensorFlow for simple collision avoidance by Simon Levy and WLU team Original AirSim Publication More technical details are available in AirSim paper (FSR 2017 Conference) . Please cite this as: @inproceedings{airsim2017fsr, author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor}, title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles}, year = {2017}, booktitle = {Field and Service Robotics}, eprint = {arXiv:1705.05065}, url = {https://arxiv.org/abs/1705.05065} } Licensing This original AirSim project is released under the MIT License. Please review the License file for more details. All changes made by Cosys-Lab are released under the same MIT License. Please review the License file for more details.","title":"Home"},{"location":"#welcome-to-cosys-airsim","text":"AirSim is a simulator for drones, cars and more, built on Unreal Engine . It is open-source, cross platform, and supports hardware-in-loop with popular flight controllers such as PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Our goal is to develop AirSim as a platform for AI research to experiment with deep learning, computer vision and reinforcement learning algorithms for autonomous vehicles. For this purpose, AirSim also exposes APIs to retrieve data and control vehicles in a platform independent way.","title":"Welcome to Cosys-AirSim"},{"location":"#cosys-lab-modifications","text":"Cosys-Lab made extensive modifications to the AirSim platform to support multiple projects and research goals. Please contact a Cosys-Lab researcher to get more in depth information on our work or if you wish to colaborate. The original AirSim MIT license applies to all native AirSim source files. Please note that we use that same MIT license as which applies to all changes made by Cosys-Lab in case you plan to do anything within this repository. Do note that this repository is provided as is, will not be actively updated and comes without warranty or support.","title":"Cosys-Lab Modifications"},{"location":"#associated-publications","text":"Cosys-AirSim: A Real-Time Simulation Framework Expanded for Complex Industrial Applications @inproceedings{jansen2023cosysairsim, title={Cosys-AirSim: A Real-Time Simulation Framework Expanded for Complex Industrial Applications}, author={Jansen, Wouter and Verreycken, Erik and Schenck, Anthony and Blanquart, Jean-Edouard and Verhulst, Connor and Huebel, Nico and Steckel, Jan}, booktitle={Accepted at Annual Modeling and Simulation Conference, ANNSIM 202}, year={2023}, eprint={2303.13381}, archivePrefix={arXiv}, primaryClass={cs.RO} } Physical LiDAR Simulation in Real-Time Engine @inproceedings{lidarsim2022jansen, author={Jansen, Wouter and Huebel, Nico and Steckel, Jan}, booktitle={2022 IEEE Sensors}, title={Physical LiDAR Simulation in Real-Time Engine}, year={2022}, volume={}, number={}, pages={1-4}, doi={10.1109/SENSORS52175.2022.9967197}} } Simulation of Pulse-Echo Radar for Vehicle Control and SLAM @Article{echosim2021schouten, author={Schouten, Girmi and Jansen, Wouter and Steckel, Jan}, title={Simulation of Pulse-Echo Radar for Vehicle Control and SLAM}, JOURNAL={Sensors}, volume={21}, year={2021}, number={2}, article-number={523}, doi={10.3390/s21020523} }","title":"Associated publications"},{"location":"#main-modifications","text":"Updated the camera, Echo and (GPU)LiDAR sensors to be able to uncouple from the vehicle and be placed as external world sensors. Added more camera sensor distortion features such as chromatic aberration, motion blur and lens distortion. Updated Python ROS implementation with completely new implementation and feature set. C++ version is deprecated. Added Echo sensor type for simulation of sensors like sonar and radar. Added Instance Segmentation . Added experimental and undocumented WiFi and UWB sensor types. Added GPU LIDAR sensor type : Uses GPU acceleration to simulate a LiDAR sensor. Can support much higher point density then normal LiDAR and behaves more authentic and has realistic intensity generation. Updated ComputerVision mode : Now has full API and Simulation just like other vehicle types. It mostly means it can now have sensors attached (outside of IMU). Improved handling and camera operation. Updated LIDAR sensor type : Fixed not tracing correctly, added ground truth (point labels) generation, added range-noise generation. Improved API pointcloud delivery to be full scan instead of being frame-rate dependent and partial. Added option to hot-reload plugin through Unreal Editor (faster development). Added skid steering SimMode and vehicle type based on NVIDIA tank PhysX vehicle model. ClearPath Husky and Pioneer P3DX implemented as vehicle types using this new vehicle model. Added BoxCar vehicle model to the Car SimMode to have a smaller vehicle to use in indoor spaces. Updated standard camera render resolution target to 960x540. Updated standard uncompressed image format to RGB instead of BGR (this breaks OpenCV support but fixes ROS images). Added option to Cameras, EchoSensor and GPULiDAR to ignore certain objects with the MarkedIgnore Unreal tag and enabling the \"IgnoreMarked\" setting in the settings file . Updated Unreal to 4.24 (custom fork needed for instance segmentation: https://github.com/WouterJansen/UnrealEngine ) Dropped support for Unity Environments.","title":"Main Modifications"},{"location":"#how-to-get-it","text":"This branch uses a custom Unreal Engine version! Please read the documentation carefully.","title":"How to Get It"},{"location":"#windows","text":"Build it","title":"Windows"},{"location":"#linux","text":"Build it","title":"Linux"},{"location":"#how-to-use-it","text":"","title":"How to Use It"},{"location":"#documentation","text":"View our detailed documentation on all aspects of AirSim.","title":"Documentation"},{"location":"#manual-drive","text":"If you have remote control (RC) as shown below, you can manually control the drone in the simulator. For cars, you can use arrow keys to drive manually. More details","title":"Manual drive"},{"location":"#programmatic-control","text":"AirSim exposes APIs so you can interact with the vehicle in the simulation programmatically. You can use these APIs to retrieve images, get state, control the vehicle and so on. The APIs are exposed through the RPC, and are accessible via a variety of languages, including C++, Python, C# and Java. These APIs are also available as part of a separate, independent cross-platform library, so you can deploy them on a companion computer on your vehicle. This way you can write and test your code in the simulator, and later execute it on the real vehicles. Transfer learning and related research is one of our focus areas. Note that you can use SimMode setting to specify the default vehicle or the new ComputerVision mode so you don't get prompted each time you start AirSim. More details","title":"Programmatic control"},{"location":"#gathering-training-data","text":"There are two ways you can generate training data from AirSim for deep learning. The easiest way is to simply press the record button in the lower right corner. This will start writing pose and images for each frame. The data logging code is pretty simple and you can modify it to your heart's content. A better way to generate training data exactly the way you want is by accessing the APIs. This allows you to be in full control of how, what, where and when you want to log data.","title":"Gathering training data"},{"location":"#computer-vision-mode","text":"Yet another way to use AirSim is the so-called \"Computer Vision\" mode. In this mode, you don't have vehicles or physics. You can use the keyboard to move around the scene, or use APIs to position available cameras in any arbitrary pose, and collect images such as depth, disparity, surface normals or object segmentation. More details","title":"Computer Vision mode"},{"location":"#weather-effects","text":"Press F10 to see various options available for weather effects. You can also control the weather using APIs . Press F1 to see other options available.","title":"Weather Effects"},{"location":"#tutorials","text":"Video - Setting up AirSim with Pixhawk Tutorial by Chris Lovett Video - Using AirSim with Pixhawk Tutorial by Chris Lovett Video - Using off-the-self environments with AirSim by Jim Piavis Reinforcement Learning with AirSim by Ashish Kapoor The Autonomous Driving Cookbook by Microsoft Deep Learning and Robotics Garage Chapter Using TensorFlow for simple collision avoidance by Simon Levy and WLU team","title":"Tutorials"},{"location":"#original-airsim-publication","text":"More technical details are available in AirSim paper (FSR 2017 Conference) . Please cite this as: @inproceedings{airsim2017fsr, author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor}, title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles}, year = {2017}, booktitle = {Field and Service Robotics}, eprint = {arXiv:1705.05065}, url = {https://arxiv.org/abs/1705.05065} }","title":"Original AirSim Publication"},{"location":"#licensing","text":"This original AirSim project is released under the MIT License. Please review the License file for more details. All changes made by Cosys-Lab are released under the same MIT License. Please review the License file for more details.","title":"Licensing"},{"location":"InfraredCamera/","text":"This is a tutorial for generating simulated thermal infrared (IR) images using AirSim and the AirSim Africa environment. Pre-compiled Africa Environment can be downloaded from the Releases tab of this Github repo: Windows Pre-compiled binary To generate your own data, you may use two python files: create_ir_segmentation_map.py and capture_ir_segmentation.py . create_ir_segmentation_map.py uses temperature, emissivity, and camera response information to estimate the thermal digital count that could be expected for the objects in the environment, and then reassigns the segmentation IDs in AirSim to match these digital counts. It should be run before starting to capture thermal IR data. Otherwise, digital counts in the IR images will be incorrect. The camera response, temperature, and emissivity data are all included for the Africa environment. capture_ir_segmentation.py is run after the segmentation IDs have been reassigned. It tracks objects of interest and records the infrared and scene images from the multirotor. It uses Computer Vision mode. Finally, the details about how temperatures were estimated for plants and animals in the Africa environment, etc. can be found in this paper: @inproceedings{bondi2018airsim, title={AirSim-W: A Simulation Environment for Wildlife Conservation with UAVs}, author={Bondi, Elizabeth and Dey, Debadeepta and Kapoor, Ashish and Piavis, Jim and Shah, Shital and Fang, Fei and Dilkina, Bistra and Hannaford, Robert and Iyer, Arvind and Joppa, Lucas and others}, booktitle={Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies}, pages={40}, year={2018}, organization={ACM} } nb","title":"Infrared Camera"},{"location":"ROS_on_Windows_with_WSL/","text":"Using ROS on Windows with WSL In Windows command prompt as admin: wsl --install reboot -Ubuntu terminal will install (default Ubuntu 20.04), choose a username and password -Optional: Run this command to never prompt the current user for a password when that user uses sudo echo \"$USER ALL=(ALL:ALL) NOPASSWD: ALL\" | sudo tee \"/etc/sudoers.d/dont-prompt-$USER-for-sudo-password\" Install ROS (for example noetic) in the ubuntu terminal sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt install curl # if you haven't already installed curl curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - sudo apt update sudo apt install ros-noetic-desktop-full echo \"source /opt/ros/noetic/setup.bash\" >> ~/.bashrc source ~/.bashrc sudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential sudo apt install python3-rosdep sudo rosdep init rosdep update Install PIP sudo apt install python3-pip Install msgpack-rpc-python pip install msgpack-rpc-python API connection For the API to work you need to insert your wsl IP address every time you reboot by doing the following: In windows command prompt: ipconfig Scroll down and for \"Ethernet adapter vEthernet (WSL)\" copy the IPv4 Address now open file: AirsimUnreal\\airsim\\PythonClient\\airsim\\client.py In line 17 paste the copied IPv4 Address inside ip= \"...\" A similar procedure is needed to use the launch files, replace the IP address in these launch files and for \"port\" put default=\"41451\" Example: DroneAPI: in Ubuntu (WSL) terminal: cd /mnt/c (to go to c drive for example) For drone API for example to takeoff: cd /mnt/c/Users/{USERNAME}/AirsimUnreal/airsim/PythonClient/multirotor python3 ./takeoff.py","title":"Using ROS on Windows with WSL"},{"location":"ROS_on_Windows_with_WSL/#using-ros-on-windows-with-wsl","text":"In Windows command prompt as admin: wsl --install reboot -Ubuntu terminal will install (default Ubuntu 20.04), choose a username and password -Optional: Run this command to never prompt the current user for a password when that user uses sudo echo \"$USER ALL=(ALL:ALL) NOPASSWD: ALL\" | sudo tee \"/etc/sudoers.d/dont-prompt-$USER-for-sudo-password\"","title":"Using ROS on Windows with WSL"},{"location":"ROS_on_Windows_with_WSL/#install-ros-for-example-noetic-in-the-ubuntu-terminal","text":"sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt install curl # if you haven't already installed curl curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - sudo apt update sudo apt install ros-noetic-desktop-full echo \"source /opt/ros/noetic/setup.bash\" >> ~/.bashrc source ~/.bashrc sudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential sudo apt install python3-rosdep sudo rosdep init rosdep update","title":"Install ROS (for example noetic) in the ubuntu terminal"},{"location":"ROS_on_Windows_with_WSL/#install-pip","text":"sudo apt install python3-pip","title":"Install PIP"},{"location":"ROS_on_Windows_with_WSL/#install-msgpack-rpc-python","text":"pip install msgpack-rpc-python","title":"Install msgpack-rpc-python"},{"location":"ROS_on_Windows_with_WSL/#api-connection","text":"For the API to work you need to insert your wsl IP address every time you reboot by doing the following: In windows command prompt: ipconfig Scroll down and for \"Ethernet adapter vEthernet (WSL)\" copy the IPv4 Address now open file: AirsimUnreal\\airsim\\PythonClient\\airsim\\client.py In line 17 paste the copied IPv4 Address inside ip= \"...\" A similar procedure is needed to use the launch files, replace the IP address in these launch files and for \"port\" put default=\"41451\"","title":"API connection"},{"location":"ROS_on_Windows_with_WSL/#example-droneapi-in-ubuntu-wsl-terminal","text":"cd /mnt/c (to go to c drive for example) For drone API for example to takeoff: cd /mnt/c/Users/{USERNAME}/AirsimUnreal/airsim/PythonClient/multirotor python3 ./takeoff.py","title":"Example: DroneAPI: in Ubuntu (WSL) terminal:"},{"location":"apis/","text":"AirSim APIs Introduction AirSim exposes APIs so you can interact with vehicle in the simulation programmatically. You can use these APIs to retrieve images, get state, control the vehicle and so on. Python Quickstart If you want to use Python to call AirSim APIs, we recommend using Anaconda with Python 3.5 or later versions however some code may also work with Python 2.7. First install this package: pip install msgpack-rpc-python Once you can run AirSim, choose Car as vehicle and then navigate to PythonClient\\car\\ folder and run: python hello_car.py If you are using Visual Studio then just open AirSim.sln, set PythonClient as startup project and choose car\\hello_car.py as your startup script. Hello Car Here's how to use AirSim APIs using Python to control simulated car (see also C++ example ): # ready to run example: PythonClient/car/hello_car.py import airsim import time # connect to the AirSim simulator client = airsim.CarClient() client.confirmConnection() client.enableApiControl(True) car_controls = airsim.CarControls() while True: # get state of the car car_state = client.getCarState() print(\"Speed %d, Gear %d\" % (car_state.speed, car_state.gear)) # set the controls for car car_controls.throttle = 1 car_controls.steering = 1 client.setCarControls(car_controls) # let car drive a bit time.sleep(1) # get camera images from the car responses = client.simGetImages([ airsim.ImageRequest(0, airsim.ImageType.DepthVis), airsim.ImageRequest(1, airsim.ImageType.DepthPlanner, True)]) print('Retrieved images: %d', len(responses)) # do something with images for response in responses: if response.pixels_as_float: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_float))) airsim.write_pfm('py1.pfm', airsim.get_pfm_array(response)) else: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_uint8))) airsim.write_file('py1.png', response.image_data_uint8) Hello Drone Here's how to use AirSim APIs using Python to control simulated quadrotor (see also C++ example ): # ready to run example: PythonClient/multirotor/hello_drone.py import airsim # connect to the AirSim simulator client = airsim.MultirotorClient() client.confirmConnection() client.enableApiControl(True) client.armDisarm(True) # Async methods returns Future. Call join() to wait for task to complete. client.takeoffAsync().join() client.moveToPositionAsync(-10, 10, -10, 5).join() # take images responses = client.simGetImages([ airsim.ImageRequest(\"0\", airsim.ImageType.DepthVis), airsim.ImageRequest(\"1\", airsim.ImageType.DepthPlanner, True)]) print('Retrieved images: %d', len(responses)) # do something with the images for response in responses: if response.pixels_as_float: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_float))) airsim.write_pfm(os.path.normpath('/temp/py1.pfm'), airsim.getPfmArray(response)) else: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_uint8))) airsim.write_file(os.path.normpath('/temp/py1.png'), response.image_data_uint8) Common APIs reset : This resets the vehicle to its original starting state. Note that you must call enableApiControl and armDisarm again after the call to reset . confirmConnection : Checks state of connection every 1 sec and reports it in Console so user can see the progress for connection. enableApiControl : For safety reasons, by default API control for autonomous vehicle is not enabled and human operator has full control (usually via RC or joystick in simulator). The client must make this call to request control via API. It is likely that human operator of vehicle might have disallowed API control which would mean that enableApiControl has no effect. This can be checked by isApiControlEnabled . isApiControlEnabled : Returns true if API control is established. If false (which is default) then API calls would be ignored. After a successful call to enableApiControl , the isApiControlEnabled should return true. ping : If connection is established then this call will return true otherwise it will be blocked until timeout. simPrintLogMessage : Prints the specified message in the simulator's window. If message_param is also supplied then its printed next to the message and in that case if this API is called with same message value but different message_param again then previous line is overwritten with new line (instead of API creating new line on display). For example, simPrintLogMessage(\"Iteration: \", to_string(i)) keeps updating same line on display when API is called with different values of i. The valid values of severity parameter is 0 to 3 inclusive that corresponds to different colors. simGetObjectPose(ned=true) , simSetObjectPose : Gets and sets the pose of specified object in Unreal environment. Here the object means \"actor\" in Unreal terminology. They are searched by tag as well as name. Please note that the names shown in UE Editor are auto-generated in each run and are not permanent. So if you want to refer to actor by name, you must change its auto-generated name in UE Editor. Alternatively you can add a tag to actor which can be done by clicking on that actor in Unreal Editor and then going to Tags property , click \"+\" sign and add some string value. If multiple actors have same tag then the first match is returned. If no matches are found then NaN pose is returned. The returned pose is in NED coordinates in SI units with its origin at Player Start by default or in Unreal NED frame if the ned boolean argument is set to talse . For simSetObjectPose , the specified actor must have Mobility set to Movable or otherwise you will get undefined behavior. The simSetObjectPose has parameter teleport which means object is moved through other objects in its way and it returns true if move was successful simListSceneObjects : Provides a list of all objects in the environment. You can also use regular expression to filter specific objects by name. For example, the code below sets all meshes which have names starting with \"wall\" you can use simListSceneObjects(\"wall[\\w]*\") . Image/Computer Vision/Instance segmentation APIs AirSim offers comprehensive images APIs to retrieve synchronized images from multiple cameras along with ground truth including depth, disparity, surface normals and vision. You can set the resolution, FOV, motion blur etc parameters in settings.json . There is also API for detecting collision state. See also complete code that generates specified number of stereo images and ground truth depth with normalization to camera plan, computation of disparity image and saving it to pfm format . Furthermore, the Instance Segmentation system can also be manipulated through the API. More on image APIs, Computer Vision mode and instance segmentation configuration . Pause and Continue APIs AirSim allows to pause and continue the simulation through pause(is_paused) API. To pause the simulation call pause(True) and to continue the simulation call pause(False) . You may have scenario, especially while using reinforcement learning, to run the simulation for specified amount of time and then automatically pause. While simulation is paused, you may then do some expensive computation, send a new command and then again run the simulation for specified amount of time. This can be achieved by API continueForTime(seconds) . This API runs the simulation for the specified number of seconds and then pauses the simulation. For example usage, please see pause_continue_car.py and pause_continue_drone.py . Collision API The collision information can be obtained using simGetCollisionInfo API. This call returns a struct that has information not only whether collision occurred but also collision position, surface normal, penetration depth and so on. Time of Day API AirSim assumes there exist sky sphere of class EngineSky/BP_Sky_Sphere in your environment with ADirectionalLight actor . By default, the position of the sun in the scene doesn't move with time. You can use settings to set up latitude, longitude, date and time which AirSim uses to compute the position of sun in the scene. You can also use following API call to set the sun position according to given date time: simSetTimeOfDay(self, is_enabled, start_datetime = \"\", is_start_datetime_dst = False, celestial_clock_speed = 1, update_interval_secs = 60, move_sun = True) The is_enabled parameter must be True to enable time of day effect. If it is False then sun position is reset to its original in the environment. Other parameters are same as in settings . Weather APIs By default all weather effects are disabled. To enable weather effect, first call: simEnableWeather(True) Various weather effects can be enabled by using simSetWeatherParameter method which takes WeatherParameter , for example, client.simSetWeatherParameter(airsim.WeatherParameter.Rain, 0.25); The second parameter value is from 0 to 1. The first parameter provides following options: class WeatherParameter: Rain = 0 Roadwetness = 1 Snow = 2 RoadSnow = 3 MapleLeaf = 4 RoadLeaf = 5 Dust = 6 Fog = 7 Please note that Roadwetness , RoadSnow and RoadLeaf effects requires adding materials to your scene. Please see example code for more details. LiDAR APIs AirSim offers API to retrieve point cloud data from (GPU)LiDAR sensors on vehicles. You can set the number of channels, points per second, horizontal and vertical FOV, etc parameters in settings.json . More on LiDAR APIs and settings , GPULiDAR APIs and settings and sensor settings Multiple Vehicles AirSim supports multiple vehicles and control them through APIs. Please Multiple Vehicles doc. Coordinate System All AirSim API uses NED coordinate system, i.e., +X is North, +Y is East and +Z is Down. All units are in SI system. Please note that this is different from coordinate system used internally by Unreal Engine. In Unreal Engine, +Z is up instead of down and length unit is in centimeters instead of meters. AirSim APIs takes care of the appropriate conversions. The starting point of the vehicle is always coordinates (0, 0, 0) in NED system. Thus when converting from Unreal coordinates to NED, we first subtract the starting offset and then scale by 100 for cm to m conversion. The vehicle is spawned in Unreal environment where the Player Start component is placed. There is a setting called OriginGeopoint in settings.json which assigns geographic longitude, longitude and altitude to the Player Start component. Vehicle Specific APIs APIs for Car Car has followings APIs available: setCarControls : This allows you to set throttle, steering, handbrake and auto or manual gear. getCarState : This retrieves the state information including speed, current gear and 6 kinematics quantities: position, orientation, linear and angular velocity, linear and angular acceleration. All quantities are in NED coordinate system, SI units in world frame except for angular velocity and accelerations which are in body frame. Image APIs . APIs for Multirotor Multirotor can be controlled by specifying angles, velocity vector, destination position or some combination of these. There are corresponding move* APIs for this purpose. When doing position control, we need to use some path following algorithm. By default AirSim uses carrot following algorithm. This is often referred to as \"high level control\" because you just need to specify high level goal and the firmware takes care of the rest. Currently lowest level control available in AirSim is moveByAngleThrottleAsync API. getMultirotorState This API returns the state of the vehicle in one call. The state includes, collision, estimated kinematics (i.e. kinematics computed by fusing sensors), and timestamp (nano seconds since epoch). The kinematics here means 6 quantities: position, orientation, linear and angular velocity, linear and angular acceleration. Please note that simple_slight currently doesn't support state estimator which means estimated and ground truth kinematics values would be same for simple_flight. Estimated kinematics are however available for PX4 except for angular acceleration. All quantities are in NED coordinate system, SI units in world frame except for angular velocity and accelerations which are in body frame. Async methods, duration and max_wait_seconds Many API methods has parameters named duration or max_wait_seconds and they have Async as suffix, for example, takeoffAsync . These methods will return immediately after starting the task in AirSim so that your client code can do something else while that task is being executed. If you want to wait for this task to complete then you can call waitOnLastTask like this: //C++ client.takeoffAsync()->waitOnLastTask(); # Python client.takeoffAsync().join() If you start another command then it automatically cancels the previous task and starts new command. This allows to use pattern where your coded continuously does the sensing, computes a new trajectory to follow and issues that path to vehicle in AirSim. Each newly issued trajectory cancels the previous trajectory allowing your code to continuously do the update as new sensor data arrives. All Async method returns concurrent.futures.Future in Python ( std::future in C++). Please note that these future classes currently do not allow to check status or cancel the task; they only allow to wait for task to complete. AirSim does provide API cancelLastTask , however. drivetrain There are two modes you can fly vehicle: drivetrain parameter is set to airsim.DrivetrainType.ForwardOnly or airsim.DrivetrainType.MaxDegreeOfFreedom . When you specify ForwardOnly, you are saying that vehicle's front should always point in the direction of travel. So if you want drone to take left turn then it would first rotate so front points to left. This mode is useful when you have only front camera and you are operating vehicle using FPV view. This is more or less like travelling in car where you always have front view. The MaxDegreeOfFreedom means you don't care where the front points to. So when you take left turn, you just start going left like crab. Quadrotors can go in any direction regardless of where front points to. The MaxDegreeOfFreedom enables this mode. yaw_mode yaw_mode is a struct YawMode with two fields, yaw_or_rate and is_rate . If is_rate field is True then yaw_or_rate field is interpreted as angular velocity in degrees/sec which means you want vehicle to rotate continuously around its axis at that angular velocity while moving. If is_rate is False then yaw_or_rate is interpreted as angle in degrees which means you want vehicle to rotate to specific angle (i.e. yaw) and keep that angle while moving. You can probably see that when yaw_mode.is_rate == true , the drivetrain parameter shouldn't be set to ForwardOnly because you are contradicting by saying that keep front pointing ahead but also rotate continuously. However if you have yaw_mode.is_rate = false in ForwardOnly mode then you can do some funky stuff. For example, you can have drone do circles and have yaw_or_rate set to 90 so camera is always pointed to center (\"super cool selfie mode\"). In MaxDegreeofFreedom also you can get some funky stuff by setting yaw_mode.is_rate = true and say yaw_mode.yaw_or_rate = 20 . This will cause drone to go in its path while rotating which may allow to do 360 scanning. In most cases, you just don't want yaw to change which you can do by setting yaw rate of 0. The shorthand for this is airsim.YawMode.Zero() (or in C++: YawMode::Zero() ). lookahead and adaptive_lookahead When you ask vehicle to follow a path, AirSim uses \"carrot following\" algorithm. This algorithm operates by looking ahead on path and adjusting its velocity vector. The parameters for this algorithm is specified by lookahead and adaptive_lookahead . For most of the time you want algorithm to auto-decide the values by simply setting lookahead = -1 and adaptive_lookahead = 0 . Using APIs on Real Vehicles We want to be able to run same code that runs in simulation as on real vehicle. This allows you to test your code in simulator and deploy to real vehicle. Generally speaking, APIs therefore shouldn't allow you to do something that cannot be done on real vehicle (for example, getting the ground truth). But, of course, simulator has much more information and it would be useful in applications that may not care about running things on real vehicle. For this reason, we clearly delineate between sim-only APIs by attaching sim prefix, for example, simGetGroundTruthKinematics . This way you can avoid using these simulation-only APIs if you care about running your code on real vehicles. The AirLib is self-contained library that you can put on an offboard computing module such as the Gigabyte barebone Mini PC. This module then can talk to the flight controllers such as PX4 using exact same code and flight controller protocol. The code you write for testing in the simulator remains unchanged. See AirLib on custom drones . Adding New APIs to AirSim Adding new APIs requires modifying the source code. Much of the changes are mechanical and required for various levels of abstractions that AirSim supports. This commit demonstrates how to add a simple API simPrintLogMessage that prints message in simulator window. Some Internals The APIs use msgpack-rpc protocol over TCP/IP through rpclib developed by Tam\u00c3\u00a1s Szelei which allows you to use variety of programming languages including C++, C#, Python, Java etc. When AirSim starts, it opens port 41451 (this can be changed via settings ) and listens for incoming request. The Python or C++ client code connects to this port and sends RPC calls using msgpack serialization format . References and Examples Car Examples Multirotor Examples Computer Vision Examples Move on Path demo showing video of fast multirotor flight through Modular Neighborhood environment Building a Hexacopter Building Point Clouds FAQ Unreal is slowed down dramatically when I run API If you see Unreal getting slowed down dramatically when Unreal Engine window loses focus then go to 'Edit->Editor Preferences' in Unreal Editor, in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. Do I need anything else on Windows? You should install VS2017 with VC++, Windows SDK 8.1 and Python. To use Python APIs you will need Python 3.5 or later (install it using Anaconda). Which version of Python should I use? We recommend Anaconda to get Python tools and libraries. Our code is tested with Python 3.5.3 :: Anaconda 4.4.0. This is important because older version have been known to have problems . I get error on import cv2 You can install OpenCV using: conda install opencv pip install opencv-python","title":"Core APIs"},{"location":"apis/#airsim-apis","text":"","title":"AirSim APIs"},{"location":"apis/#introduction","text":"AirSim exposes APIs so you can interact with vehicle in the simulation programmatically. You can use these APIs to retrieve images, get state, control the vehicle and so on.","title":"Introduction"},{"location":"apis/#python-quickstart","text":"If you want to use Python to call AirSim APIs, we recommend using Anaconda with Python 3.5 or later versions however some code may also work with Python 2.7. First install this package: pip install msgpack-rpc-python Once you can run AirSim, choose Car as vehicle and then navigate to PythonClient\\car\\ folder and run: python hello_car.py If you are using Visual Studio then just open AirSim.sln, set PythonClient as startup project and choose car\\hello_car.py as your startup script.","title":"Python Quickstart"},{"location":"apis/#hello-car","text":"Here's how to use AirSim APIs using Python to control simulated car (see also C++ example ): # ready to run example: PythonClient/car/hello_car.py import airsim import time # connect to the AirSim simulator client = airsim.CarClient() client.confirmConnection() client.enableApiControl(True) car_controls = airsim.CarControls() while True: # get state of the car car_state = client.getCarState() print(\"Speed %d, Gear %d\" % (car_state.speed, car_state.gear)) # set the controls for car car_controls.throttle = 1 car_controls.steering = 1 client.setCarControls(car_controls) # let car drive a bit time.sleep(1) # get camera images from the car responses = client.simGetImages([ airsim.ImageRequest(0, airsim.ImageType.DepthVis), airsim.ImageRequest(1, airsim.ImageType.DepthPlanner, True)]) print('Retrieved images: %d', len(responses)) # do something with images for response in responses: if response.pixels_as_float: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_float))) airsim.write_pfm('py1.pfm', airsim.get_pfm_array(response)) else: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_uint8))) airsim.write_file('py1.png', response.image_data_uint8)","title":"Hello Car"},{"location":"apis/#hello-drone","text":"Here's how to use AirSim APIs using Python to control simulated quadrotor (see also C++ example ): # ready to run example: PythonClient/multirotor/hello_drone.py import airsim # connect to the AirSim simulator client = airsim.MultirotorClient() client.confirmConnection() client.enableApiControl(True) client.armDisarm(True) # Async methods returns Future. Call join() to wait for task to complete. client.takeoffAsync().join() client.moveToPositionAsync(-10, 10, -10, 5).join() # take images responses = client.simGetImages([ airsim.ImageRequest(\"0\", airsim.ImageType.DepthVis), airsim.ImageRequest(\"1\", airsim.ImageType.DepthPlanner, True)]) print('Retrieved images: %d', len(responses)) # do something with the images for response in responses: if response.pixels_as_float: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_float))) airsim.write_pfm(os.path.normpath('/temp/py1.pfm'), airsim.getPfmArray(response)) else: print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_uint8))) airsim.write_file(os.path.normpath('/temp/py1.png'), response.image_data_uint8)","title":"Hello Drone"},{"location":"apis/#common-apis","text":"reset : This resets the vehicle to its original starting state. Note that you must call enableApiControl and armDisarm again after the call to reset . confirmConnection : Checks state of connection every 1 sec and reports it in Console so user can see the progress for connection. enableApiControl : For safety reasons, by default API control for autonomous vehicle is not enabled and human operator has full control (usually via RC or joystick in simulator). The client must make this call to request control via API. It is likely that human operator of vehicle might have disallowed API control which would mean that enableApiControl has no effect. This can be checked by isApiControlEnabled . isApiControlEnabled : Returns true if API control is established. If false (which is default) then API calls would be ignored. After a successful call to enableApiControl , the isApiControlEnabled should return true. ping : If connection is established then this call will return true otherwise it will be blocked until timeout. simPrintLogMessage : Prints the specified message in the simulator's window. If message_param is also supplied then its printed next to the message and in that case if this API is called with same message value but different message_param again then previous line is overwritten with new line (instead of API creating new line on display). For example, simPrintLogMessage(\"Iteration: \", to_string(i)) keeps updating same line on display when API is called with different values of i. The valid values of severity parameter is 0 to 3 inclusive that corresponds to different colors. simGetObjectPose(ned=true) , simSetObjectPose : Gets and sets the pose of specified object in Unreal environment. Here the object means \"actor\" in Unreal terminology. They are searched by tag as well as name. Please note that the names shown in UE Editor are auto-generated in each run and are not permanent. So if you want to refer to actor by name, you must change its auto-generated name in UE Editor. Alternatively you can add a tag to actor which can be done by clicking on that actor in Unreal Editor and then going to Tags property , click \"+\" sign and add some string value. If multiple actors have same tag then the first match is returned. If no matches are found then NaN pose is returned. The returned pose is in NED coordinates in SI units with its origin at Player Start by default or in Unreal NED frame if the ned boolean argument is set to talse . For simSetObjectPose , the specified actor must have Mobility set to Movable or otherwise you will get undefined behavior. The simSetObjectPose has parameter teleport which means object is moved through other objects in its way and it returns true if move was successful simListSceneObjects : Provides a list of all objects in the environment. You can also use regular expression to filter specific objects by name. For example, the code below sets all meshes which have names starting with \"wall\" you can use simListSceneObjects(\"wall[\\w]*\") .","title":"Common APIs"},{"location":"apis/#imagecomputer-visioninstance-segmentation-apis","text":"AirSim offers comprehensive images APIs to retrieve synchronized images from multiple cameras along with ground truth including depth, disparity, surface normals and vision. You can set the resolution, FOV, motion blur etc parameters in settings.json . There is also API for detecting collision state. See also complete code that generates specified number of stereo images and ground truth depth with normalization to camera plan, computation of disparity image and saving it to pfm format . Furthermore, the Instance Segmentation system can also be manipulated through the API. More on image APIs, Computer Vision mode and instance segmentation configuration .","title":"Image/Computer Vision/Instance segmentation APIs"},{"location":"apis/#pause-and-continue-apis","text":"AirSim allows to pause and continue the simulation through pause(is_paused) API. To pause the simulation call pause(True) and to continue the simulation call pause(False) . You may have scenario, especially while using reinforcement learning, to run the simulation for specified amount of time and then automatically pause. While simulation is paused, you may then do some expensive computation, send a new command and then again run the simulation for specified amount of time. This can be achieved by API continueForTime(seconds) . This API runs the simulation for the specified number of seconds and then pauses the simulation. For example usage, please see pause_continue_car.py and pause_continue_drone.py .","title":"Pause and Continue APIs"},{"location":"apis/#collision-api","text":"The collision information can be obtained using simGetCollisionInfo API. This call returns a struct that has information not only whether collision occurred but also collision position, surface normal, penetration depth and so on.","title":"Collision API"},{"location":"apis/#time-of-day-api","text":"AirSim assumes there exist sky sphere of class EngineSky/BP_Sky_Sphere in your environment with ADirectionalLight actor . By default, the position of the sun in the scene doesn't move with time. You can use settings to set up latitude, longitude, date and time which AirSim uses to compute the position of sun in the scene. You can also use following API call to set the sun position according to given date time: simSetTimeOfDay(self, is_enabled, start_datetime = \"\", is_start_datetime_dst = False, celestial_clock_speed = 1, update_interval_secs = 60, move_sun = True) The is_enabled parameter must be True to enable time of day effect. If it is False then sun position is reset to its original in the environment. Other parameters are same as in settings .","title":"Time of Day API"},{"location":"apis/#weather-apis","text":"By default all weather effects are disabled. To enable weather effect, first call: simEnableWeather(True) Various weather effects can be enabled by using simSetWeatherParameter method which takes WeatherParameter , for example, client.simSetWeatherParameter(airsim.WeatherParameter.Rain, 0.25); The second parameter value is from 0 to 1. The first parameter provides following options: class WeatherParameter: Rain = 0 Roadwetness = 1 Snow = 2 RoadSnow = 3 MapleLeaf = 4 RoadLeaf = 5 Dust = 6 Fog = 7 Please note that Roadwetness , RoadSnow and RoadLeaf effects requires adding materials to your scene. Please see example code for more details.","title":"Weather APIs"},{"location":"apis/#lidar-apis","text":"AirSim offers API to retrieve point cloud data from (GPU)LiDAR sensors on vehicles. You can set the number of channels, points per second, horizontal and vertical FOV, etc parameters in settings.json . More on LiDAR APIs and settings , GPULiDAR APIs and settings and sensor settings","title":"LiDAR APIs"},{"location":"apis/#multiple-vehicles","text":"AirSim supports multiple vehicles and control them through APIs. Please Multiple Vehicles doc.","title":"Multiple Vehicles"},{"location":"apis/#coordinate-system","text":"All AirSim API uses NED coordinate system, i.e., +X is North, +Y is East and +Z is Down. All units are in SI system. Please note that this is different from coordinate system used internally by Unreal Engine. In Unreal Engine, +Z is up instead of down and length unit is in centimeters instead of meters. AirSim APIs takes care of the appropriate conversions. The starting point of the vehicle is always coordinates (0, 0, 0) in NED system. Thus when converting from Unreal coordinates to NED, we first subtract the starting offset and then scale by 100 for cm to m conversion. The vehicle is spawned in Unreal environment where the Player Start component is placed. There is a setting called OriginGeopoint in settings.json which assigns geographic longitude, longitude and altitude to the Player Start component.","title":"Coordinate System"},{"location":"apis/#vehicle-specific-apis","text":"","title":"Vehicle Specific APIs"},{"location":"apis/#apis-for-car","text":"Car has followings APIs available: setCarControls : This allows you to set throttle, steering, handbrake and auto or manual gear. getCarState : This retrieves the state information including speed, current gear and 6 kinematics quantities: position, orientation, linear and angular velocity, linear and angular acceleration. All quantities are in NED coordinate system, SI units in world frame except for angular velocity and accelerations which are in body frame. Image APIs .","title":"APIs for Car"},{"location":"apis/#apis-for-multirotor","text":"Multirotor can be controlled by specifying angles, velocity vector, destination position or some combination of these. There are corresponding move* APIs for this purpose. When doing position control, we need to use some path following algorithm. By default AirSim uses carrot following algorithm. This is often referred to as \"high level control\" because you just need to specify high level goal and the firmware takes care of the rest. Currently lowest level control available in AirSim is moveByAngleThrottleAsync API.","title":"APIs for Multirotor"},{"location":"apis/#getmultirotorstate","text":"This API returns the state of the vehicle in one call. The state includes, collision, estimated kinematics (i.e. kinematics computed by fusing sensors), and timestamp (nano seconds since epoch). The kinematics here means 6 quantities: position, orientation, linear and angular velocity, linear and angular acceleration. Please note that simple_slight currently doesn't support state estimator which means estimated and ground truth kinematics values would be same for simple_flight. Estimated kinematics are however available for PX4 except for angular acceleration. All quantities are in NED coordinate system, SI units in world frame except for angular velocity and accelerations which are in body frame.","title":"getMultirotorState"},{"location":"apis/#async-methods-duration-and-max_wait_seconds","text":"Many API methods has parameters named duration or max_wait_seconds and they have Async as suffix, for example, takeoffAsync . These methods will return immediately after starting the task in AirSim so that your client code can do something else while that task is being executed. If you want to wait for this task to complete then you can call waitOnLastTask like this: //C++ client.takeoffAsync()->waitOnLastTask(); # Python client.takeoffAsync().join() If you start another command then it automatically cancels the previous task and starts new command. This allows to use pattern where your coded continuously does the sensing, computes a new trajectory to follow and issues that path to vehicle in AirSim. Each newly issued trajectory cancels the previous trajectory allowing your code to continuously do the update as new sensor data arrives. All Async method returns concurrent.futures.Future in Python ( std::future in C++). Please note that these future classes currently do not allow to check status or cancel the task; they only allow to wait for task to complete. AirSim does provide API cancelLastTask , however.","title":"Async methods, duration and max_wait_seconds"},{"location":"apis/#drivetrain","text":"There are two modes you can fly vehicle: drivetrain parameter is set to airsim.DrivetrainType.ForwardOnly or airsim.DrivetrainType.MaxDegreeOfFreedom . When you specify ForwardOnly, you are saying that vehicle's front should always point in the direction of travel. So if you want drone to take left turn then it would first rotate so front points to left. This mode is useful when you have only front camera and you are operating vehicle using FPV view. This is more or less like travelling in car where you always have front view. The MaxDegreeOfFreedom means you don't care where the front points to. So when you take left turn, you just start going left like crab. Quadrotors can go in any direction regardless of where front points to. The MaxDegreeOfFreedom enables this mode.","title":"drivetrain"},{"location":"apis/#yaw_mode","text":"yaw_mode is a struct YawMode with two fields, yaw_or_rate and is_rate . If is_rate field is True then yaw_or_rate field is interpreted as angular velocity in degrees/sec which means you want vehicle to rotate continuously around its axis at that angular velocity while moving. If is_rate is False then yaw_or_rate is interpreted as angle in degrees which means you want vehicle to rotate to specific angle (i.e. yaw) and keep that angle while moving. You can probably see that when yaw_mode.is_rate == true , the drivetrain parameter shouldn't be set to ForwardOnly because you are contradicting by saying that keep front pointing ahead but also rotate continuously. However if you have yaw_mode.is_rate = false in ForwardOnly mode then you can do some funky stuff. For example, you can have drone do circles and have yaw_or_rate set to 90 so camera is always pointed to center (\"super cool selfie mode\"). In MaxDegreeofFreedom also you can get some funky stuff by setting yaw_mode.is_rate = true and say yaw_mode.yaw_or_rate = 20 . This will cause drone to go in its path while rotating which may allow to do 360 scanning. In most cases, you just don't want yaw to change which you can do by setting yaw rate of 0. The shorthand for this is airsim.YawMode.Zero() (or in C++: YawMode::Zero() ).","title":"yaw_mode"},{"location":"apis/#lookahead-and-adaptive_lookahead","text":"When you ask vehicle to follow a path, AirSim uses \"carrot following\" algorithm. This algorithm operates by looking ahead on path and adjusting its velocity vector. The parameters for this algorithm is specified by lookahead and adaptive_lookahead . For most of the time you want algorithm to auto-decide the values by simply setting lookahead = -1 and adaptive_lookahead = 0 .","title":"lookahead and adaptive_lookahead"},{"location":"apis/#using-apis-on-real-vehicles","text":"We want to be able to run same code that runs in simulation as on real vehicle. This allows you to test your code in simulator and deploy to real vehicle. Generally speaking, APIs therefore shouldn't allow you to do something that cannot be done on real vehicle (for example, getting the ground truth). But, of course, simulator has much more information and it would be useful in applications that may not care about running things on real vehicle. For this reason, we clearly delineate between sim-only APIs by attaching sim prefix, for example, simGetGroundTruthKinematics . This way you can avoid using these simulation-only APIs if you care about running your code on real vehicles. The AirLib is self-contained library that you can put on an offboard computing module such as the Gigabyte barebone Mini PC. This module then can talk to the flight controllers such as PX4 using exact same code and flight controller protocol. The code you write for testing in the simulator remains unchanged. See AirLib on custom drones .","title":"Using APIs on Real Vehicles"},{"location":"apis/#adding-new-apis-to-airsim","text":"Adding new APIs requires modifying the source code. Much of the changes are mechanical and required for various levels of abstractions that AirSim supports. This commit demonstrates how to add a simple API simPrintLogMessage that prints message in simulator window.","title":"Adding New APIs to AirSim"},{"location":"apis/#some-internals","text":"The APIs use msgpack-rpc protocol over TCP/IP through rpclib developed by Tam\u00c3\u00a1s Szelei which allows you to use variety of programming languages including C++, C#, Python, Java etc. When AirSim starts, it opens port 41451 (this can be changed via settings ) and listens for incoming request. The Python or C++ client code connects to this port and sends RPC calls using msgpack serialization format .","title":"Some Internals"},{"location":"apis/#references-and-examples","text":"Car Examples Multirotor Examples Computer Vision Examples Move on Path demo showing video of fast multirotor flight through Modular Neighborhood environment Building a Hexacopter Building Point Clouds","title":"References and Examples"},{"location":"apis/#faq","text":"","title":"FAQ"},{"location":"apis/#unreal-is-slowed-down-dramatically-when-i-run-api","text":"If you see Unreal getting slowed down dramatically when Unreal Engine window loses focus then go to 'Edit->Editor Preferences' in Unreal Editor, in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked.","title":"Unreal is slowed down dramatically when I run API"},{"location":"apis/#do-i-need-anything-else-on-windows","text":"You should install VS2017 with VC++, Windows SDK 8.1 and Python. To use Python APIs you will need Python 3.5 or later (install it using Anaconda).","title":"Do I need anything else on Windows?"},{"location":"apis/#which-version-of-python-should-i-use","text":"We recommend Anaconda to get Python tools and libraries. Our code is tested with Python 3.5.3 :: Anaconda 4.4.0. This is important because older version have been known to have problems .","title":"Which version of Python should I use?"},{"location":"apis/#i-get-error-on-import-cv2","text":"You can install OpenCV using: conda install opencv pip install opencv-python","title":"I get error on import cv2"},{"location":"build_linux/","text":"Build AirSim on Linux The current recommended and tested environment is Ubuntu 18.04 LTS . Theoretically, you can build on other distros and OSX as well, but we haven't tested it. Host machine Build Unreal Engine and Airsim This branch uses a custom version of the Unreal Engine! - Make sure you are registered with Epic Games . This is required to get source code access for Unreal Engine. Clone Unreal in your favorite folder and build it (this may take a while!). Note : We only support Unreal 4.24.4 ( Cosys-Lab fork ) at present. bash # go to the folder where you clone GitHub projects git clone https://github.com/WouterJansen/UnrealEngine.git cd UnrealEngine ./Setup.sh ./GenerateProjectFiles.sh make Clone AirSim and build it: bash # go to the folder where you clone GitHub projects git clone https://github.com/Cosys-Lab/Cosys-AirSim.git cd AirSim ./setup.sh ./build.sh Build Unreal Environment Finally, you will need an Unreal project that hosts the environment for your vehicles. AirSim comes with a built-in \"Blocks Environment\" which you can use, or you can create your own. Please see setting up Unreal Environment . [Optional] Setup Remote Control (Multirotor Only) A remote control is required if you want to fly manually. See the remote control setup for more details. Alternatively, you can use APIs for programmatic control or use the so-called Computer Vision mode to move around using the keyboard. How to Use AirSim Once AirSim is set up by following above steps, you can, Go to UnrealEngine folder and start Unreal by running UnrealEngine/Engine/Binaries/Linux/UE4Editor . When Unreal Engine prompts for opening or creating project, select Browse and choose AirSim/Unreal/Environments/Blocks (or your custom Unreal project). If you get prompts to convert project, look for More Options or Convert-In-Place option. If you get prompted to build, chose Yes. If you get prompted to disable AirSim plugin, choose No. After Unreal Editor loads, press Play button. Tip: go to 'Edit->Editor Preferences', in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. See Using APIs and settings.json for various options available. The other environments available often need additional asset packs to be downloaded first, read here for more information. FAQs I'm getting error \" could not be compiled. Try rebuilding from source manually\". This could either happen because of compile error or the fact that your gch files are outdated. Look in to your console window. Do you see something like below? fatal error: file '/usr/include/linux/version.h''/usr/include/linux/version.h' has been modified since the precompiled header If this is the case then look for *.gch file(s) that follows after that message, delete them and try again. Here's relevant thread on Unreal Engine forums. If you see other compile errors in console then open up those source files and see if it is due to changes you made. If not, then report it as issue on GitHub. Unreal crashed! How do I know what went wrong? Go to the MyUnrealProject/Saved/Crashes folder and search for the file MyProject.log within its subdirectories. At the end of this file you will see the stack trace and messages. You can also take a look at the Diagnostics.txt file. How do I use an IDE on Linux? You can use Qt Creator or CodeLite. Instructions for Qt Creator are available here . Can I cross compile for Linux from a Windows machine? Yes, you can, but we haven't tested it. You can find the instructions here . What compiler and stdlib does AirSim use? We use the same compiler that Unreal Engine uses, Clang 5.0 , and stdlib, libc++ . AirSim's setup.sh will automatically download them both. The libc++ source code is cloned into the llvm-source-(version) folder and is built into the llvm-build folder, from where CMake uses libc++. What version of CMake does the AirSim build use? 3.9.0 or higher. This is not the default in Ubuntu 16.04 so setup.sh installs it for you. You can check your CMake version using cmake --version . If you have an older version, follow these instructions or see the CMake website . Can I compile AirSim in BashOnWindows? Yes, however, you can't run Unreal from BashOnWindows. So this is kind of useful to check a Linux compile, but not for an end-to-end run. See the BashOnWindows install guide . Make sure to have the latest version (Windows 10 Creators Edition) as previous versions had various issues. Also, don't invoke bash from Visual Studio Command Prompt , otherwise CMake might find VC++ and try and use that! Where can I find more info on running Unreal on Linux? Start here: Unreal on Linux Building Unreal on Linux Unreal Linux Support Unreal Cross Compilation","title":"Build on Linux"},{"location":"build_linux/#build-airsim-on-linux","text":"The current recommended and tested environment is Ubuntu 18.04 LTS . Theoretically, you can build on other distros and OSX as well, but we haven't tested it.","title":"Build AirSim on Linux"},{"location":"build_linux/#host-machine","text":"","title":"Host machine"},{"location":"build_linux/#build-unreal-engine-and-airsim","text":"This branch uses a custom version of the Unreal Engine! - Make sure you are registered with Epic Games . This is required to get source code access for Unreal Engine. Clone Unreal in your favorite folder and build it (this may take a while!). Note : We only support Unreal 4.24.4 ( Cosys-Lab fork ) at present. bash # go to the folder where you clone GitHub projects git clone https://github.com/WouterJansen/UnrealEngine.git cd UnrealEngine ./Setup.sh ./GenerateProjectFiles.sh make Clone AirSim and build it: bash # go to the folder where you clone GitHub projects git clone https://github.com/Cosys-Lab/Cosys-AirSim.git cd AirSim ./setup.sh ./build.sh","title":"Build Unreal Engine and Airsim"},{"location":"build_linux/#build-unreal-environment","text":"Finally, you will need an Unreal project that hosts the environment for your vehicles. AirSim comes with a built-in \"Blocks Environment\" which you can use, or you can create your own. Please see setting up Unreal Environment .","title":"Build Unreal Environment"},{"location":"build_linux/#optional-setup-remote-control-multirotor-only","text":"A remote control is required if you want to fly manually. See the remote control setup for more details. Alternatively, you can use APIs for programmatic control or use the so-called Computer Vision mode to move around using the keyboard.","title":"[Optional] Setup Remote Control (Multirotor Only)"},{"location":"build_linux/#how-to-use-airsim","text":"Once AirSim is set up by following above steps, you can, Go to UnrealEngine folder and start Unreal by running UnrealEngine/Engine/Binaries/Linux/UE4Editor . When Unreal Engine prompts for opening or creating project, select Browse and choose AirSim/Unreal/Environments/Blocks (or your custom Unreal project). If you get prompts to convert project, look for More Options or Convert-In-Place option. If you get prompted to build, chose Yes. If you get prompted to disable AirSim plugin, choose No. After Unreal Editor loads, press Play button. Tip: go to 'Edit->Editor Preferences', in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. See Using APIs and settings.json for various options available. The other environments available often need additional asset packs to be downloaded first, read here for more information.","title":"How to Use AirSim"},{"location":"build_linux/#faqs","text":"I'm getting error \" could not be compiled. Try rebuilding from source manually\". This could either happen because of compile error or the fact that your gch files are outdated. Look in to your console window. Do you see something like below? fatal error: file '/usr/include/linux/version.h''/usr/include/linux/version.h' has been modified since the precompiled header If this is the case then look for *.gch file(s) that follows after that message, delete them and try again. Here's relevant thread on Unreal Engine forums. If you see other compile errors in console then open up those source files and see if it is due to changes you made. If not, then report it as issue on GitHub. Unreal crashed! How do I know what went wrong? Go to the MyUnrealProject/Saved/Crashes folder and search for the file MyProject.log within its subdirectories. At the end of this file you will see the stack trace and messages. You can also take a look at the Diagnostics.txt file. How do I use an IDE on Linux? You can use Qt Creator or CodeLite. Instructions for Qt Creator are available here . Can I cross compile for Linux from a Windows machine? Yes, you can, but we haven't tested it. You can find the instructions here . What compiler and stdlib does AirSim use? We use the same compiler that Unreal Engine uses, Clang 5.0 , and stdlib, libc++ . AirSim's setup.sh will automatically download them both. The libc++ source code is cloned into the llvm-source-(version) folder and is built into the llvm-build folder, from where CMake uses libc++. What version of CMake does the AirSim build use? 3.9.0 or higher. This is not the default in Ubuntu 16.04 so setup.sh installs it for you. You can check your CMake version using cmake --version . If you have an older version, follow these instructions or see the CMake website . Can I compile AirSim in BashOnWindows? Yes, however, you can't run Unreal from BashOnWindows. So this is kind of useful to check a Linux compile, but not for an end-to-end run. See the BashOnWindows install guide . Make sure to have the latest version (Windows 10 Creators Edition) as previous versions had various issues. Also, don't invoke bash from Visual Studio Command Prompt , otherwise CMake might find VC++ and try and use that! Where can I find more info on running Unreal on Linux? Start here: Unreal on Linux Building Unreal on Linux Unreal Linux Support Unreal Cross Compilation","title":"FAQs"},{"location":"build_windows/","text":"Build AirSim on Windows Install Unreal Engine This branch uses a custom version of the Unreal Engine! - Make sure you are registered with Epic Games . This is required to get source code access for Unreal Engine. Clone Unreal in your favorite folder and build it (this may take a while!). Note : We only support Unreal 4.24.4 ( Cosys-Lab fork ) at present. bash # go to the folder where you clone GitHub projects git clone https://github.com/WouterJansen/UnrealEngine.git cd UnrealEngine -Visual Studio 2019 is required for building. To install the correct components for UE4 development, check the \"Game Development with C++\" workload and the \u201c.net 4.6.2\u201d, \"Unreal Engine Installer\" and \"Nuget Package Manager\" optional individual components. run Setup.bat run GenerateProjectFiles.bat as administrator Build UnrealEngine Open the generated Visual Studio project UE4.sln in the root of the repository. Once it is open you need to set it to build the Development Editor configuration for Win64 . Once that is set, you can right click the UE4 target in the Solution Explorer on the right side of the window and press build. This will take a while. please run Engine\\Binaries\\Win64\\UnrealVersionSelector-Win64-Shipping.exe once so it is detectable by your system. Build AirSim Start x64 Native Tools Command Prompt for VS 2019 . Clone the repo: git clone https://github.com/Cosys-Lab/Cosys-AirSim.git , and go the AirSim directory by cd AirSim . Run build.cmd from the command line. This will create ready to use plugin bits in the Unreal\\Plugins folder that can be dropped into any Unreal project. Build Unreal Project Finally, you will need an Unreal project that hosts the environment for your vehicles. AirSim comes with a built-in \"Blocks Environment\" which you can use, or you can create your own. Please see setting up Unreal Environment . Setup Remote Control (Multirotor only) A remote control is required if you want to fly manually. See the remote control setup for more details. Alternatively, you can use APIs for programmatic control or use the so-called Computer Vision mode to move around using the keyboard. How to Use AirSim Once AirSim is set up by following above steps, you can, Double click on .sln file to load for example the Blocks project in Unreal\\Environments\\Blocks (or .sln file in your own custom Unreal project). If you don't see .sln file then you probably haven't completed steps in Build Unreal Project section above. Select your Unreal project as Start Up project (for example, Blocks project) and make sure Build config is set to \"Develop Editor\" and x64. After Unreal Editor loads, press Play button. Tip: go to 'Edit->Editor Preferences', in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. See Using APIs and settings.json for various options available. The other environments available often need additional asset packs to be downloaded first, read here for more information. AirSim on Unity (Experimental) Unity is another great game engine platform and we have an experimental release of AirSim on Unity. Please note that this is work in progress and all features may not work yet. FAQ I get error C100 : An internal error has occurred in the compiler when running build.cmd We have noticed this happening with VS version 15.9.0 and have checked-in a workaround in AirSim code. If you have this VS version, please make sure to pull the latest AirSim code. I get error \"'corecrt.h': No such file or directory\" or \"Windows SDK version 8.1 not found\" Very likely you don't have Windows SDK installed with Visual Studio. How do I use PX4 firmware with AirSim? By default, AirSim uses its own built-in firmware called simple_flight . There is no additional setup if you just want to go with it. If you want to switch to using PX4 instead then please see this guide . I made changes in Visual Studio but there is no effect Sometimes the Unreal + VS build system doesn't recompile if you make changes to only header files. To ensure a recompile, make some Unreal based cpp file \"dirty\" like AirSimGameMode.cpp. Unreal still uses VS2015 or I'm getting some link error Running several versions of VS can lead to issues when compiling UE projects. One problem that may arise is that UE will try to compile with an older version of VS which may or may not work. There are two settings in Unreal, one for for the engine and one for the project, to adjust the version of VS to be used. 1. Edit -> Editor preferences -> General -> Source code 2. Edit -> Project Settings -> Platforms -> Windows -> Toolchain ->CompilerVersion In some cases, these settings will still not lead to the desired result and errors such as the following might be produced: LINK : fatal error LNK1181: cannot open input file 'ws2_32.lib' To resolve such issues the following procedure can be applied: 1. Uninstall all old versions of VS using the VisualStudioUninstaller 2. Repair/Install VS2017 3. Restart machine and install Epic launcher and desired version of the engine","title":"Build on Windows"},{"location":"build_windows/#build-airsim-on-windows","text":"","title":"Build AirSim on Windows"},{"location":"build_windows/#install-unreal-engine","text":"This branch uses a custom version of the Unreal Engine! - Make sure you are registered with Epic Games . This is required to get source code access for Unreal Engine. Clone Unreal in your favorite folder and build it (this may take a while!). Note : We only support Unreal 4.24.4 ( Cosys-Lab fork ) at present. bash # go to the folder where you clone GitHub projects git clone https://github.com/WouterJansen/UnrealEngine.git cd UnrealEngine -Visual Studio 2019 is required for building. To install the correct components for UE4 development, check the \"Game Development with C++\" workload and the \u201c.net 4.6.2\u201d, \"Unreal Engine Installer\" and \"Nuget Package Manager\" optional individual components. run Setup.bat run GenerateProjectFiles.bat as administrator","title":"Install Unreal Engine"},{"location":"build_windows/#build-unrealengine","text":"Open the generated Visual Studio project UE4.sln in the root of the repository. Once it is open you need to set it to build the Development Editor configuration for Win64 . Once that is set, you can right click the UE4 target in the Solution Explorer on the right side of the window and press build. This will take a while. please run Engine\\Binaries\\Win64\\UnrealVersionSelector-Win64-Shipping.exe once so it is detectable by your system.","title":"Build UnrealEngine"},{"location":"build_windows/#build-airsim","text":"Start x64 Native Tools Command Prompt for VS 2019 . Clone the repo: git clone https://github.com/Cosys-Lab/Cosys-AirSim.git , and go the AirSim directory by cd AirSim . Run build.cmd from the command line. This will create ready to use plugin bits in the Unreal\\Plugins folder that can be dropped into any Unreal project.","title":"Build AirSim"},{"location":"build_windows/#build-unreal-project","text":"Finally, you will need an Unreal project that hosts the environment for your vehicles. AirSim comes with a built-in \"Blocks Environment\" which you can use, or you can create your own. Please see setting up Unreal Environment .","title":"Build Unreal Project"},{"location":"build_windows/#setup-remote-control-multirotor-only","text":"A remote control is required if you want to fly manually. See the remote control setup for more details. Alternatively, you can use APIs for programmatic control or use the so-called Computer Vision mode to move around using the keyboard.","title":"Setup Remote Control (Multirotor only)"},{"location":"build_windows/#how-to-use-airsim","text":"Once AirSim is set up by following above steps, you can, Double click on .sln file to load for example the Blocks project in Unreal\\Environments\\Blocks (or .sln file in your own custom Unreal project). If you don't see .sln file then you probably haven't completed steps in Build Unreal Project section above. Select your Unreal project as Start Up project (for example, Blocks project) and make sure Build config is set to \"Develop Editor\" and x64. After Unreal Editor loads, press Play button. Tip: go to 'Edit->Editor Preferences', in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. See Using APIs and settings.json for various options available. The other environments available often need additional asset packs to be downloaded first, read here for more information.","title":"How to Use AirSim"},{"location":"build_windows/#airsim-on-unity-experimental","text":"Unity is another great game engine platform and we have an experimental release of AirSim on Unity. Please note that this is work in progress and all features may not work yet.","title":"AirSim on Unity (Experimental)"},{"location":"build_windows/#faq","text":"","title":"FAQ"},{"location":"build_windows/#i-get-error-c100-an-internal-error-has-occurred-in-the-compiler-when-running-buildcmd","text":"We have noticed this happening with VS version 15.9.0 and have checked-in a workaround in AirSim code. If you have this VS version, please make sure to pull the latest AirSim code.","title":"I get error C100 : An internal error has occurred in the compiler when running build.cmd"},{"location":"build_windows/#i-get-error-corecrth-no-such-file-or-directory-or-windows-sdk-version-81-not-found","text":"Very likely you don't have Windows SDK installed with Visual Studio.","title":"I get error \"'corecrt.h': No such file or directory\" or \"Windows SDK version 8.1 not found\""},{"location":"build_windows/#how-do-i-use-px4-firmware-with-airsim","text":"By default, AirSim uses its own built-in firmware called simple_flight . There is no additional setup if you just want to go with it. If you want to switch to using PX4 instead then please see this guide .","title":"How do I use PX4 firmware with AirSim?"},{"location":"build_windows/#i-made-changes-in-visual-studio-but-there-is-no-effect","text":"Sometimes the Unreal + VS build system doesn't recompile if you make changes to only header files. To ensure a recompile, make some Unreal based cpp file \"dirty\" like AirSimGameMode.cpp.","title":"I made changes in Visual Studio but there is no effect"},{"location":"build_windows/#unreal-still-uses-vs2015-or-im-getting-some-link-error","text":"Running several versions of VS can lead to issues when compiling UE projects. One problem that may arise is that UE will try to compile with an older version of VS which may or may not work. There are two settings in Unreal, one for for the engine and one for the project, to adjust the version of VS to be used. 1. Edit -> Editor preferences -> General -> Source code 2. Edit -> Project Settings -> Platforms -> Windows -> Toolchain ->CompilerVersion In some cases, these settings will still not lead to the desired result and errors such as the following might be produced: LINK : fatal error LNK1181: cannot open input file 'ws2_32.lib' To resolve such issues the following procedure can be applied: 1. Uninstall all old versions of VS using the VisualStudioUninstaller 2. Repair/Install VS2017 3. Restart machine and install Epic launcher and desired version of the engine","title":"Unreal still uses VS2015 or I'm getting some link error"},{"location":"camera_views/","text":"Camera Views The camera views that are shown on screen are the camera views you can fetch via the simGetImages API . From left to right is the depth view, segmentation view and the FPV view. See Image APIs for description of various available views. Turning ON/OFF Views Press F1 key to see keyboard shortcuts for turning on/off any or all views. You can also select various view modes there, such as \"Fly with Me\" mode, FPV mode and \"Ground View\" mode. Configuring Sub-Windows Now you can select what is shown by each of above sub windows. For instance, you can chose to show surface normals in first window (instead of depth) and disparity in second window (instead of segmentation). Below is the settings value you can use in settings.json : { \"SubWindows\": [ {\"Index\": 1, \"ImageType\": 5}, {\"Index\": 2, \"ImageType\": 3} ] } Performance Impact Note : This section is outdated and has not been updated for new performance enhancement changes. Now rendering these views does impact the FPS performance of the game, since this is additional work for the GPU. The following shows the impact on FPS when you open these views. This is measured on Intel core i7 computer with 32 gb RAM and a GeForce GTX 1080 graphics card running the Modular Neighborhood map, using cooked debug bits, no debugger or GameEditor open. The normal state with no subviews open is measuring around 16 ms per frame, which means it is keeping a nice steady 60 FPS (which is the target FPS). As it climbs up to 35ms the FPS drops to around 28 frames per second, spiking to 40ms means a few drops to 25 fps. The simulator can still function and fly correctly when all this is going on even in the worse case because the physics is decoupled from the rendering. However if the delay gets too high such that the communication with PX4 hardware is interrupted due to overly busy CPU then the flight can stall due to timeout in the offboard control messages. On the computer where this was measured the drone could fly the path.py program without any problems with all views open, and with 3 python scripts running to capture each view type. But there was one stall during this flight, but it recovered gracefully and completed the path. So it was right on the limit. The following shows the impact on CPU, perhaps a bit surprisingly, the CPU impact is also non trivial.","title":"Camera Views"},{"location":"camera_views/#camera-views","text":"The camera views that are shown on screen are the camera views you can fetch via the simGetImages API . From left to right is the depth view, segmentation view and the FPV view. See Image APIs for description of various available views.","title":"Camera Views"},{"location":"camera_views/#turning-onoff-views","text":"Press F1 key to see keyboard shortcuts for turning on/off any or all views. You can also select various view modes there, such as \"Fly with Me\" mode, FPV mode and \"Ground View\" mode.","title":"Turning ON/OFF Views"},{"location":"camera_views/#configuring-sub-windows","text":"Now you can select what is shown by each of above sub windows. For instance, you can chose to show surface normals in first window (instead of depth) and disparity in second window (instead of segmentation). Below is the settings value you can use in settings.json : { \"SubWindows\": [ {\"Index\": 1, \"ImageType\": 5}, {\"Index\": 2, \"ImageType\": 3} ] }","title":"Configuring Sub-Windows"},{"location":"camera_views/#performance-impact","text":"Note : This section is outdated and has not been updated for new performance enhancement changes. Now rendering these views does impact the FPS performance of the game, since this is additional work for the GPU. The following shows the impact on FPS when you open these views. This is measured on Intel core i7 computer with 32 gb RAM and a GeForce GTX 1080 graphics card running the Modular Neighborhood map, using cooked debug bits, no debugger or GameEditor open. The normal state with no subviews open is measuring around 16 ms per frame, which means it is keeping a nice steady 60 FPS (which is the target FPS). As it climbs up to 35ms the FPS drops to around 28 frames per second, spiking to 40ms means a few drops to 25 fps. The simulator can still function and fly correctly when all this is going on even in the worse case because the physics is decoupled from the rendering. However if the delay gets too high such that the communication with PX4 hardware is interrupted due to overly busy CPU then the flight can stall due to timeout in the offboard control messages. On the computer where this was measured the drone could fly the path.py program without any problems with all views open, and with 3 python scripts running to capture each view type. But there was one stall during this flight, but it recovered gracefully and completed the path. So it was right on the limit. The following shows the impact on CPU, perhaps a bit surprisingly, the CPU impact is also non trivial.","title":"Performance Impact"},{"location":"cmake_linux/","text":"Installing cmake on Linux If you don't have cmake version 3.10 (for example, 3.2.2 is the default on Ubuntu 14) you can run the following: mkdir ~/cmake-3.10.2 cd ~/cmake-3.10.2 wget https://cmake.org/files/v3.10/cmake-3.10.2-Linux-x86_64.sh Now you have to run this command by itself (it is interactive) sh cmake-3.10.2-Linux-x86_64.sh --prefix ~/cmake-3.10.2 Answer 'n' to the question about creating another cmake-3.10.2-Linux-x86_64 folder and then sudo update-alternatives --install /usr/bin/cmake cmake ~/cmake-3.10.2/bin/cmake 60 Now type cmake --version to make sure your cmake version is 3.10.2.","title":"Installing cmake on Linux"},{"location":"cmake_linux/#installing-cmake-on-linux","text":"If you don't have cmake version 3.10 (for example, 3.2.2 is the default on Ubuntu 14) you can run the following: mkdir ~/cmake-3.10.2 cd ~/cmake-3.10.2 wget https://cmake.org/files/v3.10/cmake-3.10.2-Linux-x86_64.sh Now you have to run this command by itself (it is interactive) sh cmake-3.10.2-Linux-x86_64.sh --prefix ~/cmake-3.10.2 Answer 'n' to the question about creating another cmake-3.10.2-Linux-x86_64 folder and then sudo update-alternatives --install /usr/bin/cmake cmake ~/cmake-3.10.2/bin/cmake 60 Now type cmake --version to make sure your cmake version is 3.10.2.","title":"Installing cmake on Linux"},{"location":"code_structure/","text":"AirLib Majority of the code is located in AirLib. This is a self-contained library that you should be able to compile with any C++11 compiler. AirLib consists of the following components: 1. Physics engine: This is header-only physics engine. It is designed to be fast and extensible to implement different vehicles. 2. Sensor models: This is header-only models for Barometer, IMU, GPS and Magnetometer 3. Vehicle models: This is header-only models for vehicle configurations and models. Currently we have implemented model for a MultiRotor and a configuration for PX4 QuadRotor in the X config. 4. Control library: This part of AirLib provides abstract base class for our APIs and concrete implementation for specific vehicle platforms such as MavLink. It also has classes for the RPC client and server. Unreal/Plugins/AirSim This is the only portion of project which is dependent on Unreal engine. We have kept it isolated so we can implement simulator for other platforms as well (for example, Unity). The Unreal code takes advantage of its UObject based classes including Blueprints. 1. SimMode_ classes : We wish to support various simulator modes such as pure Computer Vision mode where there is no drone. The SimMode classes help implement many different modes. 2. VehiclePawnBase : This is the base class for all vehicle pawn visualizations. 3. VehicleBase : This class provides abstract interface to implement a combination of rendering component (i.e. Unreal pawn), physics component (i.e. MultiRotor) and controller (i.e. MavLinkHelper). MavLinkCom This is the library developed by our own team member Chris Lovett that provides C++ classes to talk to the MavLink devices. This library is stand alone and can be used in any project. See MavLinkCom for more info. Sample Programs We have created a few sample programs to demonstrate how to use the API. See HelloDrone and DroneShell. DroneShell demonstrates how to connect to the simulator using UDP. The simulator is running a server (similar to DroneServer). Contributing See Contribution Guidelines Unreal Framework The following picture illustrates how AirSim is loaded and invoked by the Unreal Game Engine:","title":"Code Structure"},{"location":"code_structure/#airlib","text":"Majority of the code is located in AirLib. This is a self-contained library that you should be able to compile with any C++11 compiler. AirLib consists of the following components: 1. Physics engine: This is header-only physics engine. It is designed to be fast and extensible to implement different vehicles. 2. Sensor models: This is header-only models for Barometer, IMU, GPS and Magnetometer 3. Vehicle models: This is header-only models for vehicle configurations and models. Currently we have implemented model for a MultiRotor and a configuration for PX4 QuadRotor in the X config. 4. Control library: This part of AirLib provides abstract base class for our APIs and concrete implementation for specific vehicle platforms such as MavLink. It also has classes for the RPC client and server.","title":"AirLib"},{"location":"code_structure/#unrealpluginsairsim","text":"This is the only portion of project which is dependent on Unreal engine. We have kept it isolated so we can implement simulator for other platforms as well (for example, Unity). The Unreal code takes advantage of its UObject based classes including Blueprints. 1. SimMode_ classes : We wish to support various simulator modes such as pure Computer Vision mode where there is no drone. The SimMode classes help implement many different modes. 2. VehiclePawnBase : This is the base class for all vehicle pawn visualizations. 3. VehicleBase : This class provides abstract interface to implement a combination of rendering component (i.e. Unreal pawn), physics component (i.e. MultiRotor) and controller (i.e. MavLinkHelper).","title":"Unreal/Plugins/AirSim"},{"location":"code_structure/#mavlinkcom","text":"This is the library developed by our own team member Chris Lovett that provides C++ classes to talk to the MavLink devices. This library is stand alone and can be used in any project. See MavLinkCom for more info.","title":"MavLinkCom"},{"location":"code_structure/#sample-programs","text":"We have created a few sample programs to demonstrate how to use the API. See HelloDrone and DroneShell. DroneShell demonstrates how to connect to the simulator using UDP. The simulator is running a server (similar to DroneServer).","title":"Sample Programs"},{"location":"code_structure/#contributing","text":"See Contribution Guidelines","title":"Contributing"},{"location":"code_structure/#unreal-framework","text":"The following picture illustrates how AirSim is loaded and invoked by the Unreal Game Engine:","title":"Unreal Framework"},{"location":"coding_guidelines/","text":"Modern C++ Coding Guidelines We are using Modern C++11. Smart pointers, Lambdas, and C++11 multithreading primitives are your friend. Quick Note The great thing about \"standards\" is that there are many to chose from: ISO , Sutter & Stroustrup , ROS , LINUX , Google's , Microsoft's , CERN's , GCC's , ARM's , LLVM's and probably thousands of others. Unfortunately most of these can't even agree on something as basic as how to name a class or a constant. This is probably due to the fact that these standards often carry lots of legacy issues due to supporting existing code bases. The intention behind this document is to create guidance that remains as close to ISO, Sutter & Stroustrup and ROS while resolving as many conflicts, disadvantages and inconsistencies as possible among them. Naming Conventions Avoid using any sort of Hungarian notation on names and \"_ptr\" on pointers. Code Element Style Comment Namespace under_scored Differentiate from class names Class name CamelCase To differentiate from STL types which ISO recommends (do not use \"C\" or \"T\" prefixes) Function name camelCase Lower case start is almost universal except for .Net world Parameters/Locals under_scored Vast majority of standards recommends this because _ is more readable to C++ crowd (although not much to Java/.Net crowd) Member variables under_scored_with_ The prefix _ is heavily discouraged as ISO has rules around reserving _identifiers, so we recommend suffix instead Enums and its members CamelCase Most except very old standards agree with this one Globals g_under_scored You shouldn't have these in first place! Constants UPPER_CASE Very contentious and we just have to pick one here, unless if is a private constant in class or method, then use naming for Members or Locals File names Match case of class name in file Lot of pro and cons either way but this removes inconsistency in auto generated code (important for ROS) Header Files Use a namespace qualified #ifdef to protect against multiple inclusion: #ifndef msr_airsim_MyHeader_hpp #define msr_airsim_MyHeader_hpp //--your code #endif The reason we don't use #pragma once is because it's not supported if same header file exists at multiple places (which might be possible under ROS build system!). Bracketing Inside function or method body place curly bracket on same line. Outside that the Namespace, Class and methods levels use separate line. This is called K&R style and its variants are widely used in C++ vs other styles which are more popular in other languages. Notice that curlies are not required if you have single statement, but complex statements are easier to keep correct with the braces. int main(int argc, char* argv[]) { while (x == y) { f0(); if (cont()) { f1(); } else { f2(); f3(); } if (x > 100) break; } } Const and References Religiously review all non-scalar parameters you declare to be candidate for const and references. If you are coming from languages such as C#/Java/Python, the most often mistake you would make is to pass parameters by value instead of const T&; Especially most of the strings, vectors and maps you want to pass as const T&; (if they are readonly) or T& (if they are writable). Also add const suffix to methods as much as possible. Overriding When overriding virtual method, use override suffix. Pointers This is really about memory management. A simulator has much performance critical code, so we try and avoid overloading the memory manager with lots of calls to new/delete. We also want to avoid too much copying of things on the stack, so we pass things by reference when ever possible. But when the object really needs to live longer than the call stack you often need to allocate that object on the heap, and so you have a pointer. Now, if management of the lifetime of that object is going to be tricky we recommend using C++ 11 smart pointers . But smart pointers do have a cost, so don\u2019t use them blindly everywhere. For private code where performance is paramount, raw pointers can be used. Raw pointers are also often needed when interfacing with legacy systems that only accept pointer types, for example, sockets API. But we try to wrap those legacy interfaces as much as possible and avoid that style of programming from leaking into the larger code base. Religiously check if you can use const everywhere, for example, const float * const xP . Avoid using prefix or suffix to indicate pointer types in variable names, i.e. use my_obj instead of myobj_ptr except in cases where it might make sense to differentiate variables better, for example, int mynum = 5; int* mynum_ptr = mynum; This is Too Short, ye? Yes, and it's on purpose because no one likes to read 200 page coding guidelines. The goal here is to cover only most significant things which are already not covered by strict mode compilation in GCC and Level 4 warnings-as-errors in VC++. If you had like to know about how to write better code in C++, please see GotW and Effective Modern C++ book.","title":"Coding Guidelines"},{"location":"coding_guidelines/#modern-c-coding-guidelines","text":"We are using Modern C++11. Smart pointers, Lambdas, and C++11 multithreading primitives are your friend.","title":"Modern C++ Coding Guidelines"},{"location":"coding_guidelines/#quick-note","text":"The great thing about \"standards\" is that there are many to chose from: ISO , Sutter & Stroustrup , ROS , LINUX , Google's , Microsoft's , CERN's , GCC's , ARM's , LLVM's and probably thousands of others. Unfortunately most of these can't even agree on something as basic as how to name a class or a constant. This is probably due to the fact that these standards often carry lots of legacy issues due to supporting existing code bases. The intention behind this document is to create guidance that remains as close to ISO, Sutter & Stroustrup and ROS while resolving as many conflicts, disadvantages and inconsistencies as possible among them.","title":"Quick Note"},{"location":"coding_guidelines/#naming-conventions","text":"Avoid using any sort of Hungarian notation on names and \"_ptr\" on pointers. Code Element Style Comment Namespace under_scored Differentiate from class names Class name CamelCase To differentiate from STL types which ISO recommends (do not use \"C\" or \"T\" prefixes) Function name camelCase Lower case start is almost universal except for .Net world Parameters/Locals under_scored Vast majority of standards recommends this because _ is more readable to C++ crowd (although not much to Java/.Net crowd) Member variables under_scored_with_ The prefix _ is heavily discouraged as ISO has rules around reserving _identifiers, so we recommend suffix instead Enums and its members CamelCase Most except very old standards agree with this one Globals g_under_scored You shouldn't have these in first place! Constants UPPER_CASE Very contentious and we just have to pick one here, unless if is a private constant in class or method, then use naming for Members or Locals File names Match case of class name in file Lot of pro and cons either way but this removes inconsistency in auto generated code (important for ROS)","title":"Naming Conventions"},{"location":"coding_guidelines/#header-files","text":"Use a namespace qualified #ifdef to protect against multiple inclusion: #ifndef msr_airsim_MyHeader_hpp #define msr_airsim_MyHeader_hpp //--your code #endif The reason we don't use #pragma once is because it's not supported if same header file exists at multiple places (which might be possible under ROS build system!).","title":"Header Files"},{"location":"coding_guidelines/#bracketing","text":"Inside function or method body place curly bracket on same line. Outside that the Namespace, Class and methods levels use separate line. This is called K&R style and its variants are widely used in C++ vs other styles which are more popular in other languages. Notice that curlies are not required if you have single statement, but complex statements are easier to keep correct with the braces. int main(int argc, char* argv[]) { while (x == y) { f0(); if (cont()) { f1(); } else { f2(); f3(); } if (x > 100) break; } }","title":"Bracketing"},{"location":"coding_guidelines/#const-and-references","text":"Religiously review all non-scalar parameters you declare to be candidate for const and references. If you are coming from languages such as C#/Java/Python, the most often mistake you would make is to pass parameters by value instead of const T&; Especially most of the strings, vectors and maps you want to pass as const T&; (if they are readonly) or T& (if they are writable). Also add const suffix to methods as much as possible.","title":"Const and References"},{"location":"coding_guidelines/#overriding","text":"When overriding virtual method, use override suffix.","title":"Overriding"},{"location":"coding_guidelines/#pointers","text":"This is really about memory management. A simulator has much performance critical code, so we try and avoid overloading the memory manager with lots of calls to new/delete. We also want to avoid too much copying of things on the stack, so we pass things by reference when ever possible. But when the object really needs to live longer than the call stack you often need to allocate that object on the heap, and so you have a pointer. Now, if management of the lifetime of that object is going to be tricky we recommend using C++ 11 smart pointers . But smart pointers do have a cost, so don\u2019t use them blindly everywhere. For private code where performance is paramount, raw pointers can be used. Raw pointers are also often needed when interfacing with legacy systems that only accept pointer types, for example, sockets API. But we try to wrap those legacy interfaces as much as possible and avoid that style of programming from leaking into the larger code base. Religiously check if you can use const everywhere, for example, const float * const xP . Avoid using prefix or suffix to indicate pointer types in variable names, i.e. use my_obj instead of myobj_ptr except in cases where it might make sense to differentiate variables better, for example, int mynum = 5; int* mynum_ptr = mynum;","title":"Pointers"},{"location":"coding_guidelines/#this-is-too-short-ye","text":"Yes, and it's on purpose because no one likes to read 200 page coding guidelines. The goal here is to cover only most significant things which are already not covered by strict mode compilation in GCC and Level 4 warnings-as-errors in VC++. If you had like to know about how to write better code in C++, please see GotW and Effective Modern C++ book.","title":"This is Too Short, ye?"},{"location":"custom_drone/","text":"AirLib on a Real Drone The AirLib library can be compiled and deployed on the companion computer on a real drone. For our testing, we mounted a Gigabyte Brix BXi7-5500 ultra compact PC on the drone connected to the Pixhawk flight controller over USB. The Gigabyte PC is running Ubuntu, so we are able to SSH into it over Wi-Fi: Once connected you can run MavLinkTest with this command line: MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. And this will produce a log file of the flight which can then be used for playback in the simulator . You can also add -proxy:192.168.1.100:14550 to connect MavLinkTest to a remote computer where you can run QGroundControl or our PX4 Log Viewer which is another handy way to see what is going on with your drone. MavLinkTest then has some simple commands for testing your drone, here's a simple example of some commands: arm takeoff 5 orbit 10 2 This will arm the drone, takeoff of 5 meters, then do an orbit pattern radius 10 meters, at 2 m/s. Type '?' to find all available commands. Note: Some commands (for example, orbit ) are named differently and have different syntax in MavLinkTest and DroneShell (for example, circlebypath -radius 10 -velocity 21 ). When you land the drone you can stop MavLinkTest and copy the *.mavlink log file that was generated. DroneServer and DroneShell Once you are happy that the MavLinkTest is working, you can also run DroneServer and DroneShell as follows. First, run MavLinkTest with a local proxy to send everything to DroneServer: MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. -proxy:127.0.0.1:14560 Change ~/Documents/AirSim/settings.json to say \"serial\":false, because we want DroneServer to look for this UDP connection. DroneServer 0 Lastly, you can now connect DroneShell to this instance of DroneServer and use the DroneShell commands to fly your drone: DroneShell ==||=> Welcome to DroneShell 1.0. Type ? for help. Microsoft Research (c) 2016. Waiting for drone to report a valid GPS location... ==||=> requestcontrol ==||=> arm ==||=> takeoff ==||=> circlebypath -radius 10 -velocity 2 PX4 Specific Tools You can run the MavlinkCom library and MavLinkTest app to test the connection between your companion computer and flight controller. How Does This Work? AirSim uses MavLinkCom component developed by @lovettchris. The MavLinkCom has a proxy architecture where you can open a connection to PX4 either using serial or UDP and then other components share this connection. When PX4 sends MavLink message, all components receive that message. If any component sends a message then it's received by PX4 only. This allows you to connect any number of components to PX4 This code opens a connection for LogViewer and QGC. You can add something more if you like. If you want to use QGC + AirSim together than you will need QGC to let own the serial port. QGC opens up TCP connection that acts as a proxy so any other component can connect to QGC and send MavLinkMessage to QGC and then QGC forwards that message to PX4. So you tell AirSim to connect to QGC and let QGC own serial port. For companion board, the way we did it earlier was to have Gigabyte Brix on the drone. This x86 full-fledged computer that will connect to PX4 through USB. We had Ubuntu on Brix and ran DroneServer . The DroneServer created an API endpoint that we can talk to via C++ client code (or Python code) and it translated API calls to MavLink messages. That way you can write your code against the same API, test it in the simulator and then run the same code on an actual vehicle. So the companion computer has DroneServer running along with client code.","title":"AirSim on Real Drones"},{"location":"custom_drone/#airlib-on-a-real-drone","text":"The AirLib library can be compiled and deployed on the companion computer on a real drone. For our testing, we mounted a Gigabyte Brix BXi7-5500 ultra compact PC on the drone connected to the Pixhawk flight controller over USB. The Gigabyte PC is running Ubuntu, so we are able to SSH into it over Wi-Fi: Once connected you can run MavLinkTest with this command line: MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. And this will produce a log file of the flight which can then be used for playback in the simulator . You can also add -proxy:192.168.1.100:14550 to connect MavLinkTest to a remote computer where you can run QGroundControl or our PX4 Log Viewer which is another handy way to see what is going on with your drone. MavLinkTest then has some simple commands for testing your drone, here's a simple example of some commands: arm takeoff 5 orbit 10 2 This will arm the drone, takeoff of 5 meters, then do an orbit pattern radius 10 meters, at 2 m/s. Type '?' to find all available commands. Note: Some commands (for example, orbit ) are named differently and have different syntax in MavLinkTest and DroneShell (for example, circlebypath -radius 10 -velocity 21 ). When you land the drone you can stop MavLinkTest and copy the *.mavlink log file that was generated.","title":"AirLib on a Real Drone"},{"location":"custom_drone/#droneserver-and-droneshell","text":"Once you are happy that the MavLinkTest is working, you can also run DroneServer and DroneShell as follows. First, run MavLinkTest with a local proxy to send everything to DroneServer: MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. -proxy:127.0.0.1:14560 Change ~/Documents/AirSim/settings.json to say \"serial\":false, because we want DroneServer to look for this UDP connection. DroneServer 0 Lastly, you can now connect DroneShell to this instance of DroneServer and use the DroneShell commands to fly your drone: DroneShell ==||=> Welcome to DroneShell 1.0. Type ? for help. Microsoft Research (c) 2016. Waiting for drone to report a valid GPS location... ==||=> requestcontrol ==||=> arm ==||=> takeoff ==||=> circlebypath -radius 10 -velocity 2","title":"DroneServer and DroneShell"},{"location":"custom_drone/#px4-specific-tools","text":"You can run the MavlinkCom library and MavLinkTest app to test the connection between your companion computer and flight controller.","title":"PX4 Specific Tools"},{"location":"custom_drone/#how-does-this-work","text":"AirSim uses MavLinkCom component developed by @lovettchris. The MavLinkCom has a proxy architecture where you can open a connection to PX4 either using serial or UDP and then other components share this connection. When PX4 sends MavLink message, all components receive that message. If any component sends a message then it's received by PX4 only. This allows you to connect any number of components to PX4 This code opens a connection for LogViewer and QGC. You can add something more if you like. If you want to use QGC + AirSim together than you will need QGC to let own the serial port. QGC opens up TCP connection that acts as a proxy so any other component can connect to QGC and send MavLinkMessage to QGC and then QGC forwards that message to PX4. So you tell AirSim to connect to QGC and let QGC own serial port. For companion board, the way we did it earlier was to have Gigabyte Brix on the drone. This x86 full-fledged computer that will connect to PX4 through USB. We had Ubuntu on Brix and ran DroneServer . The DroneServer created an API endpoint that we can talk to via C++ client code (or Python code) and it translated API calls to MavLink messages. That way you can write your code against the same API, test it in the simulator and then run the same code on an actual vehicle. So the companion computer has DroneServer running along with client code.","title":"How Does This Work?"},{"location":"design/","text":"Paper You can read more about our architecture and design in our paper (work in progress) . You may cite this as, @techreport{MSR-TR-2017-9, title = {{A}erial {I}nformatics and {R}obotics Platform}, author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor}, year = {2017}, institution = {Microsoft Research}, number = {{M}{S}{R}-{T}{R}-2017-9}} } Architecture Below is high level overview of how different components interact with each other.","title":"Architecture"},{"location":"design/#paper","text":"You can read more about our architecture and design in our paper (work in progress) . You may cite this as, @techreport{MSR-TR-2017-9, title = {{A}erial {I}nformatics and {R}obotics Platform}, author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor}, year = {2017}, institution = {Microsoft Research}, number = {{M}{S}{R}-{T}{R}-2017-9}} }","title":"Paper"},{"location":"design/#architecture","text":"Below is high level overview of how different components interact with each other.","title":"Architecture"},{"location":"dev_workflow/","text":"Development Workflow Below is the guide on how to perform different development activities while working with AirSim. If you are new to Unreal Engine based projects and want to contribute to AirSim or make your own forks for your custom requirements, this might save you some time. Development Environment OS We highly recommend Windows 10 and Visual Studio 2019 as your development environment. The support for other OSes and IDE is unfortunately not as mature on the Unreal Engine side and you may risk severe loss of productivity trying to do workarounds and jumping through the hoops. Hardware We recommend GPUs such as NVidia 1080 or NVidia Titan series with powerful desktop such as one with 64GB RAM, 6+ cores, SSDs and 2-3 displays (ideally 4K). We have found HP Z840 work quite well for our needs. The development experience on high-end laptops is generally sub-par compared to powerful desktops however they might be useful in a pinch. You generally want laptops with discrete NVidia GPU (at least M2000 or better) with 64GB RAM, SSDs and hopefully 4K display. We have found models such as Lenovo P50 work well for our needs. Laptops with only integrated graphics might not work well. Updating and Changing AirSim Code Overview AirSim is designed as plugin. This means it can't run by itself, you need to put it in an Unreal project (we call it \"environment\"). So building and testing AirSim has two steps: (1) build the plugin (2) deploy plugin in Unreal project and run the project. The first step is accomplished by build.cmd available in AirSim root. This command will update everything you need for the plugin in the Unreal\\Plugins folder. So to deploy the plugin, you just need to copy Unreal\\Plugins folder in to your Unreal project folder. Next you should remove all intermediate files in your Unreal project and then regenerate .sln file for your Unreal project. To do this, we have two handy .bat files in Unreal\\Environments\\Blocks folder: clean.bat and GenerateProjectFiles.bat . So just run these bat files in sequence from root of your Unreal project. Now you are ready to open new .sln in Visual Studio and press F5 to run it. Steps Below are the steps we use to make changes in AirSim and test them out. The best way to do development in AirSim code is to use Blocks project . This is the light weight project so compile time is relatively faster. Generally the workflow is, REM //Use x64 Native Tools Command Prompt for VS 2017 REM //Navigate to AirSim repo folder git pull build.cmd cd Unreal\\Environments\\Blocks update_from_git.bat start Blocks.sln Above commands first builds the AirSim plugin and then deploys it to Blocks project using handy update_from_git.bat . Now you can work inside Visual Studio solution, make changes to the code and just run F5 to build, run and test your changes. The debugging, break points etc should work as usual. After you are done with you code changes, you might want to push your changes back to AirSim repo or your own fork or you may deploy the new plugin to your custom Unreal project. To do this, go back to command prompt and first update the AirSim repo folder: REM //Use x64 Native Tools Command Prompt for VS 2017 REM //run this from Unreal\\Environments\\Blocks update_to_git.bat build.cmd Above command will transfer your code changes from Unreal project folder back to Unreal\\Plugins folder. Now your changes are ready to be pushed to AirSim repo or your own fork. You can also copy Unreal\\Plugins to your custom Unreal engine project and see if everything works in your custom project as well. Take Away Once you understand how Unreal Build system and plugin model works as well as why we are doing above steps, you should feel quite comfortable in following this workflow. Don't be afraid of opening up .bat files to peek inside and see what its doing. They are quite minimal and straightforward (except, of course, build.cmd - don't look in to that one). FAQ I made changes in code in Blocks project but its not working. When you press F5 or F6 in Visual Studio to start build, the Unreal Build system kicks in and it tries to find out if any files are dirty and what it needs to build. Unfortunately, it often fails to recognize dirty files that is not the code that uses Unreal headers and object hierarchy. So, the trick is to just make some file dirty that Unreal Build system always recognizes. My favorite one is AirSimGameMode.cpp. Just insert a line, delete it and save the file. I made changes in the code outside of Visual Studio but its not working. Don't do that! Unreal Build system assumes that you are using Visual Studio and it does bunch of things to integrate with Visual Studio. If you do insist on using other editors then look up how to do command line builds in Unreal projects OR see docs on your editor on how it can integrate with Unreal build system OR run clean.bat + GenerateProjectFiles.bat to make sure VS solution is in sync. I'm trying to add new file in the Unreal Project and its not working. It won't! While you are indeed using Visual Studio solution, remember that this solution was actually generated by Unreal Build system. If you want to add new files in your project, first shut down Visual Studio, add an empty file at desired location and then run GenerateProjectFiles.bat which will scan all files in your project and then re-create the .sln file. Now open this new .sln file and you are in business. I copied Unreal\\Plugins folder but nothing happens in Unreal Project. First make sure your project's .uproject file is referencing the plugin. Then make sure you have run clean.bat and then GenerateProjectFiles.bat as described in Overview above. I have multiple Unreal projects with AirSim plugin. How do I update them easily? You are in luck! We have build_all_ue_projects.bat which exactly does that. Don't treat it as black box (at least not yet), open it up and see what it does. It has 4 variables that are being set from command line args. If these args is not supplied they are set to default values in next set of statements. You might want to change default values for the paths. This batch file builds AirSim plugin, deploys it to all listed projects (see CALL statements later in the batch file), runs packaging for those projects and puts final binaries in specified folder - all in one step! This is what we use to create our own binary releases. How do I contribute back to AirSim? Before making any changes make sure you have created your feature branch. After you test your code changes in Blocks environment, follow the usual steps to make contributions just like any other GitHub projects. If you are not familiar with Git Branch-Rebase-Merge workflow, please read this first .","title":"Development Workflow"},{"location":"dev_workflow/#development-workflow","text":"Below is the guide on how to perform different development activities while working with AirSim. If you are new to Unreal Engine based projects and want to contribute to AirSim or make your own forks for your custom requirements, this might save you some time.","title":"Development Workflow"},{"location":"dev_workflow/#development-environment","text":"","title":"Development Environment"},{"location":"dev_workflow/#os","text":"We highly recommend Windows 10 and Visual Studio 2019 as your development environment. The support for other OSes and IDE is unfortunately not as mature on the Unreal Engine side and you may risk severe loss of productivity trying to do workarounds and jumping through the hoops.","title":"OS"},{"location":"dev_workflow/#hardware","text":"We recommend GPUs such as NVidia 1080 or NVidia Titan series with powerful desktop such as one with 64GB RAM, 6+ cores, SSDs and 2-3 displays (ideally 4K). We have found HP Z840 work quite well for our needs. The development experience on high-end laptops is generally sub-par compared to powerful desktops however they might be useful in a pinch. You generally want laptops with discrete NVidia GPU (at least M2000 or better) with 64GB RAM, SSDs and hopefully 4K display. We have found models such as Lenovo P50 work well for our needs. Laptops with only integrated graphics might not work well.","title":"Hardware"},{"location":"dev_workflow/#updating-and-changing-airsim-code","text":"","title":"Updating and Changing AirSim Code"},{"location":"dev_workflow/#overview","text":"AirSim is designed as plugin. This means it can't run by itself, you need to put it in an Unreal project (we call it \"environment\"). So building and testing AirSim has two steps: (1) build the plugin (2) deploy plugin in Unreal project and run the project. The first step is accomplished by build.cmd available in AirSim root. This command will update everything you need for the plugin in the Unreal\\Plugins folder. So to deploy the plugin, you just need to copy Unreal\\Plugins folder in to your Unreal project folder. Next you should remove all intermediate files in your Unreal project and then regenerate .sln file for your Unreal project. To do this, we have two handy .bat files in Unreal\\Environments\\Blocks folder: clean.bat and GenerateProjectFiles.bat . So just run these bat files in sequence from root of your Unreal project. Now you are ready to open new .sln in Visual Studio and press F5 to run it.","title":"Overview"},{"location":"dev_workflow/#steps","text":"Below are the steps we use to make changes in AirSim and test them out. The best way to do development in AirSim code is to use Blocks project . This is the light weight project so compile time is relatively faster. Generally the workflow is, REM //Use x64 Native Tools Command Prompt for VS 2017 REM //Navigate to AirSim repo folder git pull build.cmd cd Unreal\\Environments\\Blocks update_from_git.bat start Blocks.sln Above commands first builds the AirSim plugin and then deploys it to Blocks project using handy update_from_git.bat . Now you can work inside Visual Studio solution, make changes to the code and just run F5 to build, run and test your changes. The debugging, break points etc should work as usual. After you are done with you code changes, you might want to push your changes back to AirSim repo or your own fork or you may deploy the new plugin to your custom Unreal project. To do this, go back to command prompt and first update the AirSim repo folder: REM //Use x64 Native Tools Command Prompt for VS 2017 REM //run this from Unreal\\Environments\\Blocks update_to_git.bat build.cmd Above command will transfer your code changes from Unreal project folder back to Unreal\\Plugins folder. Now your changes are ready to be pushed to AirSim repo or your own fork. You can also copy Unreal\\Plugins to your custom Unreal engine project and see if everything works in your custom project as well.","title":"Steps"},{"location":"dev_workflow/#take-away","text":"Once you understand how Unreal Build system and plugin model works as well as why we are doing above steps, you should feel quite comfortable in following this workflow. Don't be afraid of opening up .bat files to peek inside and see what its doing. They are quite minimal and straightforward (except, of course, build.cmd - don't look in to that one).","title":"Take Away"},{"location":"dev_workflow/#faq","text":"","title":"FAQ"},{"location":"dev_workflow/#i-made-changes-in-code-in-blocks-project-but-its-not-working","text":"When you press F5 or F6 in Visual Studio to start build, the Unreal Build system kicks in and it tries to find out if any files are dirty and what it needs to build. Unfortunately, it often fails to recognize dirty files that is not the code that uses Unreal headers and object hierarchy. So, the trick is to just make some file dirty that Unreal Build system always recognizes. My favorite one is AirSimGameMode.cpp. Just insert a line, delete it and save the file.","title":"I made changes in code in Blocks project but its not working."},{"location":"dev_workflow/#i-made-changes-in-the-code-outside-of-visual-studio-but-its-not-working","text":"Don't do that! Unreal Build system assumes that you are using Visual Studio and it does bunch of things to integrate with Visual Studio. If you do insist on using other editors then look up how to do command line builds in Unreal projects OR see docs on your editor on how it can integrate with Unreal build system OR run clean.bat + GenerateProjectFiles.bat to make sure VS solution is in sync.","title":"I made changes in the code outside of Visual Studio but its not working."},{"location":"dev_workflow/#im-trying-to-add-new-file-in-the-unreal-project-and-its-not-working","text":"It won't! While you are indeed using Visual Studio solution, remember that this solution was actually generated by Unreal Build system. If you want to add new files in your project, first shut down Visual Studio, add an empty file at desired location and then run GenerateProjectFiles.bat which will scan all files in your project and then re-create the .sln file. Now open this new .sln file and you are in business.","title":"I'm trying to add new file in the Unreal Project and its not working."},{"location":"dev_workflow/#i-copied-unrealplugins-folder-but-nothing-happens-in-unreal-project","text":"First make sure your project's .uproject file is referencing the plugin. Then make sure you have run clean.bat and then GenerateProjectFiles.bat as described in Overview above.","title":"I copied Unreal\\Plugins folder but nothing happens in Unreal Project."},{"location":"dev_workflow/#i-have-multiple-unreal-projects-with-airsim-plugin-how-do-i-update-them-easily","text":"You are in luck! We have build_all_ue_projects.bat which exactly does that. Don't treat it as black box (at least not yet), open it up and see what it does. It has 4 variables that are being set from command line args. If these args is not supplied they are set to default values in next set of statements. You might want to change default values for the paths. This batch file builds AirSim plugin, deploys it to all listed projects (see CALL statements later in the batch file), runs packaging for those projects and puts final binaries in specified folder - all in one step! This is what we use to create our own binary releases.","title":"I have multiple Unreal projects with AirSim plugin. How do I update them easily?"},{"location":"dev_workflow/#how-do-i-contribute-back-to-airsim","text":"Before making any changes make sure you have created your feature branch. After you test your code changes in Blocks environment, follow the usual steps to make contributions just like any other GitHub projects. If you are not familiar with Git Branch-Rebase-Merge workflow, please read this first .","title":"How do I contribute back to AirSim?"},{"location":"echo/","text":"How to Use Echo sensor modalities in AirSim AirSim supports Echo sensors for multirotors and cars. Echo sensors can be configured to behave like sonar, radar or other echo-based sensor types. The enablement of an echo sensor and the other settings can be configured via AirSimSettings json. Please see general sensors for information on configuration of general/shared sensor settings. Enabling echo sensor on a vehicle By default, echo sensors are not enabled. To enable one, set the SensorType and Enabled attributes in settings json. \"echo1\": { \"SensorType\": 7, \"Enabled\" : true, Multiple echo sensors can be enabled on a vehicle. Echo configuration The following parameters can be configured right now via settings json. Parameter Description NumberOfTraces Amount of traces (rays) being cast SensorOpeningAngle The angle for receiving signals on the sensor ReflectionOpeningAngle Opening angle of reflections AttenuationPerDistance Attenuation of signal wrt distance traveled (dB/m) AttenuationPerReflection Attenuation of signal wrt reflections (dB) AttenuationLimit Attenuation at which the signal is considered dissipated (dB) DistanceLimit Maximum distance a reflection can travel ReflectionLimit Maximum amount of reflections that can happen ReflectionDistanceLimit Maximum distance between two reflections MeasurementFrequency The frequency of the sensor (measurements/s) SensorDiameter The diameter of the sensor plane used to capture the reflecting traces (meter) PauseAfterMeasurement Pause the simulation after each measurement. Useful for API interaction to be synced X Y Z Position of the echo relative to the vehicle (in NED, in meters) Roll Pitch Yaw Orientation of the echo relative to the vehicle (in degrees, yaw-pitch-roll order to front vector +X) DrawReflectedPoints Draw debug points in world where reflected points are captured by the sensor DrawReflectedLines Draw debug lines in world from reflected points to the sensor DrawReflectedPaths Draw the full paths of the reflected points DrawInitialPoints Draw the points of the initial half sphere where the traces (rays) are cast DrawExternalPoints Draw a pointcloud coming through the API from an external source DrawBounceLines Draw lines of all bouncing reflections of the traces with their color depending on attenuation DrawSensor Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is IgnoreMarked Remove objects with the Unreal Tag MarkedIgnore from the sensor data External Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates ExternalLocal When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default e.g., { \"SeeDocsAt\": \"settings_json.md\", \"SettingsVersion\": 1.2, \"SimMode\": \"SkidVehicle\", \"Vehicles\": { \"CPHusky\": { \"VehicleType\": \"CPHusky\", \"AutoCreate\": true, \"Sensors\": { \"SonarSensor1\": { \"SensorType\": 7, \"Enabled\": true, \"X\": 0, \"Y\": 0, \"Z\": -0.55, \"Roll\": 0, \"Pitch\": 0, \"Yaw\": 0, \"MeasurementFrequency\": 5, \"NumberOfTraces\": 30000, \"DistanceLimit\": 5, \"SensorDiameter\": 0.1, \"SensorOpeningAngle\": 180, \"Wavelength\": 0.01, \"AttenuationLimit\": -100, \"ReflectionDepth\": 3, \"ReflectionDistanceLimit\": 1, \"ReflectionOpeningAngle\": 10, \"DrawInitialPoints\": false, \"DrawBounceLines\": false, \"DrawReflectedPoints\": true, \"DrawReflectedLines\": false, \"DrawReflectedPaths\": false, \"DrawExternalPoints\": false, \"DrawSensor\": false } } } } } Client API Use getEchoData(sensor name, vehicle name) API to retrieve the echo sensor data. * The API returns a Point-Cloud as a flat array of floats, the final attenuation, total distance along with the timestamp of the capture and sensor pose. * Point-Cloud: * The floats represent [x, y, z, attenuation, total_distance] for each point hit within the range in the last scan. * Echo Pose: * Default: Echo sensor pose in the vehicle frame. * External: If set to External (see table) the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true . Use setEchoData(sensor name, vehicle name, echo data) API to render an external pointcloud back to the simulation. It expects it to be [x,y,z] as a flat array of floats.","title":"Pulse Echo"},{"location":"echo/#how-to-use-echo-sensor-modalities-in-airsim","text":"AirSim supports Echo sensors for multirotors and cars. Echo sensors can be configured to behave like sonar, radar or other echo-based sensor types. The enablement of an echo sensor and the other settings can be configured via AirSimSettings json. Please see general sensors for information on configuration of general/shared sensor settings.","title":"How to Use Echo sensor modalities in AirSim"},{"location":"echo/#enabling-echo-sensor-on-a-vehicle","text":"By default, echo sensors are not enabled. To enable one, set the SensorType and Enabled attributes in settings json. \"echo1\": { \"SensorType\": 7, \"Enabled\" : true, Multiple echo sensors can be enabled on a vehicle.","title":"Enabling echo sensor on a vehicle"},{"location":"echo/#echo-configuration","text":"The following parameters can be configured right now via settings json. Parameter Description NumberOfTraces Amount of traces (rays) being cast SensorOpeningAngle The angle for receiving signals on the sensor ReflectionOpeningAngle Opening angle of reflections AttenuationPerDistance Attenuation of signal wrt distance traveled (dB/m) AttenuationPerReflection Attenuation of signal wrt reflections (dB) AttenuationLimit Attenuation at which the signal is considered dissipated (dB) DistanceLimit Maximum distance a reflection can travel ReflectionLimit Maximum amount of reflections that can happen ReflectionDistanceLimit Maximum distance between two reflections MeasurementFrequency The frequency of the sensor (measurements/s) SensorDiameter The diameter of the sensor plane used to capture the reflecting traces (meter) PauseAfterMeasurement Pause the simulation after each measurement. Useful for API interaction to be synced X Y Z Position of the echo relative to the vehicle (in NED, in meters) Roll Pitch Yaw Orientation of the echo relative to the vehicle (in degrees, yaw-pitch-roll order to front vector +X) DrawReflectedPoints Draw debug points in world where reflected points are captured by the sensor DrawReflectedLines Draw debug lines in world from reflected points to the sensor DrawReflectedPaths Draw the full paths of the reflected points DrawInitialPoints Draw the points of the initial half sphere where the traces (rays) are cast DrawExternalPoints Draw a pointcloud coming through the API from an external source DrawBounceLines Draw lines of all bouncing reflections of the traces with their color depending on attenuation DrawSensor Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is IgnoreMarked Remove objects with the Unreal Tag MarkedIgnore from the sensor data External Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates ExternalLocal When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default e.g., { \"SeeDocsAt\": \"settings_json.md\", \"SettingsVersion\": 1.2, \"SimMode\": \"SkidVehicle\", \"Vehicles\": { \"CPHusky\": { \"VehicleType\": \"CPHusky\", \"AutoCreate\": true, \"Sensors\": { \"SonarSensor1\": { \"SensorType\": 7, \"Enabled\": true, \"X\": 0, \"Y\": 0, \"Z\": -0.55, \"Roll\": 0, \"Pitch\": 0, \"Yaw\": 0, \"MeasurementFrequency\": 5, \"NumberOfTraces\": 30000, \"DistanceLimit\": 5, \"SensorDiameter\": 0.1, \"SensorOpeningAngle\": 180, \"Wavelength\": 0.01, \"AttenuationLimit\": -100, \"ReflectionDepth\": 3, \"ReflectionDistanceLimit\": 1, \"ReflectionOpeningAngle\": 10, \"DrawInitialPoints\": false, \"DrawBounceLines\": false, \"DrawReflectedPoints\": true, \"DrawReflectedLines\": false, \"DrawReflectedPaths\": false, \"DrawExternalPoints\": false, \"DrawSensor\": false } } } } }","title":"Echo configuration"},{"location":"echo/#client-api","text":"Use getEchoData(sensor name, vehicle name) API to retrieve the echo sensor data. * The API returns a Point-Cloud as a flat array of floats, the final attenuation, total distance along with the timestamp of the capture and sensor pose. * Point-Cloud: * The floats represent [x, y, z, attenuation, total_distance] for each point hit within the range in the last scan. * Echo Pose: * Default: Echo sensor pose in the vehicle frame. * External: If set to External (see table) the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true . Use setEchoData(sensor name, vehicle name, echo data) API to render an external pointcloud back to the simulation. It expects it to be [x,y,z] as a flat array of floats.","title":"Client API"},{"location":"faq/","text":"FAQ General Unreal editor is slow when it is not the active window Go to Edit/Editor Preferences, select \"All Settings\" and type \"CPU\" in the search box. It should find the setting titled \"Use Less CPU when in Background\", and you want to uncheck this checkbox. My mouse disappears in Unreal Yes, Unreal steals the mouse, and we don't draw one. So to get your mouse back just use Alt+TAB to switch to a different window. To avoid this entirely, go to Project settings in Unreal Editor, go to Input tab and disable all settings for mouse capture. Where is the setting file and how do I modify it? AirSim will create empty settings file at ~/Documents/AirSim/settings.json . You can view the available settings options . How do I arm my drone? If you're using simple_flight, your vehicle is already armed and ready to fly. For PX4 you can arm by holding both sticks on remote control down and to the center. When making API call I get error If you are getting this error, TypeError: unsupported operand type(s) for *: 'AsyncIOLoop' and 'float' its probably due to upgraded version of tornado package with version > 5.0 in Python that conflicts with msgpack-rpc-python which requires tornado package < 5.0. To fix this you can update the package like this: pip install --upgrade msgpack-rpc-python But this might break something (for example, PyTorch 0.4+) because it will uninstall newer tornado and re-install older one. To avoid this you should create new conda environment . I'm getting Eigen not found error when compiling Unreal project. This is most likely because AirSim wasn't built and Plugin folder was copied in Unreal project folder. To fix this make sure you build AirSim first (run build.cmd in Windows). Something went wrong. How do I debug? First turn on C++ exceptions from the Exceptions window: and copy the stack trace of all exceptions you see there during execution that look relevant (for example, there might be an initial exception from VSPerf140 that you can ignore) then paste these call stacks into a new AirSim GitHub issue, thanks. What do the colors mean in the Segmentation View ? See Camera Views for information on the camera views and how to change them. Unreal 4.xx doesn't look as good as 4.yy Unreal 4.15 added the ability for Foliage LOD dithering to be disabled on a case-by-case basis by unchecking the Dithered LOD Transition checkbox in the foliage materials. Note that all materials used on all LODs need to have the checkbox checked in order for dithered LOD transitions to work. When checked the transition of generated foliage will be a lot smoother and will look better than 4.14. Can I use an XBox controller to fly? See XBox controller for details. How do I use AirSim with multiple vehicles? Here is multi-vehicle setup guide . What computer do you need? It depends on how big your Unreal Environment is. The Blocks environment that comes with AirSim is very basic and works on typical laptops. The Modular Neighborhood Pack that we use ourselves for research requires GPUs with at least 4GB of RAM. The Open World environment needs GPU with 8GB RAM. Our typical development machines have 32GB of RAM and NVIDIA TitanX and a fast hard drive . How do I report issues? It's a good idea to include your configuration like below. If you can also include logs, that could also expedite the investigation. Operating System: Windows 10 64bit CPU: Intel Core i7 GPU: Nvidia GTX 1080 RAM: 32 GB Flight Controller: Pixhawk v2 Remote Control: Futaba If you have modified the default ~/Document/AirSim/settings.json , please include your settings also. If you are using PX4 then try to capture log from MavLink or PX4 . File an issue through GitHub Issues . Others Linux Build FAQ Windows Build FAQ PX4 Setup FAQ Remote Control FAQ Unreal Blocks Environment FAQ Unreal Custom Environment FAQ","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#general","text":"","title":"General"},{"location":"faq/#unreal-editor-is-slow-when-it-is-not-the-active-window","text":"Go to Edit/Editor Preferences, select \"All Settings\" and type \"CPU\" in the search box. It should find the setting titled \"Use Less CPU when in Background\", and you want to uncheck this checkbox.","title":"Unreal editor is slow when it is not the active window"},{"location":"faq/#my-mouse-disappears-in-unreal","text":"Yes, Unreal steals the mouse, and we don't draw one. So to get your mouse back just use Alt+TAB to switch to a different window. To avoid this entirely, go to Project settings in Unreal Editor, go to Input tab and disable all settings for mouse capture.","title":"My mouse disappears in Unreal"},{"location":"faq/#where-is-the-setting-file-and-how-do-i-modify-it","text":"AirSim will create empty settings file at ~/Documents/AirSim/settings.json . You can view the available settings options .","title":"Where is the setting file and how do I modify it?"},{"location":"faq/#how-do-i-arm-my-drone","text":"If you're using simple_flight, your vehicle is already armed and ready to fly. For PX4 you can arm by holding both sticks on remote control down and to the center.","title":"How do I arm my drone?"},{"location":"faq/#when-making-api-call-i-get-error","text":"If you are getting this error, TypeError: unsupported operand type(s) for *: 'AsyncIOLoop' and 'float' its probably due to upgraded version of tornado package with version > 5.0 in Python that conflicts with msgpack-rpc-python which requires tornado package < 5.0. To fix this you can update the package like this: pip install --upgrade msgpack-rpc-python But this might break something (for example, PyTorch 0.4+) because it will uninstall newer tornado and re-install older one. To avoid this you should create new conda environment .","title":"When making API call I get error"},{"location":"faq/#im-getting-eigen-not-found-error-when-compiling-unreal-project","text":"This is most likely because AirSim wasn't built and Plugin folder was copied in Unreal project folder. To fix this make sure you build AirSim first (run build.cmd in Windows).","title":"I'm getting Eigen not found error when compiling Unreal project."},{"location":"faq/#something-went-wrong-how-do-i-debug","text":"First turn on C++ exceptions from the Exceptions window: and copy the stack trace of all exceptions you see there during execution that look relevant (for example, there might be an initial exception from VSPerf140 that you can ignore) then paste these call stacks into a new AirSim GitHub issue, thanks.","title":"Something went wrong. How do I debug?"},{"location":"faq/#what-do-the-colors-mean-in-the-segmentation-view","text":"See Camera Views for information on the camera views and how to change them.","title":"What do the colors mean in the Segmentation View ?"},{"location":"faq/#unreal-4xx-doesnt-look-as-good-as-4yy","text":"Unreal 4.15 added the ability for Foliage LOD dithering to be disabled on a case-by-case basis by unchecking the Dithered LOD Transition checkbox in the foliage materials. Note that all materials used on all LODs need to have the checkbox checked in order for dithered LOD transitions to work. When checked the transition of generated foliage will be a lot smoother and will look better than 4.14.","title":"Unreal 4.xx doesn't look as good as 4.yy"},{"location":"faq/#can-i-use-an-xbox-controller-to-fly","text":"See XBox controller for details.","title":"Can I use an XBox controller to fly?"},{"location":"faq/#how-do-i-use-airsim-with-multiple-vehicles","text":"Here is multi-vehicle setup guide .","title":"How do I use AirSim with multiple vehicles?"},{"location":"faq/#what-computer-do-you-need","text":"It depends on how big your Unreal Environment is. The Blocks environment that comes with AirSim is very basic and works on typical laptops. The Modular Neighborhood Pack that we use ourselves for research requires GPUs with at least 4GB of RAM. The Open World environment needs GPU with 8GB RAM. Our typical development machines have 32GB of RAM and NVIDIA TitanX and a fast hard drive .","title":"What computer do you need?"},{"location":"faq/#how-do-i-report-issues","text":"It's a good idea to include your configuration like below. If you can also include logs, that could also expedite the investigation. Operating System: Windows 10 64bit CPU: Intel Core i7 GPU: Nvidia GTX 1080 RAM: 32 GB Flight Controller: Pixhawk v2 Remote Control: Futaba If you have modified the default ~/Document/AirSim/settings.json , please include your settings also. If you are using PX4 then try to capture log from MavLink or PX4 . File an issue through GitHub Issues .","title":"How do I report issues?"},{"location":"faq/#others","text":"Linux Build FAQ Windows Build FAQ PX4 Setup FAQ Remote Control FAQ Unreal Blocks Environment FAQ Unreal Custom Environment FAQ","title":"Others"},{"location":"flight_controller/","text":"Flight Controller What is Flight Controller? \"Wait!\" you ask, \"Why do you need flight controller for a simulator?\". The primary job of flight controller is to take in desired state as input, estimate actual state using sensors data and then drive the actuators in such a way so that actual state comes as close to the desired state. For quadrotors, desired state can be specified as roll, pitch and yaw, for example. It then estimates actual roll, pitch and yaw using gyroscope and accelerometer. Then it generates appropriate motor signals so actual state becomes desired state. You can find more in-depth in our paper . How Simulator uses Flight Controller? Simulator consumes the motor signals generated by flight controller to figure out force and thrust generated by each actuator (i.e. propellers in case of quadrotor). This is then used by the physics engine to compute the kinetic properties of the vehicle. This in turn generates simulated sensor data and feed it back to the flight controller. You can find more in-depth in our paper . What is Hardware- and Software-in-Loop? Hardware-in-Loop (HITL or HIL) means flight controller runs in actual hardware such as Naze32 or Pixhawk chip. You then connect this hardware to PC using USB port. Simulator talks to the device to retrieve actuator signals and send it simulated sensor data. This is obviously as close as you can get to real thing. However, it typically requires more steps to set up and usually hard to debug. One big issue is that simulator clock and device clock runs on their own speed and accuracy. Also, USB connection (which is usually only USB 2.0) may not be enough for real-time communication. In \"software-in-loop\" simulation (SITL or SIL) mode the firmware runs in your computer as opposed to separate board. This is generally fine except that now you are not touching any code paths that are specific to your device. Also, none of your code now runs with real-time clock usually provided by specialized hardware board. For well-designed flight controllers with software clock, these are usually not concerning issues. What Flight Controllers are Supported? AirSim has built-in flight controller called simple_flight and it is used by default. You don't need to do anything to use or configure it. AirSim also supports PX4 as another flight controller for advanced users. In the future, we also plan to support ROSFlight and Hackflight . ## Using AirSim Without Flight Controller Yes, now it's possible to use AirSim without flight controller. Please see the instructions here for how to use so-called \"Computer Vision\" mode. If you don't need vehicle dynamics, we highly recommend using this mode.","title":"Flight Controller"},{"location":"flight_controller/#flight-controller","text":"","title":"Flight Controller"},{"location":"flight_controller/#what-is-flight-controller","text":"\"Wait!\" you ask, \"Why do you need flight controller for a simulator?\". The primary job of flight controller is to take in desired state as input, estimate actual state using sensors data and then drive the actuators in such a way so that actual state comes as close to the desired state. For quadrotors, desired state can be specified as roll, pitch and yaw, for example. It then estimates actual roll, pitch and yaw using gyroscope and accelerometer. Then it generates appropriate motor signals so actual state becomes desired state. You can find more in-depth in our paper .","title":"What is Flight Controller?"},{"location":"flight_controller/#how-simulator-uses-flight-controller","text":"Simulator consumes the motor signals generated by flight controller to figure out force and thrust generated by each actuator (i.e. propellers in case of quadrotor). This is then used by the physics engine to compute the kinetic properties of the vehicle. This in turn generates simulated sensor data and feed it back to the flight controller. You can find more in-depth in our paper .","title":"How Simulator uses Flight Controller?"},{"location":"flight_controller/#what-is-hardware-and-software-in-loop","text":"Hardware-in-Loop (HITL or HIL) means flight controller runs in actual hardware such as Naze32 or Pixhawk chip. You then connect this hardware to PC using USB port. Simulator talks to the device to retrieve actuator signals and send it simulated sensor data. This is obviously as close as you can get to real thing. However, it typically requires more steps to set up and usually hard to debug. One big issue is that simulator clock and device clock runs on their own speed and accuracy. Also, USB connection (which is usually only USB 2.0) may not be enough for real-time communication. In \"software-in-loop\" simulation (SITL or SIL) mode the firmware runs in your computer as opposed to separate board. This is generally fine except that now you are not touching any code paths that are specific to your device. Also, none of your code now runs with real-time clock usually provided by specialized hardware board. For well-designed flight controllers with software clock, these are usually not concerning issues.","title":"What is Hardware- and Software-in-Loop?"},{"location":"flight_controller/#what-flight-controllers-are-supported","text":"AirSim has built-in flight controller called simple_flight and it is used by default. You don't need to do anything to use or configure it. AirSim also supports PX4 as another flight controller for advanced users. In the future, we also plan to support ROSFlight and Hackflight . ## Using AirSim Without Flight Controller Yes, now it's possible to use AirSim without flight controller. Please see the instructions here for how to use so-called \"Computer Vision\" mode. If you don't need vehicle dynamics, we highly recommend using this mode.","title":"What Flight Controllers are Supported?"},{"location":"gpulidar/","text":"How to Use GPU LiDAR in AirSim AirSim supports a GPU accelerated LiDAR for multirotors and cars. It uses a depth camera that rotates around to simulate a LiDAR while exploiting the GPU to do most of the work. Should allow for a large increase in amount of points that can be simulated. The enablement of a GPU LiDAR and the other LiDAR settings can be configured via AirSimSettings json. Please see general sensors for information on configuration of general/shared sensor settings. Note that this sensor type is currently not supported for Multirotor mode. It only works for Car and Computervision. Enabling GPU LiDAR on a vehicle By default, GPU LiDARs are not enabled. To enable the sensor, set the SensorType and Enabled attributes in settings json. \"GPULidar1\": { \"SensorType\": 8, \"Enabled\" : true, Multiple GPU LiDARs can be enabled on a vehicle. But one has to turn off DrawDebugPoints! Ignoring glass and other material types One can set an object that should be invisible to LIDAR sensors (such as glass) by giving them an Unreal Tag called LidarIgnore . GPU LiDAR configuration The following parameters can be configured right now via settings json. For some more information check the publication on this topic here . Parameter Description NumberOfChannels Number of channels/lasers of the LiDAR Range Range, in meters MeasurementsPerCycle amount of measurements in one full cycle (horizontal resolution) RotationsPerSecond Rotations per second Resolution Defines the resolution of the depth camera image that generates the LiDAR point cloud HorizontalFOVStart Horizontal FOV start for the LiDAR, in degrees HorizontalFOVEnd Horizontal FOV end for the LiDAR, in degrees VerticalFOVUpper Vertical FOV upper limit for the LiDAR, in degrees VerticalFOVLower Vertical FOV lower limit for the LiDAR, in degrees X Y Z Position of the LiDAR relative to the vehicle (in NED, in meters) Roll Pitch Yaw Orientation of the LiDAR relative to the vehicle (in degrees, yaw-pitch-roll order to front vector +X) IgnoreMarked Remove objects with the Unreal Tag MarkedIgnore from the sensor data GroundTruth Generate ground truth segmentation color values DrawSensor Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is External Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates in NED format from the settings file. ExternalLocal When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default GenerateIntensity Toggle intensity calculation on or off. This requires a surface material map to be available. See below for more information. rangeMaxLambertianPercentage Lambertian reflectivity percentage to max out on. Will act linear to 0% for below. rainMaxIntensity Rain intensity maximum to scale from in mm/hour. rainConstantA Constant one to to calculate the extinction coefficient in rain rainConstantB Constant one to to calculate the extinction coefficient in rain { \"SeeDocsAt\": \"https://github.com/Cosys-Lab/Cosys-AirSim/tree/main/docs/settings.md\", \"SettingsVersion\": 1.2, \"SimMode\": \"SkidVehicle\", \"Vehicles\": { \"airsimvehicle\": { \"VehicleType\": \"CPHusky\", \"AutoCreate\": true, \"Sensors\": { \"gpulidar1\": { \"SensorType\": 8, \"Enabled\" : true, \"External\": false, \"NumberOfChannels\": 128, \"Range\": 50, \"Resolution\": 512, \"RotationsPerSecond\": 10, \"MeasurementsPerCycle\": 1024, \"X\": 0, \"Y\": 0, \"Z\": -0.3, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"VerticalFOVUpper\": 45, \"VerticalFOVLower\": -45, \"HorizontalFOVStart\": 0, \"HorizontalFOVEnd\": 360, \"DrawDebugPoints\": true, \"DrawMode\": 4, \"Resolution\": 1024, \"IgnoreMarked\": true, \"GroundTruth\": false \"GenerateIntensity\": true, \"rangeMaxLambertianPercentage\": 80, \"rainMaxIntensity\": 70, \"rainConstantA\": 0.01, \"rainConstantB\": 0.6 } } } } } Intensity Surface Material map If 'GenerateIntensity' is enabled in the settings json, a surface material map is required. This map is used to calculate the intensity of the LiDAR points. e.g.: wood,0.9 alluminium,0.5 concrete,0.3 asphalt,0.1 This needs to be saved as 'materials.csv' in your documents folder where also your settings json file resides. Server side visualization for debugging By default, the LiDAR points are not drawn on the viewport. To enable the drawing of hit laser points on the viewport, please enable setting 'DrawDebugPoints' via settings json. This is only for testing purposes and will affect the data slightly. It also needs to be disabled when using multiple LiDAR sensors to avoid artifacts!! e.g.: \"Lidar1\": { ... \"DrawDebugPoints\": true }, You can also tweak the variation of debugging with the 'DrawMode' parameter: * 0 = no coloring * 1 = instance segmentation * 2 = material * 3 = impact angle * 4 = intensity e.g.: \"Lidar1\": { ... \"DrawDebugPoints\": true, \"DrawMode\": 4 }, Client API Use getGPULidarData(sensor name, vehicle name) API to retrieve the GPU LiDAR data. The API returns a Point-Cloud as a flat array of floats along with the timestamp of the capture and LiDAR pose. Point-Cloud: The floats represent [x,y,z, rgb, intensity] coordinate for each point hit within the range in the last scan. [x,y,z] represent the coordinates of each detected point in the local sensor frame. rgb represents a float32 representation of the RGB8 value that is linked to the instance segmentation system. See the Image API documentation and the instance segmentation documentation . The float32 comes from binary concatenation of the RGB8 values : rgb = value_segmentation.R << 16 | value_segmentation.G << 8 | value_segmentation.B \\ It can be retrieved from the API and converted back to RGB8 with for example the following Python code: LiDAR_data = client.getGPULidarData('LiDAR', 'vehicle') points = np.array(LiDAR_data.point_cloud, dtype=np.dtype('f4')) points = np.reshape(points, (int(points.shape[0] / 5), 5)) rgb_values = points[:, 3].astype(np.uint32) rgb = np.zeros((np.shape(points)[0], 3)) xyz = points[:, 0:3] for index, rgb_value in enumerate(rgb_values): rgb[index, 0] = (rgb_value >> 16) & 0xFF rgb[index, 1] = (rgb_value >> 8) & 0xFF rgb[index, 2] = rgb_value & 0xFF intensity represents the reflection strength as a float. Pose: Default: sensor pose in the vehicle frame. External: If set to External (see table) the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true .","title":"GPU LIDAR"},{"location":"gpulidar/#how-to-use-gpu-lidar-in-airsim","text":"AirSim supports a GPU accelerated LiDAR for multirotors and cars. It uses a depth camera that rotates around to simulate a LiDAR while exploiting the GPU to do most of the work. Should allow for a large increase in amount of points that can be simulated. The enablement of a GPU LiDAR and the other LiDAR settings can be configured via AirSimSettings json. Please see general sensors for information on configuration of general/shared sensor settings. Note that this sensor type is currently not supported for Multirotor mode. It only works for Car and Computervision.","title":"How to Use GPU LiDAR in AirSim"},{"location":"gpulidar/#enabling-gpu-lidar-on-a-vehicle","text":"By default, GPU LiDARs are not enabled. To enable the sensor, set the SensorType and Enabled attributes in settings json. \"GPULidar1\": { \"SensorType\": 8, \"Enabled\" : true, Multiple GPU LiDARs can be enabled on a vehicle. But one has to turn off DrawDebugPoints!","title":"Enabling GPU LiDAR on a vehicle"},{"location":"gpulidar/#ignoring-glass-and-other-material-types","text":"One can set an object that should be invisible to LIDAR sensors (such as glass) by giving them an Unreal Tag called LidarIgnore .","title":"Ignoring glass and other material types"},{"location":"gpulidar/#gpu-lidar-configuration","text":"The following parameters can be configured right now via settings json. For some more information check the publication on this topic here . Parameter Description NumberOfChannels Number of channels/lasers of the LiDAR Range Range, in meters MeasurementsPerCycle amount of measurements in one full cycle (horizontal resolution) RotationsPerSecond Rotations per second Resolution Defines the resolution of the depth camera image that generates the LiDAR point cloud HorizontalFOVStart Horizontal FOV start for the LiDAR, in degrees HorizontalFOVEnd Horizontal FOV end for the LiDAR, in degrees VerticalFOVUpper Vertical FOV upper limit for the LiDAR, in degrees VerticalFOVLower Vertical FOV lower limit for the LiDAR, in degrees X Y Z Position of the LiDAR relative to the vehicle (in NED, in meters) Roll Pitch Yaw Orientation of the LiDAR relative to the vehicle (in degrees, yaw-pitch-roll order to front vector +X) IgnoreMarked Remove objects with the Unreal Tag MarkedIgnore from the sensor data GroundTruth Generate ground truth segmentation color values DrawSensor Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is External Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates in NED format from the settings file. ExternalLocal When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default GenerateIntensity Toggle intensity calculation on or off. This requires a surface material map to be available. See below for more information. rangeMaxLambertianPercentage Lambertian reflectivity percentage to max out on. Will act linear to 0% for below. rainMaxIntensity Rain intensity maximum to scale from in mm/hour. rainConstantA Constant one to to calculate the extinction coefficient in rain rainConstantB Constant one to to calculate the extinction coefficient in rain { \"SeeDocsAt\": \"https://github.com/Cosys-Lab/Cosys-AirSim/tree/main/docs/settings.md\", \"SettingsVersion\": 1.2, \"SimMode\": \"SkidVehicle\", \"Vehicles\": { \"airsimvehicle\": { \"VehicleType\": \"CPHusky\", \"AutoCreate\": true, \"Sensors\": { \"gpulidar1\": { \"SensorType\": 8, \"Enabled\" : true, \"External\": false, \"NumberOfChannels\": 128, \"Range\": 50, \"Resolution\": 512, \"RotationsPerSecond\": 10, \"MeasurementsPerCycle\": 1024, \"X\": 0, \"Y\": 0, \"Z\": -0.3, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"VerticalFOVUpper\": 45, \"VerticalFOVLower\": -45, \"HorizontalFOVStart\": 0, \"HorizontalFOVEnd\": 360, \"DrawDebugPoints\": true, \"DrawMode\": 4, \"Resolution\": 1024, \"IgnoreMarked\": true, \"GroundTruth\": false \"GenerateIntensity\": true, \"rangeMaxLambertianPercentage\": 80, \"rainMaxIntensity\": 70, \"rainConstantA\": 0.01, \"rainConstantB\": 0.6 } } } } }","title":"GPU LiDAR configuration"},{"location":"gpulidar/#intensity-surface-material-map","text":"If 'GenerateIntensity' is enabled in the settings json, a surface material map is required. This map is used to calculate the intensity of the LiDAR points. e.g.: wood,0.9 alluminium,0.5 concrete,0.3 asphalt,0.1 This needs to be saved as 'materials.csv' in your documents folder where also your settings json file resides.","title":"Intensity Surface Material map"},{"location":"gpulidar/#server-side-visualization-for-debugging","text":"By default, the LiDAR points are not drawn on the viewport. To enable the drawing of hit laser points on the viewport, please enable setting 'DrawDebugPoints' via settings json. This is only for testing purposes and will affect the data slightly. It also needs to be disabled when using multiple LiDAR sensors to avoid artifacts!! e.g.: \"Lidar1\": { ... \"DrawDebugPoints\": true }, You can also tweak the variation of debugging with the 'DrawMode' parameter: * 0 = no coloring * 1 = instance segmentation * 2 = material * 3 = impact angle * 4 = intensity e.g.: \"Lidar1\": { ... \"DrawDebugPoints\": true, \"DrawMode\": 4 },","title":"Server side visualization for debugging"},{"location":"gpulidar/#client-api","text":"Use getGPULidarData(sensor name, vehicle name) API to retrieve the GPU LiDAR data. The API returns a Point-Cloud as a flat array of floats along with the timestamp of the capture and LiDAR pose. Point-Cloud: The floats represent [x,y,z, rgb, intensity] coordinate for each point hit within the range in the last scan. [x,y,z] represent the coordinates of each detected point in the local sensor frame. rgb represents a float32 representation of the RGB8 value that is linked to the instance segmentation system. See the Image API documentation and the instance segmentation documentation . The float32 comes from binary concatenation of the RGB8 values : rgb = value_segmentation.R << 16 | value_segmentation.G << 8 | value_segmentation.B \\ It can be retrieved from the API and converted back to RGB8 with for example the following Python code: LiDAR_data = client.getGPULidarData('LiDAR', 'vehicle') points = np.array(LiDAR_data.point_cloud, dtype=np.dtype('f4')) points = np.reshape(points, (int(points.shape[0] / 5), 5)) rgb_values = points[:, 3].astype(np.uint32) rgb = np.zeros((np.shape(points)[0], 3)) xyz = points[:, 0:3] for index, rgb_value in enumerate(rgb_values): rgb[index, 0] = (rgb_value >> 16) & 0xFF rgb[index, 1] = (rgb_value >> 8) & 0xFF rgb[index, 2] = rgb_value & 0xFF intensity represents the reflection strength as a float. Pose: Default: sensor pose in the vehicle frame. External: If set to External (see table) the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true .","title":"Client API"},{"location":"hard_drive/","text":"Busy Hard Drive It is not required, but we recommend running your Unreal Environment on a Solid State Drive (SSD). Between debugging, logging, and Unreal asset loading the hard drive can become your bottle neck. It is normal that your hard drive will be slammed while Unreal is loading the environment, but if your hard drive performance looks like this while the Unreal game is running then you will probably not get a good flying experience. In fact, if the hard drive is this busy, chances are the drone will not fly properly at all. For some unknown reason this I/O bottle neck also interferes with the drone control loop and if that loop doesn't run at a high rate (300-500 Hz) then the drone will not fly. Not surprising, the control loop inside the PX4 firmware that runs on a Pixhawk flight controller runs at 1000 Hz. Reducing I/O If you can't whip off to Fry's Electronics and pick up an overpriced super fast SSD this weekend, then the following steps can be taken to reduce the hard drive I/O: First run the Unreal Environment using Cooked content outside of the UE Editor or any debugging environment, and package the content to your fastest SSD drive. You can do that using this menu option: If you must use the UE editor (because you are actively modifying game assets), then at least don't run that in a debugger. If you are using Visual Studio use start without debugging. If you must debug the app, and you are using Visual Studio debugger, stop then Visual Studio from logging Intellitrace information. Go to Tools/Options/Debugging/Intellitrace, and turn off the main checkbox. Turn off any Unreal Analytics that your environment may have enabled, especially any file logging. I/O from Page Faults If your system is running out of RAM it may start paging memory to disk. If your operating system has enabled paging to disk, make sure it is paging to your fastest SSD. Or if you have enough RAM disable paging all together. In fact, if you disable paging and the game stops working you will know for sure you are running out of RAM. Obviously, shutting down any other unnecessary apps should also free up memory so you don't run out. Ideal Runtime performance This is what my slow hard drive looks like when flying from UE editor. You can see it's very busy, but the drone still flies ok: This is what my fast SSD looks like when the drone is flying in an Unreal Cooked app (no UE editor, no debugger). Not surprisingly it is flying perfectly in this case:","title":"Tips for Busy HDD"},{"location":"hard_drive/#busy-hard-drive","text":"It is not required, but we recommend running your Unreal Environment on a Solid State Drive (SSD). Between debugging, logging, and Unreal asset loading the hard drive can become your bottle neck. It is normal that your hard drive will be slammed while Unreal is loading the environment, but if your hard drive performance looks like this while the Unreal game is running then you will probably not get a good flying experience. In fact, if the hard drive is this busy, chances are the drone will not fly properly at all. For some unknown reason this I/O bottle neck also interferes with the drone control loop and if that loop doesn't run at a high rate (300-500 Hz) then the drone will not fly. Not surprising, the control loop inside the PX4 firmware that runs on a Pixhawk flight controller runs at 1000 Hz.","title":"Busy Hard Drive"},{"location":"hard_drive/#reducing-io","text":"If you can't whip off to Fry's Electronics and pick up an overpriced super fast SSD this weekend, then the following steps can be taken to reduce the hard drive I/O: First run the Unreal Environment using Cooked content outside of the UE Editor or any debugging environment, and package the content to your fastest SSD drive. You can do that using this menu option: If you must use the UE editor (because you are actively modifying game assets), then at least don't run that in a debugger. If you are using Visual Studio use start without debugging. If you must debug the app, and you are using Visual Studio debugger, stop then Visual Studio from logging Intellitrace information. Go to Tools/Options/Debugging/Intellitrace, and turn off the main checkbox. Turn off any Unreal Analytics that your environment may have enabled, especially any file logging.","title":"Reducing I/O"},{"location":"hard_drive/#io-from-page-faults","text":"If your system is running out of RAM it may start paging memory to disk. If your operating system has enabled paging to disk, make sure it is paging to your fastest SSD. Or if you have enough RAM disable paging all together. In fact, if you disable paging and the game stops working you will know for sure you are running out of RAM. Obviously, shutting down any other unnecessary apps should also free up memory so you don't run out.","title":"I/O from Page Faults"},{"location":"hard_drive/#ideal-runtime-performance","text":"This is what my slow hard drive looks like when flying from UE editor. You can see it's very busy, but the drone still flies ok: This is what my fast SSD looks like when the drone is flying in an Unreal Cooked app (no UE editor, no debugger). Not surprisingly it is flying perfectly in this case:","title":"Ideal Runtime performance"},{"location":"image_apis/","text":"Image APIs Please read general API doc first if you are not familiar with AirSim APIs. Getting a Single Image Here's a sample code to get a single image from camera named \"0\". The returned value is bytes of png format image. To get uncompressed and other format as well as available cameras please see next sections. Python import airsim # for car use CarClient() client = airsim.MultirotorClient() png_image = client.simGetImage(\"0\", airsim.ImageType.Scene) # do something with image Getting Images with More Flexibility The simGetImages API which is slightly more complex to use than simGetImage API, for example, you can get left camera view, right camera view and depth image from left camera in a single API call. The simGetImages API also allows you to get uncompressed images as well as floating point single channel images (instead of 3 channel (RGB), each 8 bit). Python import airsim # for car use CarClient() client = airsim.MultirotorClient() responses = client.simGetImages([ # png format airsim.ImageRequest(0, airsim.ImageType.Scene), # uncompressed RGB array bytes airsim.ImageRequest(1, airsim.ImageType.Scene, False, False), # floating point uncompressed image airsim.ImageRequest(1, airsim.ImageType.DepthPlanner, True)]) # do something with response which contains image data, pose, timestamp etc Using AirSim Images with NumPy If you plan to use numpy for image manipulation, you should get uncompressed RGB image and then convert to numpy like this: responses = client.simGetImages([ImageRequest(\"0\", airsim.ImageType.Scene, False, False)]) response = responses[0] # get numpy array img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) # reshape array to 4 channel image array H X W X 4 img_rgb = img1d.reshape(response.height, response.width, 3) # original image is fliped vertically img_rgb = np.flipud(img_rgb) # write to png airsim.write_png(os.path.normpath(filename + '.png'), img_rgb) Quick Tips The API simGetImage returns binary string literal which means you can simply dump it in binary file to create a .png file. However if you want to process it in any other way than you can handy function airsim.string_to_uint8_array . This converts binary string literal to NumPy uint8 array. The API simGetImages can accept request for multiple image types from any cameras in single call. You can specify if image is png compressed, RGB uncompressed or float array. For png compressed images, you get binary string literal . For float array you get Python list of float64. You can convert this float array to NumPy 2D array using airsim.list_to_2d_float_array(response.image_data_float, response.width, response.height) You can also save float array to .pfm file (Portable Float Map format) using airsim.write_pfm() function. Ready to Run Complete Examples Python For a more complete ready to run sample code please see sample code in AirSimClient project for multirotors or HelloCar sample . This code also demonstrates simple activities such as saving images in files or using numpy to manipulate images. Available Cameras Car The cameras on car can be accessed by following names in API calls: front_center , front_right , front_left , fpv and back_center . Here FPV camera is driver's head position in the car. Multirotor The cameras in CV mode can be accessed by following names in API calls: front_center , front_right , front_left , bottom_center and back_center . Computer Vision Mode Camera names are same as in multirotor. Backward compatibility for camera names Before AirSim v1.2, cameras were accessed using ID numbers instead of names. For backward compatibility you can still use following ID numbers for above camera names in same order as above: \"0\" , \"1\" , \"2\" , \"3\" , \"4\" . In addition, camera name \"\" is also available to access the default camera which is generally the camera \"0\" . \"Computer Vision\" Mode You can use AirSim in so-called \"Computer Vision\" mode. In this mode, physics engine is disabled. It has a standard set of cameras and can have any sensor added similar to other vehicles. You can move around using keyboard (use F1 to see help on keys, additionally use left shift to go faster and spacebar to hold in place (handy for when moving camera manually). You can press Record button to continuously generate images. Or you can call APIs to move cameras around and take images. To active this mode, edit settings.json that you can find in your Documents\\AirSim folder (or ~/Documents/AirSim on Linux) and make sure following values exist at root level: { \"SettingsVersion\": 1.2, \"SimMode\": \"ComputerVision\" } Here's the Python code example to move camera around and capture images. This mode was inspired from UnrealCV project . Setting Pose in Computer Vision Mode To move around the environment using APIs you can use simSetVehiclePose API. This API takes position and orientation and sets that on the invisible vehicle where the front-center camera is located. All rest of the cameras move along keeping the relative position. If you don't want to change position (or orientation) then just set components of position (or orientation) to floating point nan values. The simGetVehiclePose allows to retrieve the current pose. You can also use simGetGroundTruthKinematics to get the quantities kinematics quantities for the movement. Many other non-vehicle specific APIs are also available such as segmentation APIs, collision APIs and camera APIs. Camera APIs The simGetCameraInfo returns the FOV(in degrees), projection matrix of a camera as well as the pose which can be: * Default: The pose of the camera in the vehicle frame. * External: If set to External the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true . The simSetCameraOrientation sets the orientation for the specified camera as quaternion in NED frame. The handy airsim.to_quaternion() function allows to convert pitch, roll, yaw to quaternion. For example, to set camera-0 to 15-degree pitch, you can use: client.simSetCameraOrientation(0, airsim.to_quaternion(0.261799, 0, 0)); #radians Gimbal You can set stabilization for pitch, roll or yaw for any camera using settings . Please see example usage . Changing Resolution and Camera Parameters To change resolution, FOV etc, you can use settings.json . For example, below addition in settings.json sets parameters for scene capture and uses \"Computer Vision\" mode described above. If you omit any setting then below default values will be used. For more information see settings doc . If you are using stereo camera, currently the distance between left and right is fixed at 25 cm. { \"SettingsVersion\": 1.2, \"CameraDefaults\": { \"CaptureSettings\": [ { \"ImageType\": 0, \"Width\": 256, \"Height\": 144, \"FOV_Degrees\": 90, \"AutoExposureBias\": 1.3, \"AutoExposureMaxBrightness\": 0.64, \"AutoExposureMinBrightness\": 0.03, \"MotionBlurAmount\": 1, \"MotionBlurMax\": 10, \"ChromaticAberrationScale\": 2 } ] }, \"SimMode\": \"ComputerVision\" } What Does Pixel Values Mean in Different Image Types? Available ImageType Values Scene = 0, DepthPlanner = 1, DepthPerspective = 2, DepthVis = 3, DisparityNormalized = 4, Segmentation = 5, SurfaceNormals = 6, Infrared = 7 DepthPlanner and DepthPerspective You normally want to retrieve the depth image as float (i.e. set pixels_as_float = true ) and specify ImageType = DepthPlanner or ImageType = DepthPerspective in ImageRequest . For ImageType = DepthPlanner , you get depth in camera plan, i.e., all points that are in plan parallel to camera have same depth. For ImageType = DepthPerspective , you get depth from camera using a projection ray that hits that pixel. Depending on your use case, planner depth or perspective depth may be the ground truth image that you want. For example, you may be able to feed perspective depth to ROS package such as depth_image_proc to generate a point cloud. Or planner depth may be more compatible with estimated depth image generated by stereo algorithms such as SGM. DepthVis When you specify ImageType = DepthVis in ImageRequest , you get an image that helps depth visualization. In this case, each pixel value is interpolated from black to white depending on depth in camera plane in meters. The pixels with pure white means depth of 100m or more while pure black means depth of 0 meters. DisparityNormalized You normally want to retrieve disparity image as float (i.e. set pixels_as_float = true and specify ImageType = DisparityNormalized in ImageRequest ) in which case each pixel is (Xl - Xr)/Xmax , which is thereby normalized to values between 0 to 1. Segmentation When you specify ImageType = Segmentation in ImageRequest , you get an image that gives you ground truth segmentation of the scene. At the startup, AirSim assigns a random color index to each mesh available in environment. The RGB values for each color index ID can be retrieved from the API. You can assign a specific value to a specific mesh using APIs. For example, below Python code sets the object ID for the mesh called \"Ground\" to 20 in Blocks environment and hence changes its color in Segmentation view to the 20th color of the segmentation colormap: Note that this will not do a check if this color is already assigned to a different object! success = client.simSetSegmentationObjectID(\"Ground\", 20) The return value is a boolean type that lets you know if the mesh was found. Notice that typical Unreal environments, like Blocks, usually have many other meshes that comprises of same object, for example, \"Ground_2\", \"Ground_3\" and so on. As it is tedious to set object ID for all of these meshes, AirSim also supports regular expressions. For example, the code below sets all meshes which have names starting with \"ground\" (ignoring case) to 21 with just one line: success = client.simSetSegmentationObjectID(\"ground[\\w]*\", 21, True) The return value is true if at least one mesh was found using regular expression matching. When wanting to retrieve the segmentation image through the API, it is recommended that you request uncompressed image using this API to ensure you get precise RGB values for segmentation image: responses = client.simGetImages([airsim.ImageRequest( \"front_center\", airsim.ImageType.Segmentation, False, False)]) img_rgb_string = responses[0].image_data_uint8 rgbarray = np.frombuffer(img_rgb_string, np.uint8) rgbarray_shaped = rgbarray.reshape((540,960,3)) rgbarray_shaped = rgbarray_shaped img = Image.fromarray(rgbarray_shaped, 'RGB') img.show() To retrieve the color map to know which color is assign to each color index you can use: colorMap = client.simGetSegmentationColorMap() An example can be found in segmentation_test.py . For a script that generates a full list of objects and their associated color, please see the script segmentation_generate_list.py . How to Find Mesh names? To get desired ground truth segmentation you will need to know the names of the meshes in your Unreal environment. To do this, you can use the API: currentObjectList = client.simListInstanceSegmentationObjects() This will use an understandable naming depending on the hierarchy the object belong to in the Unreal World (example box_2_fullpalletspawner_5_pallet_4 or door_window_door_38 ). Note that this provides a different result from simListSceneObjects() as this one will make a simple list of all Unreal Actors in the scene, without keeping the hierarchy in mind. An extension to simListInstanceSegmentationObjects() is simListInstanceSegmentationPoses(ned=True) which will retrieve the 3D object pose of each element in the same order as the first mentioned function. Startup Object IDs At the start, AirSim assigns color indexes to each object found in environment of type UStaticMeshComponent or USkinnedMeshComponent . It then makes an understandable naming depending on the hierarchy the object belong to in the Unreal World (example box_2_fullpalletspawner_5_pallet_4 or door_window_door_38 ). Getting Object ID for Mesh The simGetSegmentationObjectID API allows you get object ID for given mesh name. More information Please see the instance segmentation documentation for some more information on the segmentation system created by Cosys-Lab. Infrared Currently, this is just a map from object ID to grey scale 0-255. So any mesh with object ID 42 shows up with color (42, 42, 42). Please see segmentation section for more details on how to set object IDs. Typically noise setting can be applied for this image type to get slightly more realistic effect. We are still working on adding other infrared artifacts and any contributions are welcome. Example Code A complete example of setting vehicle positions at random locations and orientations and then taking images can be found in GenerateImageGenerator.hpp . This example generates specified number of stereo images and ground truth disparity image and saving it to pfm format .","title":"Image APIs"},{"location":"image_apis/#image-apis","text":"Please read general API doc first if you are not familiar with AirSim APIs.","title":"Image APIs"},{"location":"image_apis/#getting-a-single-image","text":"Here's a sample code to get a single image from camera named \"0\". The returned value is bytes of png format image. To get uncompressed and other format as well as available cameras please see next sections.","title":"Getting a Single Image"},{"location":"image_apis/#python","text":"import airsim # for car use CarClient() client = airsim.MultirotorClient() png_image = client.simGetImage(\"0\", airsim.ImageType.Scene) # do something with image","title":"Python"},{"location":"image_apis/#getting-images-with-more-flexibility","text":"The simGetImages API which is slightly more complex to use than simGetImage API, for example, you can get left camera view, right camera view and depth image from left camera in a single API call. The simGetImages API also allows you to get uncompressed images as well as floating point single channel images (instead of 3 channel (RGB), each 8 bit).","title":"Getting Images with More Flexibility"},{"location":"image_apis/#python_1","text":"import airsim # for car use CarClient() client = airsim.MultirotorClient() responses = client.simGetImages([ # png format airsim.ImageRequest(0, airsim.ImageType.Scene), # uncompressed RGB array bytes airsim.ImageRequest(1, airsim.ImageType.Scene, False, False), # floating point uncompressed image airsim.ImageRequest(1, airsim.ImageType.DepthPlanner, True)]) # do something with response which contains image data, pose, timestamp etc","title":"Python"},{"location":"image_apis/#using-airsim-images-with-numpy","text":"If you plan to use numpy for image manipulation, you should get uncompressed RGB image and then convert to numpy like this: responses = client.simGetImages([ImageRequest(\"0\", airsim.ImageType.Scene, False, False)]) response = responses[0] # get numpy array img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) # reshape array to 4 channel image array H X W X 4 img_rgb = img1d.reshape(response.height, response.width, 3) # original image is fliped vertically img_rgb = np.flipud(img_rgb) # write to png airsim.write_png(os.path.normpath(filename + '.png'), img_rgb)","title":"Using AirSim Images with NumPy"},{"location":"image_apis/#quick-tips","text":"The API simGetImage returns binary string literal which means you can simply dump it in binary file to create a .png file. However if you want to process it in any other way than you can handy function airsim.string_to_uint8_array . This converts binary string literal to NumPy uint8 array. The API simGetImages can accept request for multiple image types from any cameras in single call. You can specify if image is png compressed, RGB uncompressed or float array. For png compressed images, you get binary string literal . For float array you get Python list of float64. You can convert this float array to NumPy 2D array using airsim.list_to_2d_float_array(response.image_data_float, response.width, response.height) You can also save float array to .pfm file (Portable Float Map format) using airsim.write_pfm() function.","title":"Quick Tips"},{"location":"image_apis/#ready-to-run-complete-examples","text":"","title":"Ready to Run Complete Examples"},{"location":"image_apis/#python_2","text":"For a more complete ready to run sample code please see sample code in AirSimClient project for multirotors or HelloCar sample . This code also demonstrates simple activities such as saving images in files or using numpy to manipulate images.","title":"Python"},{"location":"image_apis/#available-cameras","text":"","title":"Available Cameras"},{"location":"image_apis/#car","text":"The cameras on car can be accessed by following names in API calls: front_center , front_right , front_left , fpv and back_center . Here FPV camera is driver's head position in the car.","title":"Car"},{"location":"image_apis/#multirotor","text":"The cameras in CV mode can be accessed by following names in API calls: front_center , front_right , front_left , bottom_center and back_center .","title":"Multirotor"},{"location":"image_apis/#computer-vision-mode","text":"Camera names are same as in multirotor.","title":"Computer Vision Mode"},{"location":"image_apis/#backward-compatibility-for-camera-names","text":"Before AirSim v1.2, cameras were accessed using ID numbers instead of names. For backward compatibility you can still use following ID numbers for above camera names in same order as above: \"0\" , \"1\" , \"2\" , \"3\" , \"4\" . In addition, camera name \"\" is also available to access the default camera which is generally the camera \"0\" .","title":"Backward compatibility for camera names"},{"location":"image_apis/#computer-vision-mode_1","text":"You can use AirSim in so-called \"Computer Vision\" mode. In this mode, physics engine is disabled. It has a standard set of cameras and can have any sensor added similar to other vehicles. You can move around using keyboard (use F1 to see help on keys, additionally use left shift to go faster and spacebar to hold in place (handy for when moving camera manually). You can press Record button to continuously generate images. Or you can call APIs to move cameras around and take images. To active this mode, edit settings.json that you can find in your Documents\\AirSim folder (or ~/Documents/AirSim on Linux) and make sure following values exist at root level: { \"SettingsVersion\": 1.2, \"SimMode\": \"ComputerVision\" } Here's the Python code example to move camera around and capture images. This mode was inspired from UnrealCV project .","title":"\"Computer Vision\" Mode"},{"location":"image_apis/#setting-pose-in-computer-vision-mode","text":"To move around the environment using APIs you can use simSetVehiclePose API. This API takes position and orientation and sets that on the invisible vehicle where the front-center camera is located. All rest of the cameras move along keeping the relative position. If you don't want to change position (or orientation) then just set components of position (or orientation) to floating point nan values. The simGetVehiclePose allows to retrieve the current pose. You can also use simGetGroundTruthKinematics to get the quantities kinematics quantities for the movement. Many other non-vehicle specific APIs are also available such as segmentation APIs, collision APIs and camera APIs.","title":"Setting Pose in Computer Vision Mode"},{"location":"image_apis/#camera-apis","text":"The simGetCameraInfo returns the FOV(in degrees), projection matrix of a camera as well as the pose which can be: * Default: The pose of the camera in the vehicle frame. * External: If set to External the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true . The simSetCameraOrientation sets the orientation for the specified camera as quaternion in NED frame. The handy airsim.to_quaternion() function allows to convert pitch, roll, yaw to quaternion. For example, to set camera-0 to 15-degree pitch, you can use: client.simSetCameraOrientation(0, airsim.to_quaternion(0.261799, 0, 0)); #radians","title":"Camera APIs"},{"location":"image_apis/#gimbal","text":"You can set stabilization for pitch, roll or yaw for any camera using settings . Please see example usage .","title":"Gimbal"},{"location":"image_apis/#changing-resolution-and-camera-parameters","text":"To change resolution, FOV etc, you can use settings.json . For example, below addition in settings.json sets parameters for scene capture and uses \"Computer Vision\" mode described above. If you omit any setting then below default values will be used. For more information see settings doc . If you are using stereo camera, currently the distance between left and right is fixed at 25 cm. { \"SettingsVersion\": 1.2, \"CameraDefaults\": { \"CaptureSettings\": [ { \"ImageType\": 0, \"Width\": 256, \"Height\": 144, \"FOV_Degrees\": 90, \"AutoExposureBias\": 1.3, \"AutoExposureMaxBrightness\": 0.64, \"AutoExposureMinBrightness\": 0.03, \"MotionBlurAmount\": 1, \"MotionBlurMax\": 10, \"ChromaticAberrationScale\": 2 } ] }, \"SimMode\": \"ComputerVision\" }","title":"Changing Resolution and Camera Parameters"},{"location":"image_apis/#what-does-pixel-values-mean-in-different-image-types","text":"","title":"What Does Pixel Values Mean in Different Image Types?"},{"location":"image_apis/#available-imagetype-values","text":"Scene = 0, DepthPlanner = 1, DepthPerspective = 2, DepthVis = 3, DisparityNormalized = 4, Segmentation = 5, SurfaceNormals = 6, Infrared = 7","title":"Available ImageType Values"},{"location":"image_apis/#depthplanner-and-depthperspective","text":"You normally want to retrieve the depth image as float (i.e. set pixels_as_float = true ) and specify ImageType = DepthPlanner or ImageType = DepthPerspective in ImageRequest . For ImageType = DepthPlanner , you get depth in camera plan, i.e., all points that are in plan parallel to camera have same depth. For ImageType = DepthPerspective , you get depth from camera using a projection ray that hits that pixel. Depending on your use case, planner depth or perspective depth may be the ground truth image that you want. For example, you may be able to feed perspective depth to ROS package such as depth_image_proc to generate a point cloud. Or planner depth may be more compatible with estimated depth image generated by stereo algorithms such as SGM.","title":"DepthPlanner and DepthPerspective"},{"location":"image_apis/#depthvis","text":"When you specify ImageType = DepthVis in ImageRequest , you get an image that helps depth visualization. In this case, each pixel value is interpolated from black to white depending on depth in camera plane in meters. The pixels with pure white means depth of 100m or more while pure black means depth of 0 meters.","title":"DepthVis"},{"location":"image_apis/#disparitynormalized","text":"You normally want to retrieve disparity image as float (i.e. set pixels_as_float = true and specify ImageType = DisparityNormalized in ImageRequest ) in which case each pixel is (Xl - Xr)/Xmax , which is thereby normalized to values between 0 to 1.","title":"DisparityNormalized"},{"location":"image_apis/#segmentation","text":"When you specify ImageType = Segmentation in ImageRequest , you get an image that gives you ground truth segmentation of the scene. At the startup, AirSim assigns a random color index to each mesh available in environment. The RGB values for each color index ID can be retrieved from the API. You can assign a specific value to a specific mesh using APIs. For example, below Python code sets the object ID for the mesh called \"Ground\" to 20 in Blocks environment and hence changes its color in Segmentation view to the 20th color of the segmentation colormap: Note that this will not do a check if this color is already assigned to a different object! success = client.simSetSegmentationObjectID(\"Ground\", 20) The return value is a boolean type that lets you know if the mesh was found. Notice that typical Unreal environments, like Blocks, usually have many other meshes that comprises of same object, for example, \"Ground_2\", \"Ground_3\" and so on. As it is tedious to set object ID for all of these meshes, AirSim also supports regular expressions. For example, the code below sets all meshes which have names starting with \"ground\" (ignoring case) to 21 with just one line: success = client.simSetSegmentationObjectID(\"ground[\\w]*\", 21, True) The return value is true if at least one mesh was found using regular expression matching. When wanting to retrieve the segmentation image through the API, it is recommended that you request uncompressed image using this API to ensure you get precise RGB values for segmentation image: responses = client.simGetImages([airsim.ImageRequest( \"front_center\", airsim.ImageType.Segmentation, False, False)]) img_rgb_string = responses[0].image_data_uint8 rgbarray = np.frombuffer(img_rgb_string, np.uint8) rgbarray_shaped = rgbarray.reshape((540,960,3)) rgbarray_shaped = rgbarray_shaped img = Image.fromarray(rgbarray_shaped, 'RGB') img.show() To retrieve the color map to know which color is assign to each color index you can use: colorMap = client.simGetSegmentationColorMap() An example can be found in segmentation_test.py . For a script that generates a full list of objects and their associated color, please see the script segmentation_generate_list.py .","title":"Segmentation"},{"location":"image_apis/#how-to-find-mesh-names","text":"To get desired ground truth segmentation you will need to know the names of the meshes in your Unreal environment. To do this, you can use the API: currentObjectList = client.simListInstanceSegmentationObjects() This will use an understandable naming depending on the hierarchy the object belong to in the Unreal World (example box_2_fullpalletspawner_5_pallet_4 or door_window_door_38 ). Note that this provides a different result from simListSceneObjects() as this one will make a simple list of all Unreal Actors in the scene, without keeping the hierarchy in mind. An extension to simListInstanceSegmentationObjects() is simListInstanceSegmentationPoses(ned=True) which will retrieve the 3D object pose of each element in the same order as the first mentioned function.","title":"How to Find Mesh names?"},{"location":"image_apis/#startup-object-ids","text":"At the start, AirSim assigns color indexes to each object found in environment of type UStaticMeshComponent or USkinnedMeshComponent . It then makes an understandable naming depending on the hierarchy the object belong to in the Unreal World (example box_2_fullpalletspawner_5_pallet_4 or door_window_door_38 ).","title":"Startup Object IDs"},{"location":"image_apis/#getting-object-id-for-mesh","text":"The simGetSegmentationObjectID API allows you get object ID for given mesh name.","title":"Getting Object ID for Mesh"},{"location":"image_apis/#more-information","text":"Please see the instance segmentation documentation for some more information on the segmentation system created by Cosys-Lab.","title":"More information"},{"location":"image_apis/#infrared","text":"Currently, this is just a map from object ID to grey scale 0-255. So any mesh with object ID 42 shows up with color (42, 42, 42). Please see segmentation section for more details on how to set object IDs. Typically noise setting can be applied for this image type to get slightly more realistic effect. We are still working on adding other infrared artifacts and any contributions are welcome.","title":"Infrared"},{"location":"image_apis/#example-code","text":"A complete example of setting vehicle positions at random locations and orientations and then taking images can be found in GenerateImageGenerator.hpp . This example generates specified number of stereo images and ground truth disparity image and saving it to pfm format .","title":"Example Code"},{"location":"instance_segmentation/","text":"Instance Segmentation in AirSim An Instance segmentation system is implemented into AirSim. It uses Vertex Color rendering to allow for each object in the world to get its own color. Limitations 2744000 different colors are currently available to be assigned to unique objects. If your environment during a run requires more colors, you will generate errors and new objects will be assigned color [0,0,0]. Landscape objects aren't supported. This is the special object type in Unreal to make terrain with. As a work-around, StaticMesh terrain must be used. Foliage objects aren't supported. This is the special object type in Unreal to place trees, grass and other plants that move with the wind. As a work-around, StaticMesh objects must be used. Brush objects aren't supported. This is a special object type in Unreal to create your own meshes with. As a work-around, you can convert them to a StaticMesh. These and other unsupported object types that are less common that either will not be rendered(decals, text, foliage, ...) or will by default be given the RGB color value of [149,149,149] or 0,0,0 . Usage By default, at the start of the simulation, it will give a random color to each object. Please see the Image API documentation on how to manually set or get the color information. The easiest way to get the images from segmentation cameras, is through ros. See the ROS documentation for more information. For an example of the Instance Segmentation API, please see the script segmentation_test.py . For a script that generates a full list of objects and their associated color, please see the script segmentation_generate_list.py . When a new object is spawned in your environment by for example a c++ or blueprint extension you made, and you want it to work with the instance segmentation system, you can use the extended function ASimModeBase::AddNewActorToSegmentation(AActor) which is also available in blueprints. Make sure to provide human-readable names to your objects in your environment as the ground truth tables that the AirSim API can provide will use your object naming to create the table. Credits The method used to use Vertex Colors to segment object is a derivative of and inspired by the work of UnrealCV . Their work is licensed under the MIT License. It is made by students from Johns Hopkins University and Peking University under the supervision of Prof. Alan Yuille and Prof. Yizhou Wang. You can read the paper on their work here .","title":"Instance Segmentation"},{"location":"instance_segmentation/#instance-segmentation-in-airsim","text":"An Instance segmentation system is implemented into AirSim. It uses Vertex Color rendering to allow for each object in the world to get its own color.","title":"Instance Segmentation in AirSim"},{"location":"instance_segmentation/#limitations","text":"2744000 different colors are currently available to be assigned to unique objects. If your environment during a run requires more colors, you will generate errors and new objects will be assigned color [0,0,0]. Landscape objects aren't supported. This is the special object type in Unreal to make terrain with. As a work-around, StaticMesh terrain must be used. Foliage objects aren't supported. This is the special object type in Unreal to place trees, grass and other plants that move with the wind. As a work-around, StaticMesh objects must be used. Brush objects aren't supported. This is a special object type in Unreal to create your own meshes with. As a work-around, you can convert them to a StaticMesh. These and other unsupported object types that are less common that either will not be rendered(decals, text, foliage, ...) or will by default be given the RGB color value of [149,149,149] or 0,0,0 .","title":"Limitations"},{"location":"instance_segmentation/#usage","text":"By default, at the start of the simulation, it will give a random color to each object. Please see the Image API documentation on how to manually set or get the color information. The easiest way to get the images from segmentation cameras, is through ros. See the ROS documentation for more information. For an example of the Instance Segmentation API, please see the script segmentation_test.py . For a script that generates a full list of objects and their associated color, please see the script segmentation_generate_list.py . When a new object is spawned in your environment by for example a c++ or blueprint extension you made, and you want it to work with the instance segmentation system, you can use the extended function ASimModeBase::AddNewActorToSegmentation(AActor) which is also available in blueprints. Make sure to provide human-readable names to your objects in your environment as the ground truth tables that the AirSim API can provide will use your object naming to create the table.","title":"Usage"},{"location":"instance_segmentation/#credits","text":"The method used to use Vertex Colors to segment object is a derivative of and inspired by the work of UnrealCV . Their work is licensed under the MIT License. It is made by students from Johns Hopkins University and Peking University under the supervision of Prof. Alan Yuille and Prof. Yizhou Wang. You can read the paper on their work here .","title":"Credits"},{"location":"lidar/","text":"How to Use LiDAR in AirSim AirSim supports LiDAR for multirotors and cars. The enablement of LiDAR and the other LiDAR settings can be configured via AirSimSettings json. Please see general sensors for information on configruation of general/shared sensor settings. Enabling LiDAR on a vehicle By default, LiDARs are not enabled. To enable LiDAR, set the SensorType and Enabled attributes in settings json. \"Lidar1\": { \"SensorType\": 6, \"Enabled\" : true, Multiple LiDARs can be enabled on a vehicle. Ignoring glass and other material types One can set an object that should be invisible to LIDAR sensors (such as glass) to have no collision for Unreal Traces in order to have it be 'invisible' for LiDAR sensors. LiDAR configuration The following parameters can be configured right now via settings json. Parameter Description NumberOfChannels Number of channels/lasers of the LiDAR Range Range, in meters MeasurementsPerCycle Horizontal resolution. Amount of points in one cycle. RotationsPerSecond Rotations per second HorizontalFOVStart Horizontal FOV start for the LiDAR, in degrees HorizontalFOVEnd Horizontal FOV end for the LiDAR, in degrees VerticalFOVUpper Vertical FOV upper limit for the LiDAR, in degrees VerticalFOVLower Vertical FOV lower limit for the LiDAR, in degrees X Y Z Position of the LiDAR relative to the vehicle (in NED, in meters) Roll Pitch Yaw Orientation of the LiDAR relative to the vehicle (in degrees, yaw-pitch-roll order to front vector +X) GenerateNoise Generate and add range-noise based on normal distribution if set to true MinNoiseStandardDeviation The standard deviation to generate the noise normal distribution, in meters. This is the minimal noise (at 0 distance) NoiseDistanceScale To scale the noise with distance, set this parameter. This way the minimal noise is scaled depending on the distance compared to total maximum range of the sensor UpdateFrequency Amount of times per second that the sensor should update and calculate the next set of poins DrawSensor Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is LimitPoints Limit the amount of points that can be calculated in one measurement (to work around freezes due to bad performance). Will result in incomplete pointclouds External Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates ExternalLocal When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default { \"SeeDocsAt\": \"https://github.com/Cosys-Lab/Cosys-AirSim/tree/main/docs/settings.md\", \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"Drone1\": { \"VehicleType\": \"simpleflight\", \"AutoCreate\": true, \"Sensors\": { \"LidarSensor1\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 16, \"RotationsPerSecond\": 10, \"MeasurementsPerCycle\": 512, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"VerticalFOVUpper\": -15, \"VerticalFOVLower\": -25, \"HorizontalFOVStart\": -20, \"HorizontalFOVEnd\": 20, \"DrawDebugPoints\": true }, \"LidarSensor2\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 4, \"RotationsPerSecond\": 10, \"MeasurementsPerCycle\": 64, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"VerticalFOVUpper\": -15, \"VerticalFOVLower\": -25, \"DrawDebugPoints\": true } } } } } Server side visualization for debugging Be default, the LiDAR points are not drawn on the viewport. To enable the drawing of hit laser points on the viewport, please enable setting 'DrawDebugPoints' via settings json. e.g., \"Lidar1\": { ... \"DrawDebugPoints\": true }, Client API Use getLidarData(sensor name, vehicle name) API to retrieve the LiDAR data. * The API returns a full scan Point-Cloud as a flat array of floats along with the timestamp of the capture and LiDAR pose. * Point-Cloud: * The floats represent [x,y,z] coordinate for each point hit within the range in the last scan. It will be [0,0,0] for a laser that didn't get any reflection (out of range). * Pose: * Default: Sensor pose in the vehicle frame. * External: If set to External (see table) the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true . * LiDAR Groundtruth: * for each point of the Point-Cloud a label string is kept that has the name of the object that the point belongs to * a laser that didn't reflect anything will have label out_of_range .","title":"LIDAR"},{"location":"lidar/#how-to-use-lidar-in-airsim","text":"AirSim supports LiDAR for multirotors and cars. The enablement of LiDAR and the other LiDAR settings can be configured via AirSimSettings json. Please see general sensors for information on configruation of general/shared sensor settings.","title":"How to Use LiDAR in AirSim"},{"location":"lidar/#enabling-lidar-on-a-vehicle","text":"By default, LiDARs are not enabled. To enable LiDAR, set the SensorType and Enabled attributes in settings json. \"Lidar1\": { \"SensorType\": 6, \"Enabled\" : true, Multiple LiDARs can be enabled on a vehicle.","title":"Enabling LiDAR on a vehicle"},{"location":"lidar/#ignoring-glass-and-other-material-types","text":"One can set an object that should be invisible to LIDAR sensors (such as glass) to have no collision for Unreal Traces in order to have it be 'invisible' for LiDAR sensors.","title":"Ignoring glass and other material types"},{"location":"lidar/#lidar-configuration","text":"The following parameters can be configured right now via settings json. Parameter Description NumberOfChannels Number of channels/lasers of the LiDAR Range Range, in meters MeasurementsPerCycle Horizontal resolution. Amount of points in one cycle. RotationsPerSecond Rotations per second HorizontalFOVStart Horizontal FOV start for the LiDAR, in degrees HorizontalFOVEnd Horizontal FOV end for the LiDAR, in degrees VerticalFOVUpper Vertical FOV upper limit for the LiDAR, in degrees VerticalFOVLower Vertical FOV lower limit for the LiDAR, in degrees X Y Z Position of the LiDAR relative to the vehicle (in NED, in meters) Roll Pitch Yaw Orientation of the LiDAR relative to the vehicle (in degrees, yaw-pitch-roll order to front vector +X) GenerateNoise Generate and add range-noise based on normal distribution if set to true MinNoiseStandardDeviation The standard deviation to generate the noise normal distribution, in meters. This is the minimal noise (at 0 distance) NoiseDistanceScale To scale the noise with distance, set this parameter. This way the minimal noise is scaled depending on the distance compared to total maximum range of the sensor UpdateFrequency Amount of times per second that the sensor should update and calculate the next set of poins DrawSensor Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is LimitPoints Limit the amount of points that can be calculated in one measurement (to work around freezes due to bad performance). Will result in incomplete pointclouds External Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates ExternalLocal When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default { \"SeeDocsAt\": \"https://github.com/Cosys-Lab/Cosys-AirSim/tree/main/docs/settings.md\", \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"Drone1\": { \"VehicleType\": \"simpleflight\", \"AutoCreate\": true, \"Sensors\": { \"LidarSensor1\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 16, \"RotationsPerSecond\": 10, \"MeasurementsPerCycle\": 512, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"VerticalFOVUpper\": -15, \"VerticalFOVLower\": -25, \"HorizontalFOVStart\": -20, \"HorizontalFOVEnd\": 20, \"DrawDebugPoints\": true }, \"LidarSensor2\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 4, \"RotationsPerSecond\": 10, \"MeasurementsPerCycle\": 64, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"VerticalFOVUpper\": -15, \"VerticalFOVLower\": -25, \"DrawDebugPoints\": true } } } } }","title":"LiDAR configuration"},{"location":"lidar/#server-side-visualization-for-debugging","text":"Be default, the LiDAR points are not drawn on the viewport. To enable the drawing of hit laser points on the viewport, please enable setting 'DrawDebugPoints' via settings json. e.g., \"Lidar1\": { ... \"DrawDebugPoints\": true },","title":"Server side visualization for debugging"},{"location":"lidar/#client-api","text":"Use getLidarData(sensor name, vehicle name) API to retrieve the LiDAR data. * The API returns a full scan Point-Cloud as a flat array of floats along with the timestamp of the capture and LiDAR pose. * Point-Cloud: * The floats represent [x,y,z] coordinate for each point hit within the range in the last scan. It will be [0,0,0] for a laser that didn't get any reflection (out of range). * Pose: * Default: Sensor pose in the vehicle frame. * External: If set to External (see table) the coordinates will be in either Unreal NED when ExternalLocal is false or Local NED (from starting position from vehicle) when ExternalLocal is true . * LiDAR Groundtruth: * for each point of the Point-Cloud a label string is kept that has the name of the object that the point belongs to * a laser that didn't reflect anything will have label out_of_range .","title":"Client API"},{"location":"log_viewer/","text":"Log Viewer The LogViewer is a Windows WPF app that presents the MavLink streams that it is getting from the Unreal Simulator. You can use this to monitor what is happening on the drone while it is flying. For example, the picture below shows a real time graph of the x, y an z gyro sensor information being generated by the simulator. Usage To use this LogViewer, connect the simulator before you run the simulation. Simply press the blue connector button on the top right corner of the window, select the Socket tab , enter the port number 14388, and your localhost network. Then press the record button (triangle on the right hand side of the toolbar). Now start the simulator, pick some mavlink items to graph, you should see something like this: The drone view here is the actual estimated position coming from the PX4, so that is a great way to check whether the PX4 is in sync with the simulator. Sometimes you can see some drift here as the attitude estimation catches up with reality, this is more visible after a bad crash. Installation If you can't build the LogViewer.sln, there is also a click once installer . Configuration The magic port number 14388 can be configured in the simulator by editing the settings.json file .","title":"MavLink LogViewer"},{"location":"log_viewer/#log-viewer","text":"The LogViewer is a Windows WPF app that presents the MavLink streams that it is getting from the Unreal Simulator. You can use this to monitor what is happening on the drone while it is flying. For example, the picture below shows a real time graph of the x, y an z gyro sensor information being generated by the simulator.","title":"Log Viewer"},{"location":"log_viewer/#usage","text":"To use this LogViewer, connect the simulator before you run the simulation. Simply press the blue connector button on the top right corner of the window, select the Socket tab , enter the port number 14388, and your localhost network. Then press the record button (triangle on the right hand side of the toolbar). Now start the simulator, pick some mavlink items to graph, you should see something like this: The drone view here is the actual estimated position coming from the PX4, so that is a great way to check whether the PX4 is in sync with the simulator. Sometimes you can see some drift here as the attitude estimation catches up with reality, this is more visible after a bad crash.","title":"Usage"},{"location":"log_viewer/#installation","text":"If you can't build the LogViewer.sln, there is also a click once installer .","title":"Installation"},{"location":"log_viewer/#configuration","text":"The magic port number 14388 can be configured in the simulator by editing the settings.json file .","title":"Configuration"},{"location":"multi_vehicle/","text":"Multiple Vehicles in AirSim Since release 1.2, AirSim is fully enabled for multiple vehicles. This capability allows you to create multiple vehicles easily and use APIs to control them. Creating Multiple Vehicles It's as easy as specifying them in settings.json . The Vehicles element allows you to specify list of vehicles you want to create along with their initial positions and orientations. The positions are specified in NED coordinates in SI units with origin set at Player Start component in Unreal environment. The orientation is specified as Yaw, Pitch and Roll in degrees. Creating Multiple Cars { \"SettingsVersion\": 1.2, \"SimMode\": \"Car\", \"Vehicles\": { \"Car1\": { \"VehicleType\": \"PhysXCar\", \"X\": 4, \"Y\": 0, \"Z\": -2 }, \"Car2\": { \"VehicleType\": \"PhysXCar\", \"X\": -4, \"Y\": 0, \"Z\": -2, \"Yaw\": 90 } } } Creating Multiple Drones { \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"Drone1\": { \"VehicleType\": \"SimpleFlight\", \"X\": 4, \"Y\": 0, \"Z\": -2, \"Yaw\": -180 }, \"Drone2\": { \"VehicleType\": \"SimpleFlight\", \"X\": 8, \"Y\": 0, \"Z\": -2 } } } Using APIs for Multiple Vehicles The new APIs since AirSim 1.2 allows you to specify vehicle_name . This name corresponds to keys in json settings (for example, Car1 or Drone2 above). Example code for cars Example code for multirotors Demo","title":"Multiple Vehicles"},{"location":"multi_vehicle/#multiple-vehicles-in-airsim","text":"Since release 1.2, AirSim is fully enabled for multiple vehicles. This capability allows you to create multiple vehicles easily and use APIs to control them.","title":"Multiple Vehicles in AirSim"},{"location":"multi_vehicle/#creating-multiple-vehicles","text":"It's as easy as specifying them in settings.json . The Vehicles element allows you to specify list of vehicles you want to create along with their initial positions and orientations. The positions are specified in NED coordinates in SI units with origin set at Player Start component in Unreal environment. The orientation is specified as Yaw, Pitch and Roll in degrees.","title":"Creating Multiple Vehicles"},{"location":"multi_vehicle/#creating-multiple-cars","text":"{ \"SettingsVersion\": 1.2, \"SimMode\": \"Car\", \"Vehicles\": { \"Car1\": { \"VehicleType\": \"PhysXCar\", \"X\": 4, \"Y\": 0, \"Z\": -2 }, \"Car2\": { \"VehicleType\": \"PhysXCar\", \"X\": -4, \"Y\": 0, \"Z\": -2, \"Yaw\": 90 } } }","title":"Creating Multiple Cars"},{"location":"multi_vehicle/#creating-multiple-drones","text":"{ \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"Drone1\": { \"VehicleType\": \"SimpleFlight\", \"X\": 4, \"Y\": 0, \"Z\": -2, \"Yaw\": -180 }, \"Drone2\": { \"VehicleType\": \"SimpleFlight\", \"X\": 8, \"Y\": 0, \"Z\": -2 } } }","title":"Creating Multiple Drones"},{"location":"multi_vehicle/#using-apis-for-multiple-vehicles","text":"The new APIs since AirSim 1.2 allows you to specify vehicle_name . This name corresponds to keys in json settings (for example, Car1 or Drone2 above). Example code for cars Example code for multirotors","title":"Using APIs for Multiple Vehicles"},{"location":"multi_vehicle/#demo","text":"","title":"Demo"},{"location":"pfm/","text":"pfm Format Pfm (or Portable FloatMap) image format stores image as floating point pixels and hence are not restricted to usual 0-255 pixel value range. This is useful for HDR images or images that describes something other than colors like depth. One of the good viewer to view this file format is PfmPad . We don't recommend Maverick photo viewer because it doesn't seem to show depth images properly. AirSim has code to write pfm file for C++ and read as well as write for Python .","title":"pfm format"},{"location":"pfm/#pfm-format","text":"Pfm (or Portable FloatMap) image format stores image as floating point pixels and hence are not restricted to usual 0-255 pixel value range. This is useful for HDR images or images that describes something other than colors like depth. One of the good viewer to view this file format is PfmPad . We don't recommend Maverick photo viewer because it doesn't seem to show depth images properly. AirSim has code to write pfm file for C++ and read as well as write for Python .","title":"pfm Format"},{"location":"playback/","text":"Playback AirSim supports playing back the high level commands in a *.mavlink log file that were recorded using the MavLinkTest app for the purpose of comparing real and simulated flight. The recording.mavlink is an example of a log file captured using a real drone using the following command line: MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. Then the log file contains the commands performed, which included several \"orbit\" commands, the resulting GPS map of the flight looks like this: Side-by-side comparison Now we can copy the *.mavlink log file recorded by MavLinkTest to the PC running the Unreal simulator with AirSim plugin. When the Simulator is running and the drone is parked in a place in a map that has room to do the same maneuvers we can run this MavLinkTest command line: MavLinkTest -server:127.0.0.1:14550 This should connect to the simulator. Now you can enter this command: PlayLog recording.mavlink The same commands you performed on the real drone will now play again in the simulator. You can then press 't' to see the trace, and it will show you the trace of the real drone and the simulated drone. Every time you press 't' again you can reset the lines so they are sync'd to the current position, this way I was able to capture a side-by-side trace of the \"orbit\" command performed in this recording, which generates the picture below. The pink line is the simulated flight and the red line is the real flight: Note: I'm using the ';' key in the simulator to take control of camera position using keyboard to get this shot. Parameters It may help to set the simulator up with some of the same flight parameters that your real drone is using, for example, in my case I was using a lower than normal cruise speed, slow takeoff speed, and it helps to tell the simulator to wait a long time before disarming (COM_DISARM_LAND) and to turn off the safety switches NAV_RCL_ACT and NAV_DLL_ACT ( don't do that on a real drone). param MPC_XY_CRUISE 2 param MPC_XY_VEL_MAX 2 param MPC_TKO_SPEED 1 param COM_DISARM_LAND 60 param NAV_RCL_ACT 0 param NAV_DLL_ACT 0","title":"Playing Logs"},{"location":"playback/#playback","text":"AirSim supports playing back the high level commands in a *.mavlink log file that were recorded using the MavLinkTest app for the purpose of comparing real and simulated flight. The recording.mavlink is an example of a log file captured using a real drone using the following command line: MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. Then the log file contains the commands performed, which included several \"orbit\" commands, the resulting GPS map of the flight looks like this:","title":"Playback"},{"location":"playback/#side-by-side-comparison","text":"Now we can copy the *.mavlink log file recorded by MavLinkTest to the PC running the Unreal simulator with AirSim plugin. When the Simulator is running and the drone is parked in a place in a map that has room to do the same maneuvers we can run this MavLinkTest command line: MavLinkTest -server:127.0.0.1:14550 This should connect to the simulator. Now you can enter this command: PlayLog recording.mavlink The same commands you performed on the real drone will now play again in the simulator. You can then press 't' to see the trace, and it will show you the trace of the real drone and the simulated drone. Every time you press 't' again you can reset the lines so they are sync'd to the current position, this way I was able to capture a side-by-side trace of the \"orbit\" command performed in this recording, which generates the picture below. The pink line is the simulated flight and the red line is the real flight: Note: I'm using the ';' key in the simulator to take control of camera position using keyboard to get this shot.","title":"Side-by-side comparison"},{"location":"playback/#parameters","text":"It may help to set the simulator up with some of the same flight parameters that your real drone is using, for example, in my case I was using a lower than normal cruise speed, slow takeoff speed, and it helps to tell the simulator to wait a long time before disarming (COM_DISARM_LAND) and to turn off the safety switches NAV_RCL_ACT and NAV_DLL_ACT ( don't do that on a real drone). param MPC_XY_CRUISE 2 param MPC_XY_VEL_MAX 2 param MPC_TKO_SPEED 1 param COM_DISARM_LAND 60 param NAV_RCL_ACT 0 param NAV_DLL_ACT 0","title":"Parameters"},{"location":"px4_build/","text":"Building PX4 Source code Getting the PX4 source code is easy: git clone https://github.com/PX4/Firmware.git cd Firmware Oh, and if you don't have git yet just run this: sudo apt-get install git We are currently testing using the 1.6.0rc1 version, but the latest master branch should be ok too. Now to build it you will need the right tools. PX4 Build tools The full instructions are available on the dev.px4.io website, but we've copied the relevant subset of those instructions here for your convenience. (Note that BashOnWindows ) can be used to build the PX4 firmware, just follow the BashOnWindows instructions at the bottom of this page). Build SITL version Now you can make the SITL version that runs in posix, from the Firmware folder you created above: make posix_sitl_ekf2 Note: this build system is quite special, it knows how to update git submodules (and there's a lot of them), then it runs cmake (if necessary), then it runs the build itself. So in a way the root Makefile is a meta-meta makefile :-) It shouldn't take long, about 2 minutes. If all succeeds, the last line will link the px4 app, which you can then run using the following: make posix_sitl_ekf2 none_iris And you should see output that looks like this: creating new parameters file creating new dataman file ______ __ __ ___ | ___ \\ \\ \\ / / / | | |_/ / \\ V / / /| | | __/ / \\ / /_| | | | / /^\\ \\ \\___ | \\_| \\/ \\/ |_/ px4 starting. 18446744073709551615 WARNING: setRealtimeSched failed (not run as root?) ERROR [param] importing from 'rootfs/eeprom/parameters' failed (-1) Command 'param' failed, returned 1 SYS_AUTOSTART: curr: 0 -> new: 4010 SYS_MC_EST_GROUP: curr: 2 -> new: 1 INFO [dataman] Unkown restart, data manager file 'rootfs/fs/microsd/dataman' size is 11797680 bytes BAT_N_CELLS: curr: 0 -> new: 3 CAL_GYRO0_ID: curr: 0 -> new: 2293768 CAL_ACC0_ID: curr: 0 -> new: 1376264 CAL_ACC1_ID: curr: 0 -> new: 1310728 CAL_MAG0_ID: curr: 0 -> new: 196616 so this is good, first run sets up the px4 parameters for SITL mode. Second run has less output. This app is also an interactive console where you can type commands. Type 'help' to see what they are and just type ctrl-C to kill it. You can do that and restart it any time, that's a great way to reset any wonky state if you need to (it's equivalent to a Pixhawk hardware reboot). ARM embedded tools If you plan to build the PX4 firmware for real Pixhawk hardware then you will need the gcc cross-compiler for ARM Cortex-M4 chipset. You can get this compiler by PX4 DevGuide, specifically this is in their ubuntu_sim_nuttx.sh setup script. After following those setup instructions you can verify the install by entering this command arm-none-eabi-gcc --version . You should see the following output: arm-none-eabi-gcc (GNU Tools for Arm Embedded Processors 7-2017-q4-major) 7.2.1 20170904 (release) [ARM/embedded-7-branch revision 255204] Copyright (C) 2017 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Build PX4 for ARM hardware Now you can build the PX4 firmware for running on real pixhawk hardware: make px4fmu-v2_default This build will take a little longer because it is building a lot more including the NuttX real time OS, all the drivers for the sensors in the Pixhawk flight controller, and more. It is also running the compiler in super size-squeezing mode so it can fit all that in a 1 megabyte ROM !! One nice tid bit is you can plug in your pixhawk USB, and type make px4fmu-v2_default upload to flash the hardware with these brand new bits, so you don't need to use QGroundControl for that. Some Useful Parameters PX4 has many customizable parameters (over 700 of them, in fact) and to get best results with AirSim we have found the following parameters are handy: // be sure to enable the new position estimator module: param set SYS_MC_EST_GROUP 2 // increase default limits on cruise speed so you can move around a large map more quickly. param MPC_XY_CRUISE 10 param MPC_XY_VEL_MAX 10 param MPC_Z_VEL_MAX_DN 2 // increase timeout for auto-disarm on landing so that any long running app doesn't have to worry about it param COM_DISARM_LAND 60 // make it possible to fly without radio control attached (do NOT do this one on a real drone) param NAV_RCL_ACT 0 // enable new syslogger to get more information from PX4 logs param set SYS_LOGGER 1 Using BashOnWindows See Bash on Windows Toolchain .","title":"Building PX4"},{"location":"px4_build/#building-px4","text":"","title":"Building PX4"},{"location":"px4_build/#source-code","text":"Getting the PX4 source code is easy: git clone https://github.com/PX4/Firmware.git cd Firmware Oh, and if you don't have git yet just run this: sudo apt-get install git We are currently testing using the 1.6.0rc1 version, but the latest master branch should be ok too. Now to build it you will need the right tools.","title":"Source code"},{"location":"px4_build/#px4-build-tools","text":"The full instructions are available on the dev.px4.io website, but we've copied the relevant subset of those instructions here for your convenience. (Note that BashOnWindows ) can be used to build the PX4 firmware, just follow the BashOnWindows instructions at the bottom of this page).","title":"PX4 Build tools"},{"location":"px4_build/#build-sitl-version","text":"Now you can make the SITL version that runs in posix, from the Firmware folder you created above: make posix_sitl_ekf2 Note: this build system is quite special, it knows how to update git submodules (and there's a lot of them), then it runs cmake (if necessary), then it runs the build itself. So in a way the root Makefile is a meta-meta makefile :-) It shouldn't take long, about 2 minutes. If all succeeds, the last line will link the px4 app, which you can then run using the following: make posix_sitl_ekf2 none_iris And you should see output that looks like this: creating new parameters file creating new dataman file ______ __ __ ___ | ___ \\ \\ \\ / / / | | |_/ / \\ V / / /| | | __/ / \\ / /_| | | | / /^\\ \\ \\___ | \\_| \\/ \\/ |_/ px4 starting. 18446744073709551615 WARNING: setRealtimeSched failed (not run as root?) ERROR [param] importing from 'rootfs/eeprom/parameters' failed (-1) Command 'param' failed, returned 1 SYS_AUTOSTART: curr: 0 -> new: 4010 SYS_MC_EST_GROUP: curr: 2 -> new: 1 INFO [dataman] Unkown restart, data manager file 'rootfs/fs/microsd/dataman' size is 11797680 bytes BAT_N_CELLS: curr: 0 -> new: 3 CAL_GYRO0_ID: curr: 0 -> new: 2293768 CAL_ACC0_ID: curr: 0 -> new: 1376264 CAL_ACC1_ID: curr: 0 -> new: 1310728 CAL_MAG0_ID: curr: 0 -> new: 196616 so this is good, first run sets up the px4 parameters for SITL mode. Second run has less output. This app is also an interactive console where you can type commands. Type 'help' to see what they are and just type ctrl-C to kill it. You can do that and restart it any time, that's a great way to reset any wonky state if you need to (it's equivalent to a Pixhawk hardware reboot).","title":"Build SITL version"},{"location":"px4_build/#arm-embedded-tools","text":"If you plan to build the PX4 firmware for real Pixhawk hardware then you will need the gcc cross-compiler for ARM Cortex-M4 chipset. You can get this compiler by PX4 DevGuide, specifically this is in their ubuntu_sim_nuttx.sh setup script. After following those setup instructions you can verify the install by entering this command arm-none-eabi-gcc --version . You should see the following output: arm-none-eabi-gcc (GNU Tools for Arm Embedded Processors 7-2017-q4-major) 7.2.1 20170904 (release) [ARM/embedded-7-branch revision 255204] Copyright (C) 2017 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.","title":"ARM embedded tools"},{"location":"px4_build/#build-px4-for-arm-hardware","text":"Now you can build the PX4 firmware for running on real pixhawk hardware: make px4fmu-v2_default This build will take a little longer because it is building a lot more including the NuttX real time OS, all the drivers for the sensors in the Pixhawk flight controller, and more. It is also running the compiler in super size-squeezing mode so it can fit all that in a 1 megabyte ROM !! One nice tid bit is you can plug in your pixhawk USB, and type make px4fmu-v2_default upload to flash the hardware with these brand new bits, so you don't need to use QGroundControl for that.","title":"Build PX4 for ARM hardware"},{"location":"px4_build/#some-useful-parameters","text":"PX4 has many customizable parameters (over 700 of them, in fact) and to get best results with AirSim we have found the following parameters are handy: // be sure to enable the new position estimator module: param set SYS_MC_EST_GROUP 2 // increase default limits on cruise speed so you can move around a large map more quickly. param MPC_XY_CRUISE 10 param MPC_XY_VEL_MAX 10 param MPC_Z_VEL_MAX_DN 2 // increase timeout for auto-disarm on landing so that any long running app doesn't have to worry about it param COM_DISARM_LAND 60 // make it possible to fly without radio control attached (do NOT do this one on a real drone) param NAV_RCL_ACT 0 // enable new syslogger to get more information from PX4 logs param set SYS_LOGGER 1","title":"Some Useful Parameters"},{"location":"px4_build/#using-bashonwindows","text":"See Bash on Windows Toolchain .","title":"Using BashOnWindows"},{"location":"px4_logging/","text":"PX4/MavLink Logging Thanks to Chris Lovett for developing various tools for PX4/MavLink logging mentioned on this page! Logging MavLink Messages The following command will connect MavLinkTest app to the Simulator and enable logging of all mavlink commands to and from the PX4. MavLinkTest -server:127.0.0.1:14550 -logdir:d:\\temp Sometimes you will also want to log the \"output\" from the Simulator that is being sent to the PX4. Specifically this will capture the \"hilgps\" and \"hilsensor\" messages that are generated by the Simulator. To do that run this as well: MavLinkTest -server:127.0.0.1:14389 -logdir:d:\\temp\\output You will then see log files organized by date in d:\\temp\\logs, specifically input.mavlink and output.mavlink files. MavLink LogViewer For MavLink enabled drones, you can also use our Log Viewer to visualize the streams of data. PX4 Log in SITL Mode In SITL mode, please a log file is produced when drone is armed. The SITL terminal will contain the path to the log file, it should look something like this INFO [logger] Opened log file: rootfs/fs/microsd/log/2017-03-27/20_02_49.ulg PX4 Log in SITL Mode If you are using Pixhawk hardware in HIL mode, then set parameter SYS_LOGGER=1 using QGroundControl. PX4 will write log file on device which you can download at later date using QGroundControl.","title":"PX4/MavLink Logging"},{"location":"px4_logging/#px4mavlink-logging","text":"Thanks to Chris Lovett for developing various tools for PX4/MavLink logging mentioned on this page!","title":"PX4/MavLink Logging"},{"location":"px4_logging/#logging-mavlink-messages","text":"The following command will connect MavLinkTest app to the Simulator and enable logging of all mavlink commands to and from the PX4. MavLinkTest -server:127.0.0.1:14550 -logdir:d:\\temp Sometimes you will also want to log the \"output\" from the Simulator that is being sent to the PX4. Specifically this will capture the \"hilgps\" and \"hilsensor\" messages that are generated by the Simulator. To do that run this as well: MavLinkTest -server:127.0.0.1:14389 -logdir:d:\\temp\\output You will then see log files organized by date in d:\\temp\\logs, specifically input.mavlink and output.mavlink files.","title":"Logging MavLink Messages"},{"location":"px4_logging/#mavlink-logviewer","text":"For MavLink enabled drones, you can also use our Log Viewer to visualize the streams of data.","title":"MavLink LogViewer"},{"location":"px4_logging/#px4-log-in-sitl-mode","text":"In SITL mode, please a log file is produced when drone is armed. The SITL terminal will contain the path to the log file, it should look something like this INFO [logger] Opened log file: rootfs/fs/microsd/log/2017-03-27/20_02_49.ulg","title":"PX4 Log in SITL Mode"},{"location":"px4_logging/#px4-log-in-sitl-mode_1","text":"If you are using Pixhawk hardware in HIL mode, then set parameter SYS_LOGGER=1 using QGroundControl. PX4 will write log file on device which you can download at later date using QGroundControl.","title":"PX4 Log in SITL Mode"},{"location":"px4_setup/","text":"PX4 Setup for AirSim The PX4 software stack is an open source very popular flight controller with support for wide variety of boards and sensors as well as built-in capability for higher level tasks such as mission planning. Please visit px4.io for more information. Warning : While all releases of AirSim are always tested with PX4 to ensure the support, setting up PX4 is not a trivial task. Unless you have at least intermediate level of experience with PX4 stack, we recommend you use simple_flight , which is now a default in AirSim. Supported Hardware The following Pixhawk hardware has been tested with AirSim: 3DR Pixhawk v2 3DR Pixhawk mini Pixhawk PX4 2.4.8 PixFalcon PixRacer Pixhawk 2.1 (using PX4 Flight Stack) The 3DR Pixhawk Mini works out of the box, the others you may need to re-flash with the latest firmware. Setting up PX4 Hardware-in-Loop For this you will need one of the supported device listed above. For manual flight you will also need RC + transmitter. Make sure your RC receiver is bound with its RC transmitter. Connect the RC transmitter to the flight controller's RC port. Refer to your RC manual and PX4 docs for more information. Download QGroundControl , launch it and connect your flight controller to the USB port. Use QGroundControl to flash the latest PX4 Flight Stack. See also initial firmware setup video . In QGroundControl, configure your Pixhawk for HIL simulation by selecting the HIL Quadrocopter X airframe. After PX4 reboots, check that \"HIL Quadrocopter X\" is indeed selected. In QGroundControl, go to Radio tab and calibrate (make sure the remote control is on and the receiver is showing the indicator for the binding). Go to the Flight Mode tab and chose one of the remote control switches as \"Mode Channel\". Then set (for example) Stabilized and Attitude flight modes for two positions of the switch. Go to the Tuning section of QGroundControl and set appropriate values. For example, for Fly Sky's FS-TH9X remote control, the following settings give a more realistic feel: Hover Throttle = mid+1 mark, Roll and pitch sensitivity = mid-3 mark, Altitude and position control sensitivity = mid-2 mark. In AirSim settings file, specify PX4 for your vehicle config like this: { \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\" } } } After above setup you should be able to use RC to fly in the AirSim. You can usually arm the vehicle by lowering and bringing two sticks of RC together in-wards. You don't need QGroundControl after the initial setup. Typically the Stabilized (instead of Manual) mode gives better experience for beginners. You can also control the drone from Python APIs . See Walkthrough Demo Video and Unreal AirSim Setup Video that shows you all the setup steps in this document. Setting up PX4 Software-in-Loop The PX4 SITL mode doesn't require you to have separate device such as a Pixhawk or Pixracer. This is in fact the recommended way to use PX4 with simulators by PX4 team. However, this is indeed harder to set up. Please see this dedicated page for setting up PX4 in SITL mode. FAQ Drone doesn't fly properly, it just goes \"crazy\". There are a few reasons that can cause this. First, make sure your drone doesn't fall down large distance when starting the simulator. This might happen if you have created a custom Unreal environment and Player Start is placed too high above the ground. It seems that when this happens internal calibration in PX4 gets confused. You should also use QGroundControl and make sure you can arm and takeoff in QGroundControl properly. Finally, this also can be a machine performance issue in some rare cases, check your hard drive performance . Can I use Arducopter or other MavLink implementations? Our code is tested with the PX4 firmware . We have not tested Arducopter or other mavlink implementations. Some of the flight API's do use the PX4 custom modes in the MAV_CMD_DO_SET_MODE messages (like PX4_CUSTOM_MAIN_MODE_AUTO) It is not finding my Pixhawk hardware Check your settings.json file for this line \"SerialPort\":\"*,115200\". The asterisk here means \"find any serial port that looks like a Pixhawk device, but this doesn't always work for all types of Pixhawk hardware. So on Windows you can find the actual COM port using Device Manager, look under \"Ports (COM & LPT), plug the device in and see what new COM port shows up. Let's say you see a new port named \"USB Serial Port (COM5)\". Well, then change the SerialPort setting to this: \"SerialPort\":\"COM5,115200\". On Linux, the device can be found by running \"ls /dev/serial/by-id\" if you see a device name listed that looks like this usb-3D_Robotics_PX4_FMU_v2.x_0-if00 then you can use that name to connect, like this: \"SerialPort\":\"/dev/serial/by-id/usb-3D_Robotics_PX4_FMU_v2.x_0-if00\". Note this long name is actually a symbolic link to the real name, if you use \"ls -l ...\" you can find that symbolic link, it is usually something like \"/dev/ttyACM0\", so this will also work \"SerialPort\":\"/dev/ttyACM0,115200\". But that mapping is similar to windows, it is automatically assigned and can change, whereas the long name will work even if the actual TTY serial device mapping changes. WARN [commander] Takeoff denied, disarm and re-try This happens if you try and take off when PX4 still has not computed the home position. PX4 will report the home position once it is happy with the GPS signal, and you will see these messages: INFO [commander] home: 47.6414680, -122.1401672, 119.99 INFO [tone_alarm] home_set Up until this point in time, however, the PX4 will reject takeoff commands. When I tell the drone to do something it always lands For example, you use DroneShell moveToPosition -z -20 -x 50 -y 0 which it does, but when it gets to the target location the drone starts to land. This is the default behavior of PX4 when offboard mode completes. To set the drone to hover instead set this PX4 parameter: param set COM_OBL_ACT 1 I get message length mismatches errors You might need to set MAV_PROTO_VER parameter in QGC to \"Always use version 1\". Please see this issue more details.","title":"PX4 Setup for AirSim"},{"location":"px4_setup/#px4-setup-for-airsim","text":"The PX4 software stack is an open source very popular flight controller with support for wide variety of boards and sensors as well as built-in capability for higher level tasks such as mission planning. Please visit px4.io for more information. Warning : While all releases of AirSim are always tested with PX4 to ensure the support, setting up PX4 is not a trivial task. Unless you have at least intermediate level of experience with PX4 stack, we recommend you use simple_flight , which is now a default in AirSim.","title":"PX4 Setup for AirSim"},{"location":"px4_setup/#supported-hardware","text":"The following Pixhawk hardware has been tested with AirSim: 3DR Pixhawk v2 3DR Pixhawk mini Pixhawk PX4 2.4.8 PixFalcon PixRacer Pixhawk 2.1 (using PX4 Flight Stack) The 3DR Pixhawk Mini works out of the box, the others you may need to re-flash with the latest firmware.","title":"Supported Hardware"},{"location":"px4_setup/#setting-up-px4-hardware-in-loop","text":"For this you will need one of the supported device listed above. For manual flight you will also need RC + transmitter. Make sure your RC receiver is bound with its RC transmitter. Connect the RC transmitter to the flight controller's RC port. Refer to your RC manual and PX4 docs for more information. Download QGroundControl , launch it and connect your flight controller to the USB port. Use QGroundControl to flash the latest PX4 Flight Stack. See also initial firmware setup video . In QGroundControl, configure your Pixhawk for HIL simulation by selecting the HIL Quadrocopter X airframe. After PX4 reboots, check that \"HIL Quadrocopter X\" is indeed selected. In QGroundControl, go to Radio tab and calibrate (make sure the remote control is on and the receiver is showing the indicator for the binding). Go to the Flight Mode tab and chose one of the remote control switches as \"Mode Channel\". Then set (for example) Stabilized and Attitude flight modes for two positions of the switch. Go to the Tuning section of QGroundControl and set appropriate values. For example, for Fly Sky's FS-TH9X remote control, the following settings give a more realistic feel: Hover Throttle = mid+1 mark, Roll and pitch sensitivity = mid-3 mark, Altitude and position control sensitivity = mid-2 mark. In AirSim settings file, specify PX4 for your vehicle config like this: { \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\" } } } After above setup you should be able to use RC to fly in the AirSim. You can usually arm the vehicle by lowering and bringing two sticks of RC together in-wards. You don't need QGroundControl after the initial setup. Typically the Stabilized (instead of Manual) mode gives better experience for beginners. You can also control the drone from Python APIs . See Walkthrough Demo Video and Unreal AirSim Setup Video that shows you all the setup steps in this document.","title":"Setting up PX4 Hardware-in-Loop"},{"location":"px4_setup/#setting-up-px4-software-in-loop","text":"The PX4 SITL mode doesn't require you to have separate device such as a Pixhawk or Pixracer. This is in fact the recommended way to use PX4 with simulators by PX4 team. However, this is indeed harder to set up. Please see this dedicated page for setting up PX4 in SITL mode.","title":"Setting up PX4 Software-in-Loop"},{"location":"px4_setup/#faq","text":"","title":"FAQ"},{"location":"px4_setup/#drone-doesnt-fly-properly-it-just-goes-crazy","text":"There are a few reasons that can cause this. First, make sure your drone doesn't fall down large distance when starting the simulator. This might happen if you have created a custom Unreal environment and Player Start is placed too high above the ground. It seems that when this happens internal calibration in PX4 gets confused. You should also use QGroundControl and make sure you can arm and takeoff in QGroundControl properly. Finally, this also can be a machine performance issue in some rare cases, check your hard drive performance .","title":"Drone doesn't fly properly, it just goes \"crazy\"."},{"location":"px4_setup/#can-i-use-arducopter-or-other-mavlink-implementations","text":"Our code is tested with the PX4 firmware . We have not tested Arducopter or other mavlink implementations. Some of the flight API's do use the PX4 custom modes in the MAV_CMD_DO_SET_MODE messages (like PX4_CUSTOM_MAIN_MODE_AUTO)","title":"Can I use Arducopter or other MavLink implementations?"},{"location":"px4_setup/#it-is-not-finding-my-pixhawk-hardware","text":"Check your settings.json file for this line \"SerialPort\":\"*,115200\". The asterisk here means \"find any serial port that looks like a Pixhawk device, but this doesn't always work for all types of Pixhawk hardware. So on Windows you can find the actual COM port using Device Manager, look under \"Ports (COM & LPT), plug the device in and see what new COM port shows up. Let's say you see a new port named \"USB Serial Port (COM5)\". Well, then change the SerialPort setting to this: \"SerialPort\":\"COM5,115200\". On Linux, the device can be found by running \"ls /dev/serial/by-id\" if you see a device name listed that looks like this usb-3D_Robotics_PX4_FMU_v2.x_0-if00 then you can use that name to connect, like this: \"SerialPort\":\"/dev/serial/by-id/usb-3D_Robotics_PX4_FMU_v2.x_0-if00\". Note this long name is actually a symbolic link to the real name, if you use \"ls -l ...\" you can find that symbolic link, it is usually something like \"/dev/ttyACM0\", so this will also work \"SerialPort\":\"/dev/ttyACM0,115200\". But that mapping is similar to windows, it is automatically assigned and can change, whereas the long name will work even if the actual TTY serial device mapping changes.","title":"It is not finding my Pixhawk hardware"},{"location":"px4_setup/#warn-commander-takeoff-denied-disarm-and-re-try","text":"This happens if you try and take off when PX4 still has not computed the home position. PX4 will report the home position once it is happy with the GPS signal, and you will see these messages: INFO [commander] home: 47.6414680, -122.1401672, 119.99 INFO [tone_alarm] home_set Up until this point in time, however, the PX4 will reject takeoff commands.","title":"WARN  [commander] Takeoff denied, disarm and re-try"},{"location":"px4_setup/#when-i-tell-the-drone-to-do-something-it-always-lands","text":"For example, you use DroneShell moveToPosition -z -20 -x 50 -y 0 which it does, but when it gets to the target location the drone starts to land. This is the default behavior of PX4 when offboard mode completes. To set the drone to hover instead set this PX4 parameter: param set COM_OBL_ACT 1","title":"When I tell the drone to do something it always lands"},{"location":"px4_setup/#i-get-message-length-mismatches-errors","text":"You might need to set MAV_PROTO_VER parameter in QGC to \"Always use version 1\". Please see this issue more details.","title":"I get message length mismatches errors"},{"location":"px4_sitl/","text":"Setting up PX4 Software-in-Loop The PX4 software provides a \"software-in-loop\" simulation (SITL) version of their stack that runs in Linux. Sorry it doesn't run in Windows, but if you install BashOnWindows you can build and run it there. From your Linux bash terminal follow these steps for Linux and follow all the instructions under NuttX based hardware to install prerequisites. We've also included out own copy of the PX4 build instructions which is a bit more concise about what we need exactly. Get the PX4 source code and build the posix SITL version of PX4: mkdir -p PX4 cd PX4 git clone https://github.com/PX4/Firmware.git cd Firmware git checkout v1.8.2 # Pick a well known \"good\" release tag. Use following command to build and start PX4 firmware in SITL mode: make posix_sitl_ekf2 none_iris You should see a message like this you INFO [simulator] Waiting for initial data on UDP port 14560 which means the SITL PX4 app is waiting for someone to connect. Now edit AirSim settings file to make sure you have followings: ```json { \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", \"UseSerial\": false } } } } `` 6. Run Unreal environment and it should connect to SITL via UDP. You should see a bunch of messages from the SITL PX4 window from things like [mavlink] and [commander]` and so on. 7. You should also be able to use QGroundControl just like with flight controller hardware. Note that as we don't have physical board, RC cannot be connected directly to it. So the alternatives are either use XBox 360 Controller or connect your RC using USB (for example, in case of FrSky Taranis X9D Plus) or using trainer USB cable to PC. This makes your RC look like joystick. You will need to do extra set up in QGroundControl to use virtual joystick for RC control. Setting GPS origin PX4 SITL mode needs to be configured to get the home location correct. Run the following in the PX4 console window so that the origin matches that which is setup in AirSim AVehiclePawnBase::HomeLatitude and HomeLongitude. param set LPE_LAT 47.641468 param set LPE_LON -122.140165 You might also want to set this one so that the drone automatically hovers after each offboard control command (the default setting is to land): param set COM_OBL_ACT 1 Now close Unreal app, restart the px4 app and re-start the unreal app. In fact, every time you stop the unreal app you have top restart the px4 app. Check the Home Position If you are using DroneShell to execute commands (arm, takeoff, etc) then you should wait until the Home position is set. You will see the PX4 SITL console output this message: INFO [commander] home: 47.6414680, -122.1401672, 119.99 INFO [tone_alarm] home_set Now DroneShell 'pos' command should report this position and the commands should be accepted by PX4. If you attempt to takeoff without a home position you will see the message: WARN [commander] Takeoff denied, disarm and re-try After home position is set check the local position reported by 'pos' command : Local position: x=-0.0326988, y=0.00656854, z=5.48506 If the z coordinate is large like this then takeoff might not work as expected. Resetting the SITL and simulation should fix that problem. No Remote Control If you plan to fly with no remote control, just using DroneShell commands for example, then you will need to set the following parameters to stop the PX4 from triggering \"failsafe mode on\" every time a move command is finished. param set NAV_RCL_ACT 0 param set NAV_DLL_ACT 0 NOTE: Do NOT do this on a real drone as it is too dangerous to fly without these failsafe measures. Using VirtualBox Ubuntu If you want to run the above posix_sitl in a VirtualBox Ubuntu machine then it will have a different ip address from localhost. So in this case you need to edit the settings file and change the UdpIp and SitlIp to the ip address of your virtual machine set the LocalIpAddress to the address of your host machine running the Unreal engine. Remote Controller There are several options for flying the simulated drone using a remote control or joystick like xbox gamepad. See remote controllers","title":"PX4 in SITL"},{"location":"px4_sitl/#setting-up-px4-software-in-loop","text":"The PX4 software provides a \"software-in-loop\" simulation (SITL) version of their stack that runs in Linux. Sorry it doesn't run in Windows, but if you install BashOnWindows you can build and run it there. From your Linux bash terminal follow these steps for Linux and follow all the instructions under NuttX based hardware to install prerequisites. We've also included out own copy of the PX4 build instructions which is a bit more concise about what we need exactly. Get the PX4 source code and build the posix SITL version of PX4: mkdir -p PX4 cd PX4 git clone https://github.com/PX4/Firmware.git cd Firmware git checkout v1.8.2 # Pick a well known \"good\" release tag. Use following command to build and start PX4 firmware in SITL mode: make posix_sitl_ekf2 none_iris You should see a message like this you INFO [simulator] Waiting for initial data on UDP port 14560 which means the SITL PX4 app is waiting for someone to connect. Now edit AirSim settings file to make sure you have followings: ```json { \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", \"UseSerial\": false } } } } `` 6. Run Unreal environment and it should connect to SITL via UDP. You should see a bunch of messages from the SITL PX4 window from things like [mavlink] and [commander]` and so on. 7. You should also be able to use QGroundControl just like with flight controller hardware. Note that as we don't have physical board, RC cannot be connected directly to it. So the alternatives are either use XBox 360 Controller or connect your RC using USB (for example, in case of FrSky Taranis X9D Plus) or using trainer USB cable to PC. This makes your RC look like joystick. You will need to do extra set up in QGroundControl to use virtual joystick for RC control.","title":"Setting up PX4 Software-in-Loop"},{"location":"px4_sitl/#setting-gps-origin","text":"PX4 SITL mode needs to be configured to get the home location correct. Run the following in the PX4 console window so that the origin matches that which is setup in AirSim AVehiclePawnBase::HomeLatitude and HomeLongitude. param set LPE_LAT 47.641468 param set LPE_LON -122.140165 You might also want to set this one so that the drone automatically hovers after each offboard control command (the default setting is to land): param set COM_OBL_ACT 1 Now close Unreal app, restart the px4 app and re-start the unreal app. In fact, every time you stop the unreal app you have top restart the px4 app.","title":"Setting GPS origin"},{"location":"px4_sitl/#check-the-home-position","text":"If you are using DroneShell to execute commands (arm, takeoff, etc) then you should wait until the Home position is set. You will see the PX4 SITL console output this message: INFO [commander] home: 47.6414680, -122.1401672, 119.99 INFO [tone_alarm] home_set Now DroneShell 'pos' command should report this position and the commands should be accepted by PX4. If you attempt to takeoff without a home position you will see the message: WARN [commander] Takeoff denied, disarm and re-try After home position is set check the local position reported by 'pos' command : Local position: x=-0.0326988, y=0.00656854, z=5.48506 If the z coordinate is large like this then takeoff might not work as expected. Resetting the SITL and simulation should fix that problem.","title":"Check the Home Position"},{"location":"px4_sitl/#no-remote-control","text":"If you plan to fly with no remote control, just using DroneShell commands for example, then you will need to set the following parameters to stop the PX4 from triggering \"failsafe mode on\" every time a move command is finished. param set NAV_RCL_ACT 0 param set NAV_DLL_ACT 0 NOTE: Do NOT do this on a real drone as it is too dangerous to fly without these failsafe measures.","title":"No Remote Control"},{"location":"px4_sitl/#using-virtualbox-ubuntu","text":"If you want to run the above posix_sitl in a VirtualBox Ubuntu machine then it will have a different ip address from localhost. So in this case you need to edit the settings file and change the UdpIp and SitlIp to the ip address of your virtual machine set the LocalIpAddress to the address of your host machine running the Unreal engine.","title":"Using VirtualBox Ubuntu"},{"location":"px4_sitl/#remote-controller","text":"There are several options for flying the simulated drone using a remote control or joystick like xbox gamepad. See remote controllers","title":"Remote Controller"},{"location":"reinforcement_learning/","text":"Reinforcement Learning in AirSim We below describe how we can implement DQN in AirSim using CNTK. The easiest way is to first install python only CNTK ( instructions ). CNTK provides several demo examples of deep RL . We will modify the DeepQNeuralNetwork.py to work with AirSim. We can utilize most of the classes and methods corresponding to the DQN algorithm. However, there are certain additions we need to make for AirSim. Disclaimer This is still in active development. What we share below is a framework that can be extended and tweaked to obtain better performance. RL with Car Source code This example works with AirSimNeighborhood environment available in releases . First, we need to get the images from simulation and transform them appropriately. Below, we show how a depth image can be obtained from the ego camera and transformed to an 84X84 input to the network. (you can use other sensor modalities, and sensor inputs as well \u2013 of course you\u2019ll have to modify the code accordingly). responses = client.simGetImages([ImageRequest(0, AirSimImageType.DepthPerspective, True, False)]) current_state = transform_input(responses) We further define the six actions (breaking, straight with throttle, full-left with throttle, full-right with throttle, half-left with throttle, half-right with throttle) that an agent can execute. This is done via the function interpret_action : def interpret_action(action): car_controls.brake = 0 car_controls.throttle = 1 if action == 0: car_controls.throttle = 0 car_controls.brake = 1 elif action == 1: car_controls.steering = 0 elif action == 2: car_controls.steering = 0.5 elif action == 3: car_controls.steering = -0.5 elif action == 4: car_controls.steering = 0.25 else: car_controls.steering = -0.25 return car_controls We then define the reward function in compute_reward as a convex combination of how fast the vehicle is travelling and how much it deviates from the center line. The agent gets a high reward when its moving fast and staying in the center of the lane. def compute_reward(car_state): MAX_SPEED = 300 MIN_SPEED = 10 thresh_dist = 3.5 beta = 3 z = 0 pts = [np.array([0, -1, z]), np.array([130, -1, z]), np.array([130, 125, z]), np.array([0, 125, z]), np.array([0, -1, z]), np.array([130, -1, z]), np.array([130, -128, z]), np.array([0, -128, z]), np.array([0, -1, z])] pd = car_state.position car_pt = np.array(list(pd.values())) dist = 10000000 for i in range(0, len(pts)-1): dist = min(dist, np.linalg.norm(np.cross((car_pt - pts[i]), (car_pt - pts[i+1])))/np.linalg.norm(pts[i]-pts[i+1])) #print(dist) if dist > thresh_dist: reward = -3 else: reward_dist = (math.exp(-beta*dist) - 0.5) reward_speed = (((car_state.speed - MIN_SPEED)/(MAX_SPEED - MIN_SPEED)) - 0.5) reward = reward_dist + reward_speed return reward The function isDone determines if the episode has terminated (e.g. due to collision). We look at the speed of the vehicle and if it is less than a threshold than the episode is considered to be terminated. def isDone(car_state, car_controls, reward): done = 0 if reward < -1: done = 1 if car_controls.brake == 0: if car_state.speed <= 5: done = 1 return done The main loop then sequences through obtaining the image, computing the action to take according to the current policy, getting a reward and so forth. If the episode terminates then we reset the vehicle to the original state via: client.reset() client.enableApiControl(True) client.armDisarm(True) car_control = interpret_action(1) // Reset position and drive straight for one second client.setCarControls(car_control) time.sleep(1) Note that the simulation needs to be up and running before you execute DQNcar.py. The video below shows first few episodes of DQN training. RL with Quadrotor Source code This example works with AirSimMountainLandscape environment available in releases . We can similarly apply RL for various autonomous flight scenarios with quadrotors. Below is an example on how RL could be used to train quadrotors to follow high tension power lines (e.g. application for energy infrastructure inspection). There are seven actions here that correspond to different directions in which the quadrotor can move in (six directions + one hovering action). def interpret_action(action): if action == 0: quad_offset = (0, 0, 0) elif action == 1: quad_offset = (1, 0, 0) elif action == 2: quad_offset = (0, 1, 0) elif action == 3: quad_offset = (0, 0, 1) elif action == 4: quad_offset = (-1, 0, 0) elif action == 5: quad_offset = (0, -1, 0) elif action == 6: quad_offset = (0, 0, -1) return quad_offset The reward again is a function how how fast the quad travels in conjunction with how far it gets from the known powerlines. def compute_reward(quad_state, quad_vel, collision_info): thresh_dist = 10 beta = 1 z = -10 pts = [np.array([0, 0, z]), np.array([130, 0, z]), np.array([130, 125, z]), np.array([0, 125, z]), np.array([0, 0, z]), np.array([130, 0, z]), np.array([130, -128, z]), np.array([0, -128, z]), np.array([0, 0, z])] quad_pt = np.array(list((quad_state.x_val, quad_state.y_val, quad_state.z_val))) if collision_info.has_collided: reward = -100 else: dist = 10000000 for i in range(0, len(pts)-1): dist = min(dist, np.linalg.norm(np.cross((quad_pt - pts[i]), (quad_pt - pts[i+1])))/np.linalg.norm(pts[i]-pts[i+1])) if dist > thresh_dist: reward = -10 else: reward = 2*(0.5 - math.exp(-beta*dist)) + np.linalg.norm([quad_vel.x_val, quad_vel.y_val, quad_vel.z_val]) return reward We consider an episode to terminate if it drifts too much away from the known power line coordinates. The reset function here flies the quadrotor to the initial starting point: if done: client.moveToZAsync(clearZ, 2).join() client.moveToPositionAsync(initX, initY, clearZ, 2).join() client.moveToPositionAsync(initZ, initY, initZ, 2).join() current_step +=1 Here is the video of first few episodes during the training. Related Please also see The Autonomous Driving Cookbook by Microsoft Deep Learning and Robotics Garage Chapter.","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#reinforcement-learning-in-airsim","text":"We below describe how we can implement DQN in AirSim using CNTK. The easiest way is to first install python only CNTK ( instructions ). CNTK provides several demo examples of deep RL . We will modify the DeepQNeuralNetwork.py to work with AirSim. We can utilize most of the classes and methods corresponding to the DQN algorithm. However, there are certain additions we need to make for AirSim.","title":"Reinforcement Learning in AirSim"},{"location":"reinforcement_learning/#disclaimer","text":"This is still in active development. What we share below is a framework that can be extended and tweaked to obtain better performance.","title":"Disclaimer"},{"location":"reinforcement_learning/#rl-with-car","text":"Source code This example works with AirSimNeighborhood environment available in releases . First, we need to get the images from simulation and transform them appropriately. Below, we show how a depth image can be obtained from the ego camera and transformed to an 84X84 input to the network. (you can use other sensor modalities, and sensor inputs as well \u2013 of course you\u2019ll have to modify the code accordingly). responses = client.simGetImages([ImageRequest(0, AirSimImageType.DepthPerspective, True, False)]) current_state = transform_input(responses) We further define the six actions (breaking, straight with throttle, full-left with throttle, full-right with throttle, half-left with throttle, half-right with throttle) that an agent can execute. This is done via the function interpret_action : def interpret_action(action): car_controls.brake = 0 car_controls.throttle = 1 if action == 0: car_controls.throttle = 0 car_controls.brake = 1 elif action == 1: car_controls.steering = 0 elif action == 2: car_controls.steering = 0.5 elif action == 3: car_controls.steering = -0.5 elif action == 4: car_controls.steering = 0.25 else: car_controls.steering = -0.25 return car_controls We then define the reward function in compute_reward as a convex combination of how fast the vehicle is travelling and how much it deviates from the center line. The agent gets a high reward when its moving fast and staying in the center of the lane. def compute_reward(car_state): MAX_SPEED = 300 MIN_SPEED = 10 thresh_dist = 3.5 beta = 3 z = 0 pts = [np.array([0, -1, z]), np.array([130, -1, z]), np.array([130, 125, z]), np.array([0, 125, z]), np.array([0, -1, z]), np.array([130, -1, z]), np.array([130, -128, z]), np.array([0, -128, z]), np.array([0, -1, z])] pd = car_state.position car_pt = np.array(list(pd.values())) dist = 10000000 for i in range(0, len(pts)-1): dist = min(dist, np.linalg.norm(np.cross((car_pt - pts[i]), (car_pt - pts[i+1])))/np.linalg.norm(pts[i]-pts[i+1])) #print(dist) if dist > thresh_dist: reward = -3 else: reward_dist = (math.exp(-beta*dist) - 0.5) reward_speed = (((car_state.speed - MIN_SPEED)/(MAX_SPEED - MIN_SPEED)) - 0.5) reward = reward_dist + reward_speed return reward The function isDone determines if the episode has terminated (e.g. due to collision). We look at the speed of the vehicle and if it is less than a threshold than the episode is considered to be terminated. def isDone(car_state, car_controls, reward): done = 0 if reward < -1: done = 1 if car_controls.brake == 0: if car_state.speed <= 5: done = 1 return done The main loop then sequences through obtaining the image, computing the action to take according to the current policy, getting a reward and so forth. If the episode terminates then we reset the vehicle to the original state via: client.reset() client.enableApiControl(True) client.armDisarm(True) car_control = interpret_action(1) // Reset position and drive straight for one second client.setCarControls(car_control) time.sleep(1) Note that the simulation needs to be up and running before you execute DQNcar.py. The video below shows first few episodes of DQN training.","title":"RL with Car"},{"location":"reinforcement_learning/#rl-with-quadrotor","text":"Source code This example works with AirSimMountainLandscape environment available in releases . We can similarly apply RL for various autonomous flight scenarios with quadrotors. Below is an example on how RL could be used to train quadrotors to follow high tension power lines (e.g. application for energy infrastructure inspection). There are seven actions here that correspond to different directions in which the quadrotor can move in (six directions + one hovering action). def interpret_action(action): if action == 0: quad_offset = (0, 0, 0) elif action == 1: quad_offset = (1, 0, 0) elif action == 2: quad_offset = (0, 1, 0) elif action == 3: quad_offset = (0, 0, 1) elif action == 4: quad_offset = (-1, 0, 0) elif action == 5: quad_offset = (0, -1, 0) elif action == 6: quad_offset = (0, 0, -1) return quad_offset The reward again is a function how how fast the quad travels in conjunction with how far it gets from the known powerlines. def compute_reward(quad_state, quad_vel, collision_info): thresh_dist = 10 beta = 1 z = -10 pts = [np.array([0, 0, z]), np.array([130, 0, z]), np.array([130, 125, z]), np.array([0, 125, z]), np.array([0, 0, z]), np.array([130, 0, z]), np.array([130, -128, z]), np.array([0, -128, z]), np.array([0, 0, z])] quad_pt = np.array(list((quad_state.x_val, quad_state.y_val, quad_state.z_val))) if collision_info.has_collided: reward = -100 else: dist = 10000000 for i in range(0, len(pts)-1): dist = min(dist, np.linalg.norm(np.cross((quad_pt - pts[i]), (quad_pt - pts[i+1])))/np.linalg.norm(pts[i]-pts[i+1])) if dist > thresh_dist: reward = -10 else: reward = 2*(0.5 - math.exp(-beta*dist)) + np.linalg.norm([quad_vel.x_val, quad_vel.y_val, quad_vel.z_val]) return reward We consider an episode to terminate if it drifts too much away from the known power line coordinates. The reset function here flies the quadrotor to the initial starting point: if done: client.moveToZAsync(clearZ, 2).join() client.moveToPositionAsync(initX, initY, clearZ, 2).join() client.moveToPositionAsync(initZ, initY, initZ, 2).join() current_step +=1 Here is the video of first few episodes during the training.","title":"RL with Quadrotor"},{"location":"reinforcement_learning/#related","text":"Please also see The Autonomous Driving Cookbook by Microsoft Deep Learning and Robotics Garage Chapter.","title":"Related"},{"location":"remote_control/","text":"Remote Control To fly manually, you need remote control or RC. If you don't have one then you can use APIs to fly programmatically or use so-called Computer Vision mode to move around using keyboard. RC Setup for Default Config By default AirSim uses simple_flight as its flight controller which connects to RC via USB port to your computer. You can either use XBox controller or FrSky Taranis X9D Plus . Note that XBox 360 controller is not precise enough and is not recommended if you wanted more real world experience. See FAQ below if things are not working. ### Other Devices AirSim can detect large variety of devices however devices other than above might need extra configuration. In future we will add ability to set this config through settings.json. For now, if things are not working then you might want to try workarounds such as x360ce or chnage code in SimJoystick.cpp file . ### Note on FrSky Taranis X9D Plus FrSky Taranis X9D Plus is real UAV remote control with an advantage that it has USB port so it can be directly connected to PC. You can download AirSim config file and follow this tutorial to import it in your RC. You should then see \"sim\" model in RC with all channels configured properly. Note on Linux Currently default config on Linux is for using Xbox controller. This means other devices might not work properly. In future we will add ability to configure RC in settings.json but for now you might have to change code in SimJoystick.cpp file to use other devices. RC Setup for PX4 AirSim supports PX4 flight controller however it requires different setup. There are many remote control options that you can use with quadrotors. We have successfully used FrSky Taranis X9D Plus, FlySky FS-TH9X and Futaba 14SG with AirSim. Following are the high level steps to configure your RC: If you are going to use Hardware-in-Loop mode, you need transmitter for your specific brand of RC and bind it. You can find this information in RC's user guide. For Hardware-in-Loop mode, you connect transmitter to Pixhawk. Usually you can find online doc or YouTube video tutorial on how to do that. Calibrate your RC in QGroundControl . See PX4 RC configuration and Please see this guide for more information. Using XBox 360 USB Gamepad You can also use an xbox controller in SITL mode, it just won't be as precise as a real RC controller. See xbox controller for details on how to set that up. Using Playstation 3 controller A Playstation 3 controller is confirmed to work as an AirSim controller. On Windows, an emulator to make it look like an Xbox 360 controller, is required however. Many different solutions are available online, for example x360ce Xbox 360 Controller Emulator . DJI Controller Nils Tijtgat wrote an excellent blog on how to get the DJI controller working with AirSim . FAQ I'm using default config and AirSim says my RC is not detected on USB. This typically happens if you have multiple RCs and or XBox/Playstation gamepads etc connected. In Windows, hit Windows+S key and search for \"Set up USB Game controllers\" (in older versions of Windows try \"joystick\"). This will show you all game controllers connected to your PC. If you don't see yours than Windows haven't detected it and so you need to first solve that issue. If you do see yours but not at the top of the list (i.e. index 0) than you need to tell AirSim because AirSim by default tries to use RC at index 0. To do this, navigate to your ~/Documents/AirSim folder, open up settings.json and add/modify following setting. Below tells AirSim to use RC at index = 2. { \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"RC\": { \"RemoteControlID\": 2 } } } } Vehicle seems unstable when using XBox/PS3 controller Regular gamepads are not very precise and have lot of random noise. Most of the times you may see significant offsets as well (i.e. output is not zero when sticks are at zero). So this behavior is expected. Where is RC calibration in AirSim? We haven't implemented it yet. This means your RC firmware will need to have a capability to do calibration for now. My RC is not working with PX4 setup. First you want to make sure your RC is working in QGroundControl . If it doesn't then it will sure not work in AirSim. The PX4 mode is suitable for folks who have at least intermediate level of experience to deal with various issues related to PX4 and we would generally refer you to get help from PX4 forums.","title":"Remote Control"},{"location":"remote_control/#remote-control","text":"To fly manually, you need remote control or RC. If you don't have one then you can use APIs to fly programmatically or use so-called Computer Vision mode to move around using keyboard.","title":"Remote Control"},{"location":"remote_control/#rc-setup-for-default-config","text":"By default AirSim uses simple_flight as its flight controller which connects to RC via USB port to your computer. You can either use XBox controller or FrSky Taranis X9D Plus . Note that XBox 360 controller is not precise enough and is not recommended if you wanted more real world experience. See FAQ below if things are not working. ### Other Devices AirSim can detect large variety of devices however devices other than above might need extra configuration. In future we will add ability to set this config through settings.json. For now, if things are not working then you might want to try workarounds such as x360ce or chnage code in SimJoystick.cpp file . ### Note on FrSky Taranis X9D Plus FrSky Taranis X9D Plus is real UAV remote control with an advantage that it has USB port so it can be directly connected to PC. You can download AirSim config file and follow this tutorial to import it in your RC. You should then see \"sim\" model in RC with all channels configured properly.","title":"RC Setup for Default Config"},{"location":"remote_control/#note-on-linux","text":"Currently default config on Linux is for using Xbox controller. This means other devices might not work properly. In future we will add ability to configure RC in settings.json but for now you might have to change code in SimJoystick.cpp file to use other devices.","title":"Note on Linux"},{"location":"remote_control/#rc-setup-for-px4","text":"AirSim supports PX4 flight controller however it requires different setup. There are many remote control options that you can use with quadrotors. We have successfully used FrSky Taranis X9D Plus, FlySky FS-TH9X and Futaba 14SG with AirSim. Following are the high level steps to configure your RC: If you are going to use Hardware-in-Loop mode, you need transmitter for your specific brand of RC and bind it. You can find this information in RC's user guide. For Hardware-in-Loop mode, you connect transmitter to Pixhawk. Usually you can find online doc or YouTube video tutorial on how to do that. Calibrate your RC in QGroundControl . See PX4 RC configuration and Please see this guide for more information.","title":"RC Setup for PX4"},{"location":"remote_control/#using-xbox-360-usb-gamepad","text":"You can also use an xbox controller in SITL mode, it just won't be as precise as a real RC controller. See xbox controller for details on how to set that up.","title":"Using XBox 360 USB Gamepad"},{"location":"remote_control/#using-playstation-3-controller","text":"A Playstation 3 controller is confirmed to work as an AirSim controller. On Windows, an emulator to make it look like an Xbox 360 controller, is required however. Many different solutions are available online, for example x360ce Xbox 360 Controller Emulator .","title":"Using Playstation 3 controller"},{"location":"remote_control/#dji-controller","text":"Nils Tijtgat wrote an excellent blog on how to get the DJI controller working with AirSim .","title":"DJI Controller"},{"location":"remote_control/#faq","text":"","title":"FAQ"},{"location":"remote_control/#im-using-default-config-and-airsim-says-my-rc-is-not-detected-on-usb","text":"This typically happens if you have multiple RCs and or XBox/Playstation gamepads etc connected. In Windows, hit Windows+S key and search for \"Set up USB Game controllers\" (in older versions of Windows try \"joystick\"). This will show you all game controllers connected to your PC. If you don't see yours than Windows haven't detected it and so you need to first solve that issue. If you do see yours but not at the top of the list (i.e. index 0) than you need to tell AirSim because AirSim by default tries to use RC at index 0. To do this, navigate to your ~/Documents/AirSim folder, open up settings.json and add/modify following setting. Below tells AirSim to use RC at index = 2. { \"SettingsVersion\": 1.2, \"SimMode\": \"Multirotor\", \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"RC\": { \"RemoteControlID\": 2 } } } }","title":"I'm using default config and AirSim says my RC is not detected on USB."},{"location":"remote_control/#vehicle-seems-unstable-when-using-xboxps3-controller","text":"Regular gamepads are not very precise and have lot of random noise. Most of the times you may see significant offsets as well (i.e. output is not zero when sticks are at zero). So this behavior is expected.","title":"Vehicle seems unstable when using XBox/PS3 controller"},{"location":"remote_control/#where-is-rc-calibration-in-airsim","text":"We haven't implemented it yet. This means your RC firmware will need to have a capability to do calibration for now.","title":"Where is RC calibration in AirSim?"},{"location":"remote_control/#my-rc-is-not-working-with-px4-setup","text":"First you want to make sure your RC is working in QGroundControl . If it doesn't then it will sure not work in AirSim. The PX4 mode is suitable for folks who have at least intermediate level of experience to deal with various issues related to PX4 and we would generally refer you to get help from PX4 forums.","title":"My RC is not working with PX4 setup."},{"location":"ros/","text":"How to use AirSim with Robot Operating System (ROS) AirSim and ROS can be integrated using Python. Some example ROS node are provided demonstrating how to publish data from AirSim as ROS topics. Prerequisites These instructions are for Ubuntu 20.04, ROS Noetic, UE4 4.24.4 and latest AirSim release. You should have these components installed and working before proceeding. Publish node There is one single Python script airsim_publish.py that can be used as a ROS Node. It can be used in two ways: - Get and publish the entire TF tree of map, vehicle and sensors; vehicle movement groundtruth ; all sensor data as well as the poses of world objects. - Replays a route rosbag that holds an existing trajectory of a vehicle. The script will then replay this trajectory while recording all sensor data for each pose of the trajectory. It generates a new rosbag holding both the route and sensor data as well as all TF information. This allows for better performance and deterministic datasets over the same route. Example launch files Some basic launch files are available for the ROS node in these two configurations mentioned above. - airsim_publish.launch : This shows all available parameters for the node. It also shows how to use the node in the first configuration. - record_route.launch : This is a variant of the one above but only exposing and enabling those to create a route rosbag for the second configuration. It will automatically record a rosbag as well. - replay_route_record_sensors.launch : This is the script to use a route rosbag created with the previous launch file type to replay it and record all sensor and TF data and create a single merged rosbag. 2 other launch files are also available, which match the configurations of the two example vehicles (car and drone) found in the example settings.json file found here . Setup Setup workspace and Airsim package Option A: Create a new ROS package in your catkin workspace following these instructions. Create a new ROS package called AirSim or whatever you like. If you don't already have a catkin workspace, you should first work through the ROS beginner tutorials. In the ROS package directory you made, copy the ROS node scripts from the AirSim/ros/python_ws/src/airsim directory to your ROS package. Change the code below to match your AirSim and catkin workspace paths. cp AirSim/ros/python_ws/src/airsim/scripts ../catkin_ws/src/airsim Option B: Use provided workspace Airsim/ros/python_ws itself is already a workspace which can be used out of the box after building. For building see below. Build ROS AirSim package Change directory to your top level catkin workspace folder i.e. cd ~/catkin_ws and run catkin_make This will build the AirSim package. Next, run source devel/setup.bash so ROS can find the new package. You can add this command to your ~/.bashrc to load your catkin workspace automatically. NOTE FOR OLDER ROS VERSIONS: If you use Python2, change the scripts to use #!/usr/bin/env python at the top instead of #!/usr/bin/env python3 How to run ROS AirSim nodes First make sure you are running an AirSim project and that the simulation is playing. The implemented AirSim node can be run using rosrun airsim airsim_publish.py . Or alternatively you can use launch files such as the example ones that can be found in AirSim/ros/python_ws/src/airsim/launch like rosrun airsim airsim_publish.launch . You can also make use of WSL to run ROS nodes on Windows. See here for more information.","title":"ROS: AirSim ROS Wrapper"},{"location":"ros/#how-to-use-airsim-with-robot-operating-system-ros","text":"AirSim and ROS can be integrated using Python. Some example ROS node are provided demonstrating how to publish data from AirSim as ROS topics.","title":"How to use AirSim with Robot Operating System (ROS)"},{"location":"ros/#prerequisites","text":"These instructions are for Ubuntu 20.04, ROS Noetic, UE4 4.24.4 and latest AirSim release. You should have these components installed and working before proceeding.","title":"Prerequisites"},{"location":"ros/#publish-node","text":"There is one single Python script airsim_publish.py that can be used as a ROS Node. It can be used in two ways: - Get and publish the entire TF tree of map, vehicle and sensors; vehicle movement groundtruth ; all sensor data as well as the poses of world objects. - Replays a route rosbag that holds an existing trajectory of a vehicle. The script will then replay this trajectory while recording all sensor data for each pose of the trajectory. It generates a new rosbag holding both the route and sensor data as well as all TF information. This allows for better performance and deterministic datasets over the same route.","title":"Publish node"},{"location":"ros/#example-launch-files","text":"Some basic launch files are available for the ROS node in these two configurations mentioned above. - airsim_publish.launch : This shows all available parameters for the node. It also shows how to use the node in the first configuration. - record_route.launch : This is a variant of the one above but only exposing and enabling those to create a route rosbag for the second configuration. It will automatically record a rosbag as well. - replay_route_record_sensors.launch : This is the script to use a route rosbag created with the previous launch file type to replay it and record all sensor and TF data and create a single merged rosbag. 2 other launch files are also available, which match the configurations of the two example vehicles (car and drone) found in the example settings.json file found here .","title":"Example launch files"},{"location":"ros/#setup","text":"","title":"Setup"},{"location":"ros/#setup-workspace-and-airsim-package","text":"","title":"Setup workspace and Airsim package"},{"location":"ros/#option-a-create-a-new-ros-package-in-your-catkin-workspace-following-these-instructions","text":"Create a new ROS package called AirSim or whatever you like. If you don't already have a catkin workspace, you should first work through the ROS beginner tutorials. In the ROS package directory you made, copy the ROS node scripts from the AirSim/ros/python_ws/src/airsim directory to your ROS package. Change the code below to match your AirSim and catkin workspace paths. cp AirSim/ros/python_ws/src/airsim/scripts ../catkin_ws/src/airsim","title":"Option A: Create a new ROS package in your catkin workspace following these instructions."},{"location":"ros/#option-b-use-provided-workspace","text":"Airsim/ros/python_ws itself is already a workspace which can be used out of the box after building. For building see below.","title":"Option B: Use provided workspace"},{"location":"ros/#build-ros-airsim-package","text":"Change directory to your top level catkin workspace folder i.e. cd ~/catkin_ws and run catkin_make This will build the AirSim package. Next, run source devel/setup.bash so ROS can find the new package. You can add this command to your ~/.bashrc to load your catkin workspace automatically. NOTE FOR OLDER ROS VERSIONS: If you use Python2, change the scripts to use #!/usr/bin/env python at the top instead of #!/usr/bin/env python3","title":"Build ROS AirSim package"},{"location":"ros/#how-to-run-ros-airsim-nodes","text":"First make sure you are running an AirSim project and that the simulation is playing. The implemented AirSim node can be run using rosrun airsim airsim_publish.py . Or alternatively you can use launch files such as the example ones that can be found in AirSim/ros/python_ws/src/airsim/launch like rosrun airsim airsim_publish.launch . You can also make use of WSL to run ROS nodes on Windows. See here for more information.","title":"How to run ROS AirSim nodes"},{"location":"sensors/","text":"Sensors in AirSim AirSim currently supports the following sensors. Each sensor is associated with a integer enum specifying its sensor type. Camera Barometer = 1 Imu = 2 Gps = 3 Magnetometer = 4 Distance Sensor = 5 LiDAR = 6 Echo = 7 GPULiDAR = 8 Note : Cameras are configured differently than the other sensors and do not have an enum associated with them. Look at general settings and image API for camera config and API. Default sensors If no sensors are specified in the settings json , the the following sensors are enabled by default based on the sim mode. Multirotor Imu Magnetometer Gps Barometer Car Gps ComputerVision None Behind the scenes, 'createDefaultSensorSettings' method in AirSimSettings.hpp which sets up the above sensors with their default parameters, depending on the sim mode specified in the settings.json file. Configuring the default sensor list The default sensor list can be configured in settings json: \"DefaultSensors\": { \"Barometer\": { \"SensorType\": 1, \"Enabled\" : true }, \"Imu\": { \"SensorType\": 2, \"Enabled\" : true }, \"Gps\": { \"SensorType\": 3, \"Enabled\" : true }, \"Magnetometer\": { \"SensorType\": 4, \"Enabled\" : true }, \"Distance\": { \"SensorType\": 5, \"Enabled\" : true }, \"Lidar\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 4, \"PointsPerSecond\": 10000 }, \"Echo\": { \"SensorType\": 7, \"Enabled\" : true, \"NumberOfChannels\": 4, \"PointsPerSecond\": 10000 }, \"GPULidar\": { \"SensorType\": 8, \"Enabled\" : true, \"NumberOfChannels\": 4 } }, Configuring vehicle-specific sensor list If a vehicle provides its sensor list, it must provide the whole list. Selective add/remove/update of the default sensor list is NOT supported. A (vehicle specific) sensor list can be specified in the vehicle settings part of the json. e.g., \"Vehicles\": { \"Drone1\": { \"VehicleType\": \"SimpleFlight\", \"AutoCreate\": true, ... \"Sensors\": { \"MyLidar1\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 16, \"PointsPerSecond\": 10000, \"X\": 0, \"Y\": 0, \"Z\": -1, \"DrawDebugPoints\": true }, \"MyLidar2\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 4, \"PointsPerSecond\": 10000, \"X\": 0, \"Y\": 0, \"Z\": -1, \"DrawDebugPoints\": true } } } } Sensor specific settings Each sensor-type has its own set of settings as well. Please see LiDAR for example of LiDAR specific settings. Please see echo for example of Echo specific settings. Please see GPU LiDAR for example of GPU LiDAR specific settings. Sensor APIs Barometer python barometer_data = getBarometerData(barometer_name = \"\", vehicle_name = \"\") IMU python imu_data = getImuData(imu_name = \"\", vehicle_name = \"\") GPS python gps_data = getGpsData(gps_name = \"\", vehicle_name = \"\") Magnetometer python magnetometer_data = getMagnetometerData(magnetometer_name = \"\", vehicle_name = \"\") Distance sensor python distance_sensor_name = getDistanceSensorData(distance_sensor_name = \"\", vehicle_name = \"\") LiDAR See LiDAR for LiDAR API. Echo See echo for Echo API. GPU LiDAR See GPU Lidar for GPU LiDAR API.","title":"Sensors"},{"location":"sensors/#sensors-in-airsim","text":"AirSim currently supports the following sensors. Each sensor is associated with a integer enum specifying its sensor type. Camera Barometer = 1 Imu = 2 Gps = 3 Magnetometer = 4 Distance Sensor = 5 LiDAR = 6 Echo = 7 GPULiDAR = 8 Note : Cameras are configured differently than the other sensors and do not have an enum associated with them. Look at general settings and image API for camera config and API.","title":"Sensors in AirSim"},{"location":"sensors/#default-sensors","text":"If no sensors are specified in the settings json , the the following sensors are enabled by default based on the sim mode.","title":"Default sensors"},{"location":"sensors/#multirotor","text":"Imu Magnetometer Gps Barometer","title":"Multirotor"},{"location":"sensors/#car","text":"Gps","title":"Car"},{"location":"sensors/#computervision","text":"None Behind the scenes, 'createDefaultSensorSettings' method in AirSimSettings.hpp which sets up the above sensors with their default parameters, depending on the sim mode specified in the settings.json file.","title":"ComputerVision"},{"location":"sensors/#configuring-the-default-sensor-list","text":"The default sensor list can be configured in settings json: \"DefaultSensors\": { \"Barometer\": { \"SensorType\": 1, \"Enabled\" : true }, \"Imu\": { \"SensorType\": 2, \"Enabled\" : true }, \"Gps\": { \"SensorType\": 3, \"Enabled\" : true }, \"Magnetometer\": { \"SensorType\": 4, \"Enabled\" : true }, \"Distance\": { \"SensorType\": 5, \"Enabled\" : true }, \"Lidar\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 4, \"PointsPerSecond\": 10000 }, \"Echo\": { \"SensorType\": 7, \"Enabled\" : true, \"NumberOfChannels\": 4, \"PointsPerSecond\": 10000 }, \"GPULidar\": { \"SensorType\": 8, \"Enabled\" : true, \"NumberOfChannels\": 4 } },","title":"Configuring the default sensor list"},{"location":"sensors/#configuring-vehicle-specific-sensor-list","text":"If a vehicle provides its sensor list, it must provide the whole list. Selective add/remove/update of the default sensor list is NOT supported. A (vehicle specific) sensor list can be specified in the vehicle settings part of the json. e.g., \"Vehicles\": { \"Drone1\": { \"VehicleType\": \"SimpleFlight\", \"AutoCreate\": true, ... \"Sensors\": { \"MyLidar1\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 16, \"PointsPerSecond\": 10000, \"X\": 0, \"Y\": 0, \"Z\": -1, \"DrawDebugPoints\": true }, \"MyLidar2\": { \"SensorType\": 6, \"Enabled\" : true, \"NumberOfChannels\": 4, \"PointsPerSecond\": 10000, \"X\": 0, \"Y\": 0, \"Z\": -1, \"DrawDebugPoints\": true } } } }","title":"Configuring vehicle-specific sensor list"},{"location":"sensors/#sensor-specific-settings","text":"Each sensor-type has its own set of settings as well. Please see LiDAR for example of LiDAR specific settings. Please see echo for example of Echo specific settings. Please see GPU LiDAR for example of GPU LiDAR specific settings.","title":"Sensor specific settings"},{"location":"sensors/#sensor-apis","text":"Barometer python barometer_data = getBarometerData(barometer_name = \"\", vehicle_name = \"\") IMU python imu_data = getImuData(imu_name = \"\", vehicle_name = \"\") GPS python gps_data = getGpsData(gps_name = \"\", vehicle_name = \"\") Magnetometer python magnetometer_data = getMagnetometerData(magnetometer_name = \"\", vehicle_name = \"\") Distance sensor python distance_sensor_name = getDistanceSensorData(distance_sensor_name = \"\", vehicle_name = \"\") LiDAR See LiDAR for LiDAR API. Echo See echo for Echo API. GPU LiDAR See GPU Lidar for GPU LiDAR API.","title":"Sensor APIs"},{"location":"settings/","text":"AirSim Settings Where are Settings Stored? Windows: Documents\\AirSim Linux: ~/Documents/AirSim The file is in usual json format . On first startup AirSim would create settings.json file with no settings. To avoid problems, always use ASCII format to save json file. A good usable example can also be found in the docs folder here . How to Chose Between Car and Multirotor? The default is to use multirotor. To use car simple set \"SimMode\": \"Car\" like this: { \"SettingsVersion\": 1.2, \"SimMode\": \"Car\" } To choose multirotor, set \"SimMode\": \"Multirotor\" . If you want to prompt user to select vehicle type then use \"SimMode\": \"\" . Available Settings and Their Defaults Below are complete list of settings available along with their default values. If any of the settings is missing from json file, then default value is used. Some default values are simply specified as \"\" which means actual value may be chosen based on the vehicle you are using. For example, ViewMode setting has default value \"\" which translates to \"FlyWithMe\" for drones and \"SpringArmChase\" for cars. WARNING: Do not copy paste all of below in your settings.json. We strongly recommend adding only those settings that you don't want default values. Only required element is \"SettingsVersion\" . { \"SimMode\": \"\", \"ClockType\": \"\", \"ClockSpeed\": 1, \"LocalHostIp\": \"127.0.0.1\", \"ApiServerPort\": 41451, \"RecordUIVisible\": true, \"LogMessagesVisible\": true, \"ViewMode\": \"\", \"RpcEnabled\": true, \"EngineSound\": true, \"PhysicsEngineName\": \"\", \"SpeedUnitFactor\": 1.0, \"SpeedUnitLabel\": \"m/s\", \"Recording\": { \"RecordOnMove\": false, \"RecordInterval\": 0.05, \"Cameras\": [ { \"CameraName\": \"0\", \"ImageType\": 0, \"PixelsAsFloat\": false, \"Compress\": true } ] }, \"CameraDefaults\": { \"CaptureSettings\": [ { \"ImageType\": 0, \"Width\": 256, \"Height\": 144, \"FOV_Degrees\": 90, \"AutoExposureSpeed\": 100, \"AutoExposureBias\": 0, \"AutoExposureMaxBrightness\": 0.64, \"AutoExposureMinBrightness\": 0.03, \"MotionBlurAmount\": 0, \"TargetGamma\": 1.0, \"ProjectionMode\": \"\", \"OrthoWidth\": 5.12, \"MotionBlurAmount\": 1, \"MotionBlurMax\": 10, \"ChromaticAberrationScale\": 2, \"IgnoreMarked\": false } ], \"NoiseSettings\": [ { \"Enabled\": false, \"ImageType\": 0, \"RandContrib\": 0.2, \"RandSpeed\": 100000.0, \"RandSize\": 500.0, \"RandDensity\": 2, \"HorzWaveContrib\":0.03, \"HorzWaveStrength\": 0.08, \"HorzWaveVertSize\": 1.0, \"HorzWaveScreenSize\": 1.0, \"HorzNoiseLinesContrib\": 1.0, \"HorzNoiseLinesDensityY\": 0.01, \"HorzNoiseLinesDensityXY\": 0.5, \"HorzDistortionContrib\": 1.0, \"HorzDistortionStrength\": 0.002, \"LensDistortionEnable\": true, \"LensDistortionAreaFalloff\": 2, \"LensDistortionAreaRadius\": 1, \"LensDistortionInvert\": false } ], \"Gimbal\": { \"Stabilization\": 0, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN } \"X\": NaN, \"Y\": NaN, \"Z\": NaN, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN }, \"OriginGeopoint\": { \"Latitude\": 47.641468, \"Longitude\": -122.140165, \"Altitude\": 122 }, \"TimeOfDay\": { \"Enabled\": false, \"StartDateTime\": \"\", \"CelestialClockSpeed\": 1, \"StartDateTimeDst\": false, \"UpdateIntervalSecs\": 60 }, \"SubWindows\": [ {\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"Visible\": false}, {\"WindowID\": 1, \"CameraName\": \"0\", \"ImageType\": 5, \"Visible\": false}, {\"WindowID\": 2, \"CameraName\": \"0\", \"ImageType\": 0, \"Visible\": false} ], \"PawnPaths\": { \"BareboneCar\": {\"PawnBP\": \"Class'/AirSim/VehicleAdv/Vehicle/VehicleAdvPawn.VehicleAdvPawn_C'\"}, \"DefaultCar\": {\"PawnBP\": \"Class'/AirSim/VehicleAdv/SUV/SuvCarPawn.SuvCarPawn_C'\"}, \"DefaultQuadrotor\": {\"PawnBP\": \"Class'/AirSim/Blueprints/BP_FlyingPawn.BP_FlyingPawn_C'\"}, \"DefaultComputerVision\": {\"PawnBP\": \"Class'/AirSim/Blueprints/BP_ComputerVisionPawn.BP_ComputerVisionPawn_C'\"} }, \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"DefaultVehicleState\": \"Armed\", \"AutoCreate\": true, \"PawnPath\": \"\", \"EnableCollisionPassthrogh\": false, \"EnableCollisions\": true, \"AllowAPIAlways\": true, \"RC\": { \"RemoteControlID\": 0, \"AllowAPIWhenDisconnected\": false }, \"Cameras\": { //same elements as CameraDefaults above, key as name }, \"X\": NaN, \"Y\": NaN, \"Z\": NaN, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN }, \"PhysXCar\": { \"VehicleType\": \"PhysXCar\", \"DefaultVehicleState\": \"\", \"AutoCreate\": true, \"PawnPath\": \"\", \"EnableCollisionPassthrogh\": false, \"EnableCollisions\": true, \"RC\": { \"RemoteControlID\": -1 }, \"Cameras\": { \"MyCamera1\": { //same elements as elements inside CameraDefaults above }, \"MyCamera2\": { //same elements as elements inside CameraDefaults above }, }, \"X\": NaN, \"Y\": NaN, \"Z\": NaN, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN } } } SimMode SimMode determines which simulation mode will be used. Below are currently supported values: - \"\" : prompt user to select vehicle type multirotor or car - \"Multirotor\" : Use multirotor simulation - \"Car\" : Use car simulation - \"ComputerVision\" : Use only camera, no vehicle or physics - \"SkidVehicle\" : use skid-steering vehicle simulation - \"UrdfBot\" : use URDF Bot simulation. ViewMode The ViewMode determines which camera to use as default and how camera will follow the vehicle. For multirotors, the default ViewMode is \"FlyWithMe\" while for cars the default ViewMode is \"SpringArmChase\" . FlyWithMe : Chase the vehicle from behind with 6 degrees of freedom GroundObserver : Chase the vehicle from 6' above the ground but with full freedom in XY plane. Fpv : View the scene from front camera of vehicle Manual : Don't move camera automatically. Use arrow keys and ASWD keys for move camera manually. SpringArmChase : Chase the vehicle with camera mounted on (invisible) arm that is attached to the vehicle via spring (so it has some latency in movement). NoDisplay : This will freeze rendering for main screen however rendering for subwindows, recording and APIs remain active. This mode is useful to save resources in \"headless\" mode where you are only interested in getting images and don't care about what gets rendered on main screen. This may also improve FPS for recording images. TimeOfDay This setting controls the position of Sun in the environment. By default Enabled is false which means Sun's position is left at whatever was the default in the environment and it doesn't change over the time. If Enabled is true then Sun position is computed using longitude, latitude and altitude specified in OriginGeopoint section for the date specified in StartDateTime in the string format as %Y-%m-%d %H:%M:%S , for example, 2018-02-12 15:20:00 . If this string is empty then current date and time is used. If StartDateTimeDst is true then we adjust for day light savings time. The Sun's position is then continuously updated at the interval specified in UpdateIntervalSecs . In some cases, it might be desirable to have celestial clock run faster or slower than simulation clock. This can be specified using CelestialClockSpeed , for example, value 100 means for every 1 second of simulation clock, Sun's position is advanced by 100 seconds so Sun will move in sky much faster. Also see Time of Day API . OriginGeopoint This setting specifies the latitude, longitude and altitude of the Player Start component placed in the Unreal environment. The vehicle's home point is computed using this transformation. Note that all coordinates exposed via APIs are using NED system in SI units which means each vehicle starts at (0, 0, 0) in NED system. Time of Day settings are computed for geographical coordinates specified in OriginGeopoint . SubWindows This setting determines what is shown in each of 3 subwindows which are visible when you press 0 key. The WindowsID can be 0 to 2, CameraName is any available camera on the vehicle. ImageType integer value determines what kind of image gets shown according to ImageType enum . For example, for car vehicles below shows driver view, front bumper view and rear view as scene, depth and surface normals respectively. \"SubWindows\": [ {\"WindowID\": 0, \"ImageType\": 0, \"CameraName\": \"3\", \"Visible\": true}, {\"WindowID\": 1, \"ImageType\": 3, \"CameraName\": \"0\", \"Visible\": true}, {\"WindowID\": 2, \"ImageType\": 6, \"CameraName\": \"4\", \"Visible\": true} ] Recording The recording feature allows you to record data such as position, orientation, velocity along with the captured image at specified intervals. You can start recording by pressing red Record button on lower right or the R key. The data is stored in the Documents\\AirSim folder, in a time stamped subfolder for each recording session, as tab separated file. RecordInterval : specifies minimal interval in seconds between capturing two images. RecordOnMove : specifies that do not record frame if there was vehicle's position or orientation hasn't changed. Cameras : this element controls which cameras are used to capture images. By default scene image from camera 0 is recorded as compressed png format. This setting is json array so you can specify multiple cameras to capture images, each with potentially different image types . When PixelsAsFloat is true, image is saved as pfm file instead of png file. ClockSpeed This setting allows you to set the speed of simulation clock with respect to wall clock. For example, value of 5.0 would mean simulation clock has 5 seconds elapsed when wall clock has 1 second elapsed (i.e. simulation is running faster). The value of 0.1 means that simulation clock is 10X slower than wall clock. The value of 1 means simulation is running in real time. It is important to realize that quality of simulation may decrease as the simulation clock runs faster. You might see artifacts like object moving past obstacles because collision is not detected. However slowing down simulation clock (i.e. values < 1.0) generally improves the quality of simulation. Camera Settings The CameraDefaults element at root level specifies defaults used for all cameras. These defaults can be overridden for individual camera in Cameras element inside Vehicles as described later. Main settings Like other sensors the pose of the sensor in the vehicle frame can be defined by X, Y, Z, Roll, Pitch, Yaw parameters. Furthermore there are some other settings available: * DrawSensor : Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is. * External : Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates. * ExternalLocal : When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default. Note on ImageType element The ImageType element in JSON array determines which image type that settings applies to. The valid values are described in ImageType section . In addition, we also support special value ImageType: -1 to apply the settings to external camera (i.e. what you are looking at on the screen). For example, CaptureSettings element is json array so you can add settings for multiple image types easily. CaptureSettings The CaptureSettings determines how different image types such as scene, depth, disparity, surface normals and segmentation views are rendered. The Width, Height and FOV settings should be self explanatory. The AutoExposureSpeed decides how fast eye adaptation works. We set to generally high value such as 100 to avoid artifacts in image capture. Similarly we set MotionBlurAmount to 0 by default to avoid artifacts in ground truth images. The ProjectionMode decides the projection used by the capture camera and can take value \"perspective\" (default) or \"orthographic\". If projection mode is \"orthographic\" then OrthoWidth determines width of projected area captured in meters. To disable the rendering of certain objects on specific cameras or all, use the IgnoreMarked boolean setting. This requires to mark individual objects that have to be ignore using an Unreal Tag called MarkedIgnore . You can also tweak the motion blur and chromatic Aberration here. For explanation of other settings, please see this article . NoiseSettings The NoiseSettings allows to add noise to the specified image type with a goal of simulating camera sensor noise, interference and other artifacts. By default no noise is added, i.e., Enabled: false . If you set Enabled: true then following different types of noise and interference artifacts are enabled, each can be further tuned using setting. The noise effects are implemented as shader created as post processing material in Unreal Engine called CameraSensorNoise . Demo of camera noise and interference simulation: Random noise This adds random noise blobs with following parameters. * RandContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * RandSpeed : This determines how fast noise fluctuates, 1 means no fluctuation and higher values like 1E6 means full fluctuation. * RandSize : This determines how coarse noise is, 1 means every pixel has its own noise while higher value means more than 1 pixels share same noise value. * RandDensity : This determines how many pixels out of total will have noise, 1 means all pixels while higher value means lesser number of pixels (exponentially). Horizontal bump distortion This adds horizontal bumps / flickering / ghosting effect. * HorzWaveContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * HorzWaveStrength : This determines overall strength of the effect. * HorzWaveVertSize : This determines how many vertical pixels would be effected by the effect. * HorzWaveScreenSize : This determines how much of the screen is effected by the effect. Horizontal noise lines This adds regions of noise on horizontal lines. * HorzNoiseLinesContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * HorzNoiseLinesDensityY : This determines how many pixels in horizontal line gets affected. * HorzNoiseLinesDensityXY : This determines how many lines on screen gets affected. Horizontal line distortion This adds fluctuations on horizontal line. * HorzDistortionContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * HorzDistortionStrength : This determines how large is the distortion. Radial Lens Distortion This adds radial lens distortion to the camera sensor. * LensDistortionEnable : Enable or disable this feature * LensDistortionAreaFalloff : The size of the area to distort * LensDistortionAreaRadius : The distortion radius * LensDistortionInvert : Set to true to invert and create 'pincushion distortion' or false for 'barrel distortion' Gimbal The Gimbal element allows to freeze camera orientation for pitch, roll and/or yaw. This setting is ignored unless ImageType is -1. The Stabilization is defaulted to 0 meaning no gimbal i.e. camera orientation changes with body orientation on all axis. The value of 1 means full stabilization. The value between 0 to 1 acts as a weight for fixed angles specified (in degrees, in world-frame) in Pitch , Roll and Yaw elements and orientation of the vehicle body. When any of the angles is omitted from json or set to NaN, that angle is not stabilized (i.e. it moves along with vehicle body). Vehicles Settings Each simulation mode will go through the list of vehicles specified in this setting and create the ones that has \"AutoCreate\": true . Each vehicle specified in this setting has key which becomes the name of the vehicle. If \"Vehicles\" element is missing then this list is populated with default car named \"PhysXCar\" and default multirotor named \"SimpleFlight\". Common Vehicle Setting VehicleType : This could be either PhysXCar or BoxCar for the Car SimMode, SimpleFlight or PX4Multirotor for the MultiRotor SimMode, ComputerVision for the ComputerVision SimMode , CPHusky or Pioneer for SkidVehicle SimMode and UrdfBot for the UrdfBot SimMode. you can use There is no default value therefore this element must be specified. PawnPath : This allows to override the pawn blueprint to use for the vehicle. For example, you may create new pawn blueprint derived from ACarPawn for a warehouse robot in your own project outside the AirSim code and then specify its path here. See also PawnPaths . DefaultVehicleState : Possible value for multirotors is Armed or Disarmed . AutoCreate : If true then this vehicle would be spawned (if supported by selected sim mode). RC : This sub-element allows to specify which remote controller to use for vehicle using RemoteControlID . The value of -1 means use keyboard (not supported yet for multirotors). The value >= 0 specifies one of many remote controllers connected to the system. The list of available RCs can be seen in Game Controllers panel in Windows, for example. X, Y, Z, Yaw, Roll, Pitch : These elements allows you to specify the initial position and orientation of the vehicle. Position is in NED coordinates in SI units with origin set to Player Start location in Unreal environment. The orientation is specified in degrees. IsFpvVehicle : This setting allows to specify which vehicle camera will follow and the view that will be shown when ViewMode is set to Fpv. By default, AirSim selects the first vehicle in settings as FPV vehicle. Cameras : This element specifies camera settings for vehicle. The key in this element is name of the available camera and the value is same as CameraDefaults as described above. For example, to change FOV for the front center camera to 120 degrees, you can use this for Vehicles setting: \"Vehicles\": { \"FishEyeDrone\": { \"VehicleType\": \"SimpleFlight\", \"Cameras\": { \"front-center\": { \"CaptureSettings\": [ { \"ImageType\": 0, \"FOV_Degrees\": 120 } ] } } } } Using PX4 By default we use simple_flight so you don't have to do separate HITL or SITL setups. We also support \"PX4\" for advanced users. To use PX4 with AirSim, you can use the following for Vehicles setting: \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", } } Additional PX4 Settings The defaults for PX4 is to enable hardware-in-loop setup. There are various other settings available for PX4 as follows with their default values: \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", \"LogViewerHostIp\": \"127.0.0.1\", \"LogViewerPort\": 14388, \"OffboardCompID\": 1, \"OffboardSysID\": 134, \"QgcHostIp\": \"127.0.0.1\", \"QgcPort\": 14550, \"SerialBaudRate\": 115200, \"SerialPort\": \"*\", \"SimCompID\": 42, \"SimSysID\": 142, \"SitlIp\": \"127.0.0.1\", \"SitlPort\": 14556, \"UdpIp\": \"127.0.0.1\", \"UdpPort\": 14560, \"UseSerial\": true, \"VehicleCompID\": 1, \"VehicleSysID\": 135, \"Model\": \"Generic\", \"LocalHostIp\": \"127.0.0.1\" } } These settings define the MavLink SystemId and ComponentId for the Simulator (SimSysID, SimCompID), and for an optional external renderer (ExtRendererSysID, ExtRendererCompID) and the node that allows remote control of the drone from another app this is called the Air Control node (AirControlSysID, AirControlCompID). If you want the simulator to also talk to your ground control app (like QGroundControl) you can also set the UDP address for that in case you want to run that on a different machine (QgcHostIp,QgcPort). You can connect the simulator to the LogViewer app, provided in this repo, by setting the UDP address for that (LogViewerHostIp,LogViewerPort). And for each flying drone added to the simulator there is a named block of additional settings. In the above you see the default name \"PX4\". You can change this name from the Unreal Editor when you add a new BP_FlyingPawn asset. You will see these properties grouped under the category \"MavLink\". The MavLink node for this pawn can be remote over UDP or it can be connected to a local serial port. If serial then set UseSerial to true, otherwise set UseSerial to false and set the appropriate bard rate. The default of 115200 works with Pixhawk version 2 over USB. Other Settings URDF For aal URDF related settings see the seperate documentation . EngineSound To turn off the engine sound use setting \"EngineSound\": false . Currently this setting applies only to car. PawnPaths This allows you to specify your own vehicle pawn blueprints, for example, you can replace the default car in AirSim with your own car. Your vehicle BP can reside in Content folder of your own Unreal project (i.e. outside of AirSim plugin folder). For example, if you have a car BP located in file Content\\MyCar\\MySedanBP.uasset in your project then you can set \"DefaultCar\": {\"PawnBP\":\"Class'/Game/MyCar/MySedanBP.MySedanBP_C'\"} . The XYZ.XYZ_C is a special notation required to specify class for BP XYZ . Please note that your BP must be derived from CarPawn class. By default this is not the case but you can re-parent the BP using the \"Class Settings\" button in toolbar in UE editor after you open the BP and then choosing \"Car Pawn\" for Parent Class settings in Class Options. It's also a good idea to disable \"Auto Possess Player\" and \"Auto Possess AI\" as well as set AI Controller Class to None in BP details. Please make sure your asset is included for cooking in packaging options if you are creating binary. PhysicsEngineName For cars, we support only PhysX for now (regardless of value in this setting). For multirotors, we support \"FastPhysicsEngine\" only. LocalHostIp Setting Now when connecting to remote machines you may need to pick a specific Ethernet adapter to reach those machines, for example, it might be over Ethernet or over Wi-Fi, or some other special virtual adapter or a VPN. Your PC may have multiple networks, and those networks might not be allowed to talk to each other, in which case the UDP messages from one network will not get through to the others. So the LocalHostIp allows you to configure how you are reaching those machines. The default of 127.0.0.1 is not able to reach external machines, this default is only used when everything you are talking to is contained on a single PC. ApiServerPort This setting determines the server port that used by airsim clients, default port is 41451. By specifying different ports, the user can run multiple environments in parallel to accelerate data collection process. SpeedUnitFactor Unit conversion factor for speed related to m/s , default is 1. Used in conjunction with SpeedUnitLabel. This may be only used for display purposes for example on-display speed when car is being driven. For example, to get speed in miles/hr use factor 2.23694. SpeedUnitLabel Unit label for speed, default is m/s . Used in conjunction with SpeedUnitFactor.","title":"Settings"},{"location":"settings/#airsim-settings","text":"","title":"AirSim Settings"},{"location":"settings/#where-are-settings-stored","text":"Windows: Documents\\AirSim Linux: ~/Documents/AirSim The file is in usual json format . On first startup AirSim would create settings.json file with no settings. To avoid problems, always use ASCII format to save json file. A good usable example can also be found in the docs folder here .","title":"Where are Settings Stored?"},{"location":"settings/#how-to-chose-between-car-and-multirotor","text":"The default is to use multirotor. To use car simple set \"SimMode\": \"Car\" like this: { \"SettingsVersion\": 1.2, \"SimMode\": \"Car\" } To choose multirotor, set \"SimMode\": \"Multirotor\" . If you want to prompt user to select vehicle type then use \"SimMode\": \"\" .","title":"How to Chose Between Car and Multirotor?"},{"location":"settings/#available-settings-and-their-defaults","text":"Below are complete list of settings available along with their default values. If any of the settings is missing from json file, then default value is used. Some default values are simply specified as \"\" which means actual value may be chosen based on the vehicle you are using. For example, ViewMode setting has default value \"\" which translates to \"FlyWithMe\" for drones and \"SpringArmChase\" for cars. WARNING: Do not copy paste all of below in your settings.json. We strongly recommend adding only those settings that you don't want default values. Only required element is \"SettingsVersion\" . { \"SimMode\": \"\", \"ClockType\": \"\", \"ClockSpeed\": 1, \"LocalHostIp\": \"127.0.0.1\", \"ApiServerPort\": 41451, \"RecordUIVisible\": true, \"LogMessagesVisible\": true, \"ViewMode\": \"\", \"RpcEnabled\": true, \"EngineSound\": true, \"PhysicsEngineName\": \"\", \"SpeedUnitFactor\": 1.0, \"SpeedUnitLabel\": \"m/s\", \"Recording\": { \"RecordOnMove\": false, \"RecordInterval\": 0.05, \"Cameras\": [ { \"CameraName\": \"0\", \"ImageType\": 0, \"PixelsAsFloat\": false, \"Compress\": true } ] }, \"CameraDefaults\": { \"CaptureSettings\": [ { \"ImageType\": 0, \"Width\": 256, \"Height\": 144, \"FOV_Degrees\": 90, \"AutoExposureSpeed\": 100, \"AutoExposureBias\": 0, \"AutoExposureMaxBrightness\": 0.64, \"AutoExposureMinBrightness\": 0.03, \"MotionBlurAmount\": 0, \"TargetGamma\": 1.0, \"ProjectionMode\": \"\", \"OrthoWidth\": 5.12, \"MotionBlurAmount\": 1, \"MotionBlurMax\": 10, \"ChromaticAberrationScale\": 2, \"IgnoreMarked\": false } ], \"NoiseSettings\": [ { \"Enabled\": false, \"ImageType\": 0, \"RandContrib\": 0.2, \"RandSpeed\": 100000.0, \"RandSize\": 500.0, \"RandDensity\": 2, \"HorzWaveContrib\":0.03, \"HorzWaveStrength\": 0.08, \"HorzWaveVertSize\": 1.0, \"HorzWaveScreenSize\": 1.0, \"HorzNoiseLinesContrib\": 1.0, \"HorzNoiseLinesDensityY\": 0.01, \"HorzNoiseLinesDensityXY\": 0.5, \"HorzDistortionContrib\": 1.0, \"HorzDistortionStrength\": 0.002, \"LensDistortionEnable\": true, \"LensDistortionAreaFalloff\": 2, \"LensDistortionAreaRadius\": 1, \"LensDistortionInvert\": false } ], \"Gimbal\": { \"Stabilization\": 0, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN } \"X\": NaN, \"Y\": NaN, \"Z\": NaN, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN }, \"OriginGeopoint\": { \"Latitude\": 47.641468, \"Longitude\": -122.140165, \"Altitude\": 122 }, \"TimeOfDay\": { \"Enabled\": false, \"StartDateTime\": \"\", \"CelestialClockSpeed\": 1, \"StartDateTimeDst\": false, \"UpdateIntervalSecs\": 60 }, \"SubWindows\": [ {\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"Visible\": false}, {\"WindowID\": 1, \"CameraName\": \"0\", \"ImageType\": 5, \"Visible\": false}, {\"WindowID\": 2, \"CameraName\": \"0\", \"ImageType\": 0, \"Visible\": false} ], \"PawnPaths\": { \"BareboneCar\": {\"PawnBP\": \"Class'/AirSim/VehicleAdv/Vehicle/VehicleAdvPawn.VehicleAdvPawn_C'\"}, \"DefaultCar\": {\"PawnBP\": \"Class'/AirSim/VehicleAdv/SUV/SuvCarPawn.SuvCarPawn_C'\"}, \"DefaultQuadrotor\": {\"PawnBP\": \"Class'/AirSim/Blueprints/BP_FlyingPawn.BP_FlyingPawn_C'\"}, \"DefaultComputerVision\": {\"PawnBP\": \"Class'/AirSim/Blueprints/BP_ComputerVisionPawn.BP_ComputerVisionPawn_C'\"} }, \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"DefaultVehicleState\": \"Armed\", \"AutoCreate\": true, \"PawnPath\": \"\", \"EnableCollisionPassthrogh\": false, \"EnableCollisions\": true, \"AllowAPIAlways\": true, \"RC\": { \"RemoteControlID\": 0, \"AllowAPIWhenDisconnected\": false }, \"Cameras\": { //same elements as CameraDefaults above, key as name }, \"X\": NaN, \"Y\": NaN, \"Z\": NaN, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN }, \"PhysXCar\": { \"VehicleType\": \"PhysXCar\", \"DefaultVehicleState\": \"\", \"AutoCreate\": true, \"PawnPath\": \"\", \"EnableCollisionPassthrogh\": false, \"EnableCollisions\": true, \"RC\": { \"RemoteControlID\": -1 }, \"Cameras\": { \"MyCamera1\": { //same elements as elements inside CameraDefaults above }, \"MyCamera2\": { //same elements as elements inside CameraDefaults above }, }, \"X\": NaN, \"Y\": NaN, \"Z\": NaN, \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN } } }","title":"Available Settings and Their Defaults"},{"location":"settings/#simmode","text":"SimMode determines which simulation mode will be used. Below are currently supported values: - \"\" : prompt user to select vehicle type multirotor or car - \"Multirotor\" : Use multirotor simulation - \"Car\" : Use car simulation - \"ComputerVision\" : Use only camera, no vehicle or physics - \"SkidVehicle\" : use skid-steering vehicle simulation - \"UrdfBot\" : use URDF Bot simulation.","title":"SimMode"},{"location":"settings/#viewmode","text":"The ViewMode determines which camera to use as default and how camera will follow the vehicle. For multirotors, the default ViewMode is \"FlyWithMe\" while for cars the default ViewMode is \"SpringArmChase\" . FlyWithMe : Chase the vehicle from behind with 6 degrees of freedom GroundObserver : Chase the vehicle from 6' above the ground but with full freedom in XY plane. Fpv : View the scene from front camera of vehicle Manual : Don't move camera automatically. Use arrow keys and ASWD keys for move camera manually. SpringArmChase : Chase the vehicle with camera mounted on (invisible) arm that is attached to the vehicle via spring (so it has some latency in movement). NoDisplay : This will freeze rendering for main screen however rendering for subwindows, recording and APIs remain active. This mode is useful to save resources in \"headless\" mode where you are only interested in getting images and don't care about what gets rendered on main screen. This may also improve FPS for recording images.","title":"ViewMode"},{"location":"settings/#timeofday","text":"This setting controls the position of Sun in the environment. By default Enabled is false which means Sun's position is left at whatever was the default in the environment and it doesn't change over the time. If Enabled is true then Sun position is computed using longitude, latitude and altitude specified in OriginGeopoint section for the date specified in StartDateTime in the string format as %Y-%m-%d %H:%M:%S , for example, 2018-02-12 15:20:00 . If this string is empty then current date and time is used. If StartDateTimeDst is true then we adjust for day light savings time. The Sun's position is then continuously updated at the interval specified in UpdateIntervalSecs . In some cases, it might be desirable to have celestial clock run faster or slower than simulation clock. This can be specified using CelestialClockSpeed , for example, value 100 means for every 1 second of simulation clock, Sun's position is advanced by 100 seconds so Sun will move in sky much faster. Also see Time of Day API .","title":"TimeOfDay"},{"location":"settings/#origingeopoint","text":"This setting specifies the latitude, longitude and altitude of the Player Start component placed in the Unreal environment. The vehicle's home point is computed using this transformation. Note that all coordinates exposed via APIs are using NED system in SI units which means each vehicle starts at (0, 0, 0) in NED system. Time of Day settings are computed for geographical coordinates specified in OriginGeopoint .","title":"OriginGeopoint"},{"location":"settings/#subwindows","text":"This setting determines what is shown in each of 3 subwindows which are visible when you press 0 key. The WindowsID can be 0 to 2, CameraName is any available camera on the vehicle. ImageType integer value determines what kind of image gets shown according to ImageType enum . For example, for car vehicles below shows driver view, front bumper view and rear view as scene, depth and surface normals respectively. \"SubWindows\": [ {\"WindowID\": 0, \"ImageType\": 0, \"CameraName\": \"3\", \"Visible\": true}, {\"WindowID\": 1, \"ImageType\": 3, \"CameraName\": \"0\", \"Visible\": true}, {\"WindowID\": 2, \"ImageType\": 6, \"CameraName\": \"4\", \"Visible\": true} ]","title":"SubWindows"},{"location":"settings/#recording","text":"The recording feature allows you to record data such as position, orientation, velocity along with the captured image at specified intervals. You can start recording by pressing red Record button on lower right or the R key. The data is stored in the Documents\\AirSim folder, in a time stamped subfolder for each recording session, as tab separated file. RecordInterval : specifies minimal interval in seconds between capturing two images. RecordOnMove : specifies that do not record frame if there was vehicle's position or orientation hasn't changed. Cameras : this element controls which cameras are used to capture images. By default scene image from camera 0 is recorded as compressed png format. This setting is json array so you can specify multiple cameras to capture images, each with potentially different image types . When PixelsAsFloat is true, image is saved as pfm file instead of png file.","title":"Recording"},{"location":"settings/#clockspeed","text":"This setting allows you to set the speed of simulation clock with respect to wall clock. For example, value of 5.0 would mean simulation clock has 5 seconds elapsed when wall clock has 1 second elapsed (i.e. simulation is running faster). The value of 0.1 means that simulation clock is 10X slower than wall clock. The value of 1 means simulation is running in real time. It is important to realize that quality of simulation may decrease as the simulation clock runs faster. You might see artifacts like object moving past obstacles because collision is not detected. However slowing down simulation clock (i.e. values < 1.0) generally improves the quality of simulation.","title":"ClockSpeed"},{"location":"settings/#camera-settings","text":"The CameraDefaults element at root level specifies defaults used for all cameras. These defaults can be overridden for individual camera in Cameras element inside Vehicles as described later.","title":"Camera Settings"},{"location":"settings/#main-settings","text":"Like other sensors the pose of the sensor in the vehicle frame can be defined by X, Y, Z, Roll, Pitch, Yaw parameters. Furthermore there are some other settings available: * DrawSensor : Draw the physical sensor in the world on the vehicle with a 3D axes shown where the sensor is. * External : Uncouple the sensor from the vehicle. If enabled, the position and orientation will be relative to Unreal world coordinates. * ExternalLocal : When in external mode, if this is enabled the retrieved pose of the sensor will be in Local NED coordinates(from starting position from vehicle) and not converted Unreal NED coordinates which is default.","title":"Main settings"},{"location":"settings/#note-on-imagetype-element","text":"The ImageType element in JSON array determines which image type that settings applies to. The valid values are described in ImageType section . In addition, we also support special value ImageType: -1 to apply the settings to external camera (i.e. what you are looking at on the screen). For example, CaptureSettings element is json array so you can add settings for multiple image types easily.","title":"Note on ImageType element"},{"location":"settings/#capturesettings","text":"The CaptureSettings determines how different image types such as scene, depth, disparity, surface normals and segmentation views are rendered. The Width, Height and FOV settings should be self explanatory. The AutoExposureSpeed decides how fast eye adaptation works. We set to generally high value such as 100 to avoid artifacts in image capture. Similarly we set MotionBlurAmount to 0 by default to avoid artifacts in ground truth images. The ProjectionMode decides the projection used by the capture camera and can take value \"perspective\" (default) or \"orthographic\". If projection mode is \"orthographic\" then OrthoWidth determines width of projected area captured in meters. To disable the rendering of certain objects on specific cameras or all, use the IgnoreMarked boolean setting. This requires to mark individual objects that have to be ignore using an Unreal Tag called MarkedIgnore . You can also tweak the motion blur and chromatic Aberration here. For explanation of other settings, please see this article .","title":"CaptureSettings"},{"location":"settings/#noisesettings","text":"The NoiseSettings allows to add noise to the specified image type with a goal of simulating camera sensor noise, interference and other artifacts. By default no noise is added, i.e., Enabled: false . If you set Enabled: true then following different types of noise and interference artifacts are enabled, each can be further tuned using setting. The noise effects are implemented as shader created as post processing material in Unreal Engine called CameraSensorNoise . Demo of camera noise and interference simulation:","title":"NoiseSettings"},{"location":"settings/#random-noise","text":"This adds random noise blobs with following parameters. * RandContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * RandSpeed : This determines how fast noise fluctuates, 1 means no fluctuation and higher values like 1E6 means full fluctuation. * RandSize : This determines how coarse noise is, 1 means every pixel has its own noise while higher value means more than 1 pixels share same noise value. * RandDensity : This determines how many pixels out of total will have noise, 1 means all pixels while higher value means lesser number of pixels (exponentially).","title":"Random noise"},{"location":"settings/#horizontal-bump-distortion","text":"This adds horizontal bumps / flickering / ghosting effect. * HorzWaveContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * HorzWaveStrength : This determines overall strength of the effect. * HorzWaveVertSize : This determines how many vertical pixels would be effected by the effect. * HorzWaveScreenSize : This determines how much of the screen is effected by the effect.","title":"Horizontal bump distortion"},{"location":"settings/#horizontal-noise-lines","text":"This adds regions of noise on horizontal lines. * HorzNoiseLinesContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * HorzNoiseLinesDensityY : This determines how many pixels in horizontal line gets affected. * HorzNoiseLinesDensityXY : This determines how many lines on screen gets affected.","title":"Horizontal noise lines"},{"location":"settings/#horizontal-line-distortion","text":"This adds fluctuations on horizontal line. * HorzDistortionContrib : This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * HorzDistortionStrength : This determines how large is the distortion.","title":"Horizontal line distortion"},{"location":"settings/#radial-lens-distortion","text":"This adds radial lens distortion to the camera sensor. * LensDistortionEnable : Enable or disable this feature * LensDistortionAreaFalloff : The size of the area to distort * LensDistortionAreaRadius : The distortion radius * LensDistortionInvert : Set to true to invert and create 'pincushion distortion' or false for 'barrel distortion'","title":"Radial Lens Distortion"},{"location":"settings/#gimbal","text":"The Gimbal element allows to freeze camera orientation for pitch, roll and/or yaw. This setting is ignored unless ImageType is -1. The Stabilization is defaulted to 0 meaning no gimbal i.e. camera orientation changes with body orientation on all axis. The value of 1 means full stabilization. The value between 0 to 1 acts as a weight for fixed angles specified (in degrees, in world-frame) in Pitch , Roll and Yaw elements and orientation of the vehicle body. When any of the angles is omitted from json or set to NaN, that angle is not stabilized (i.e. it moves along with vehicle body).","title":"Gimbal"},{"location":"settings/#vehicles-settings","text":"Each simulation mode will go through the list of vehicles specified in this setting and create the ones that has \"AutoCreate\": true . Each vehicle specified in this setting has key which becomes the name of the vehicle. If \"Vehicles\" element is missing then this list is populated with default car named \"PhysXCar\" and default multirotor named \"SimpleFlight\".","title":"Vehicles Settings"},{"location":"settings/#common-vehicle-setting","text":"VehicleType : This could be either PhysXCar or BoxCar for the Car SimMode, SimpleFlight or PX4Multirotor for the MultiRotor SimMode, ComputerVision for the ComputerVision SimMode , CPHusky or Pioneer for SkidVehicle SimMode and UrdfBot for the UrdfBot SimMode. you can use There is no default value therefore this element must be specified. PawnPath : This allows to override the pawn blueprint to use for the vehicle. For example, you may create new pawn blueprint derived from ACarPawn for a warehouse robot in your own project outside the AirSim code and then specify its path here. See also PawnPaths . DefaultVehicleState : Possible value for multirotors is Armed or Disarmed . AutoCreate : If true then this vehicle would be spawned (if supported by selected sim mode). RC : This sub-element allows to specify which remote controller to use for vehicle using RemoteControlID . The value of -1 means use keyboard (not supported yet for multirotors). The value >= 0 specifies one of many remote controllers connected to the system. The list of available RCs can be seen in Game Controllers panel in Windows, for example. X, Y, Z, Yaw, Roll, Pitch : These elements allows you to specify the initial position and orientation of the vehicle. Position is in NED coordinates in SI units with origin set to Player Start location in Unreal environment. The orientation is specified in degrees. IsFpvVehicle : This setting allows to specify which vehicle camera will follow and the view that will be shown when ViewMode is set to Fpv. By default, AirSim selects the first vehicle in settings as FPV vehicle. Cameras : This element specifies camera settings for vehicle. The key in this element is name of the available camera and the value is same as CameraDefaults as described above. For example, to change FOV for the front center camera to 120 degrees, you can use this for Vehicles setting: \"Vehicles\": { \"FishEyeDrone\": { \"VehicleType\": \"SimpleFlight\", \"Cameras\": { \"front-center\": { \"CaptureSettings\": [ { \"ImageType\": 0, \"FOV_Degrees\": 120 } ] } } } }","title":"Common Vehicle Setting"},{"location":"settings/#using-px4","text":"By default we use simple_flight so you don't have to do separate HITL or SITL setups. We also support \"PX4\" for advanced users. To use PX4 with AirSim, you can use the following for Vehicles setting: \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", } }","title":"Using PX4"},{"location":"settings/#additional-px4-settings","text":"The defaults for PX4 is to enable hardware-in-loop setup. There are various other settings available for PX4 as follows with their default values: \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", \"LogViewerHostIp\": \"127.0.0.1\", \"LogViewerPort\": 14388, \"OffboardCompID\": 1, \"OffboardSysID\": 134, \"QgcHostIp\": \"127.0.0.1\", \"QgcPort\": 14550, \"SerialBaudRate\": 115200, \"SerialPort\": \"*\", \"SimCompID\": 42, \"SimSysID\": 142, \"SitlIp\": \"127.0.0.1\", \"SitlPort\": 14556, \"UdpIp\": \"127.0.0.1\", \"UdpPort\": 14560, \"UseSerial\": true, \"VehicleCompID\": 1, \"VehicleSysID\": 135, \"Model\": \"Generic\", \"LocalHostIp\": \"127.0.0.1\" } } These settings define the MavLink SystemId and ComponentId for the Simulator (SimSysID, SimCompID), and for an optional external renderer (ExtRendererSysID, ExtRendererCompID) and the node that allows remote control of the drone from another app this is called the Air Control node (AirControlSysID, AirControlCompID). If you want the simulator to also talk to your ground control app (like QGroundControl) you can also set the UDP address for that in case you want to run that on a different machine (QgcHostIp,QgcPort). You can connect the simulator to the LogViewer app, provided in this repo, by setting the UDP address for that (LogViewerHostIp,LogViewerPort). And for each flying drone added to the simulator there is a named block of additional settings. In the above you see the default name \"PX4\". You can change this name from the Unreal Editor when you add a new BP_FlyingPawn asset. You will see these properties grouped under the category \"MavLink\". The MavLink node for this pawn can be remote over UDP or it can be connected to a local serial port. If serial then set UseSerial to true, otherwise set UseSerial to false and set the appropriate bard rate. The default of 115200 works with Pixhawk version 2 over USB.","title":"Additional PX4 Settings"},{"location":"settings/#other-settings","text":"","title":"Other Settings"},{"location":"settings/#urdf","text":"For aal URDF related settings see the seperate documentation .","title":"URDF"},{"location":"settings/#enginesound","text":"To turn off the engine sound use setting \"EngineSound\": false . Currently this setting applies only to car.","title":"EngineSound"},{"location":"settings/#pawnpaths","text":"This allows you to specify your own vehicle pawn blueprints, for example, you can replace the default car in AirSim with your own car. Your vehicle BP can reside in Content folder of your own Unreal project (i.e. outside of AirSim plugin folder). For example, if you have a car BP located in file Content\\MyCar\\MySedanBP.uasset in your project then you can set \"DefaultCar\": {\"PawnBP\":\"Class'/Game/MyCar/MySedanBP.MySedanBP_C'\"} . The XYZ.XYZ_C is a special notation required to specify class for BP XYZ . Please note that your BP must be derived from CarPawn class. By default this is not the case but you can re-parent the BP using the \"Class Settings\" button in toolbar in UE editor after you open the BP and then choosing \"Car Pawn\" for Parent Class settings in Class Options. It's also a good idea to disable \"Auto Possess Player\" and \"Auto Possess AI\" as well as set AI Controller Class to None in BP details. Please make sure your asset is included for cooking in packaging options if you are creating binary.","title":"PawnPaths"},{"location":"settings/#physicsenginename","text":"For cars, we support only PhysX for now (regardless of value in this setting). For multirotors, we support \"FastPhysicsEngine\" only.","title":"PhysicsEngineName"},{"location":"settings/#localhostip-setting","text":"Now when connecting to remote machines you may need to pick a specific Ethernet adapter to reach those machines, for example, it might be over Ethernet or over Wi-Fi, or some other special virtual adapter or a VPN. Your PC may have multiple networks, and those networks might not be allowed to talk to each other, in which case the UDP messages from one network will not get through to the others. So the LocalHostIp allows you to configure how you are reaching those machines. The default of 127.0.0.1 is not able to reach external machines, this default is only used when everything you are talking to is contained on a single PC.","title":"LocalHostIp Setting"},{"location":"settings/#apiserverport","text":"This setting determines the server port that used by airsim clients, default port is 41451. By specifying different ports, the user can run multiple environments in parallel to accelerate data collection process.","title":"ApiServerPort"},{"location":"settings/#speedunitfactor","text":"Unit conversion factor for speed related to m/s , default is 1. Used in conjunction with SpeedUnitLabel. This may be only used for display purposes for example on-display speed when car is being driven. For example, to get speed in miles/hr use factor 2.23694.","title":"SpeedUnitFactor"},{"location":"settings/#speedunitlabel","text":"Unit label for speed, default is m/s . Used in conjunction with SpeedUnitFactor.","title":"SpeedUnitLabel"},{"location":"simple_flight/","text":"simple_flight If you don't know what flight controller does than see What is Flight Controller? . AirSim has built-in flight controller called simple_flight and it is used by default. You don't need to do anything to use or configure it. AirSim also supports PX4 as another flight controller for advanced users. In future, we also plan to support ROSFlight and Hackflight . Advantages The advantage of using simple_flight is zero additional setup you need to do and it \"just works\". Also, simple_flight uses steppable clock which means you can pause the simulation and things are not at mercy of high variance low precision clock that operating system provides. Further, simple_flight is simple, cross platform and 100% header-only dependency-free C++ code which means you can literally step through from simulator to inside flight controller code within same code base! Design Normally flight controllers are designed to run on actual hardware on vehicles and their support for running in simulator varies widely. They are often fairly difficult to configure for non-expert users and typically have complex build usually lacking cross platform support. All these problems have played significant part in design of simple_flight. simple_flight is designed from ground up as library with clean interface that can work onboard the vehicle as well as simulator. The core principle is that flight controller has no way to specify special simulation mode and there for it has no way to know if it is running under simulation or real vehicle. We thus view flight controller simply as collection of algorithms packaged in a library. Another key emphasis is to develop this code as dependency free header-only pure standard C++11 code. This means there is no special build required to compile simple_flight. You just copy its source code to any project you wish and it just works. Control simple_flight can control vehicle by taking in desired input as angle rate, angle level, velocity or position. Each axis of control can be specified with one of these modes. Internally simple_flight uses cascade of PID controllers to finally generate actuator signals. This means position PID drives velocity PID which drives angle level PID which finally drives angle rate PID. State Estimation In current release we are using ground truth from simulator for our state estimation. We plan to add complimentary filter based state estimation for angular velocity and orientation using 2 sensors (gyroscope, accelerometer) in near future. In more longer term, we plan to integrate another library to do velocity and position estimation using 4 sensors (gyroscope, accelerometer, magnetometer and barometer) using EKF. If you have experience this area than we encourage you to engage with us and contribute! Supported Boards Currently we have implemented simple_flight interfaces for simulated board. We plan to implement it for Pixhawk V2 board and possibly Naze32 board. We expect all our code to remain unchanged and the implementation would mainly involve adding drivers for various sensors, handling ISRs and managing other board specific details. If you have experience this area than we encourage you to engage with us and contribute! Configuration To have AirSim use simple_flight, you can specify it in settings.json as shown below. Note that this is default so you don't have to do it explicitly. \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", } } By default, vehicle using simple_flight is already armed which is why you would see propellers spinning. However if you don't want that than set DefaultVehicleState to Inactive like this: \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"DefaultVehicleState\": \"Inactive\" } } In this case, you will need to either manually arm using RC sticks in down inward position or using APIs. For safety reasons, flight controllers would disallow API control unless human operator has consented using a switch on RC. Also, when RC control is lost, vehicle should disable API control and enter hover mode for safety reasons. To simplify things a bit, simple_flight enables API control without human consent using RC and even when RC is not detected by default however you can change this using following setting: \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"AllowAPIAlways\": true, \"RC\": { \"RemoteControlID\": 0, \"AllowAPIWhenDisconnected\": true } } } Finally, simple_flight uses steppable clock by default which means clock advances when simulator tells it to advance (unlike wall clock which advances strictly according to passage of time). This means clock can be paused, for example, if code hits the break point and there is zero variance in clock (clock APIs provides by operating system might have significant variance unless its \"real time\" OS). If you want simple_flight to use wall clock instead than use following settings: \"ClockType\": \"ScalableClock\"","title":"Simple Flight"},{"location":"simple_flight/#simple_flight","text":"If you don't know what flight controller does than see What is Flight Controller? . AirSim has built-in flight controller called simple_flight and it is used by default. You don't need to do anything to use or configure it. AirSim also supports PX4 as another flight controller for advanced users. In future, we also plan to support ROSFlight and Hackflight .","title":"simple_flight"},{"location":"simple_flight/#advantages","text":"The advantage of using simple_flight is zero additional setup you need to do and it \"just works\". Also, simple_flight uses steppable clock which means you can pause the simulation and things are not at mercy of high variance low precision clock that operating system provides. Further, simple_flight is simple, cross platform and 100% header-only dependency-free C++ code which means you can literally step through from simulator to inside flight controller code within same code base!","title":"Advantages"},{"location":"simple_flight/#design","text":"Normally flight controllers are designed to run on actual hardware on vehicles and their support for running in simulator varies widely. They are often fairly difficult to configure for non-expert users and typically have complex build usually lacking cross platform support. All these problems have played significant part in design of simple_flight. simple_flight is designed from ground up as library with clean interface that can work onboard the vehicle as well as simulator. The core principle is that flight controller has no way to specify special simulation mode and there for it has no way to know if it is running under simulation or real vehicle. We thus view flight controller simply as collection of algorithms packaged in a library. Another key emphasis is to develop this code as dependency free header-only pure standard C++11 code. This means there is no special build required to compile simple_flight. You just copy its source code to any project you wish and it just works.","title":"Design"},{"location":"simple_flight/#control","text":"simple_flight can control vehicle by taking in desired input as angle rate, angle level, velocity or position. Each axis of control can be specified with one of these modes. Internally simple_flight uses cascade of PID controllers to finally generate actuator signals. This means position PID drives velocity PID which drives angle level PID which finally drives angle rate PID.","title":"Control"},{"location":"simple_flight/#state-estimation","text":"In current release we are using ground truth from simulator for our state estimation. We plan to add complimentary filter based state estimation for angular velocity and orientation using 2 sensors (gyroscope, accelerometer) in near future. In more longer term, we plan to integrate another library to do velocity and position estimation using 4 sensors (gyroscope, accelerometer, magnetometer and barometer) using EKF. If you have experience this area than we encourage you to engage with us and contribute!","title":"State Estimation"},{"location":"simple_flight/#supported-boards","text":"Currently we have implemented simple_flight interfaces for simulated board. We plan to implement it for Pixhawk V2 board and possibly Naze32 board. We expect all our code to remain unchanged and the implementation would mainly involve adding drivers for various sensors, handling ISRs and managing other board specific details. If you have experience this area than we encourage you to engage with us and contribute!","title":"Supported Boards"},{"location":"simple_flight/#configuration","text":"To have AirSim use simple_flight, you can specify it in settings.json as shown below. Note that this is default so you don't have to do it explicitly. \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", } } By default, vehicle using simple_flight is already armed which is why you would see propellers spinning. However if you don't want that than set DefaultVehicleState to Inactive like this: \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"DefaultVehicleState\": \"Inactive\" } } In this case, you will need to either manually arm using RC sticks in down inward position or using APIs. For safety reasons, flight controllers would disallow API control unless human operator has consented using a switch on RC. Also, when RC control is lost, vehicle should disable API control and enter hover mode for safety reasons. To simplify things a bit, simple_flight enables API control without human consent using RC and even when RC is not detected by default however you can change this using following setting: \"Vehicles\": { \"SimpleFlight\": { \"VehicleType\": \"SimpleFlight\", \"AllowAPIAlways\": true, \"RC\": { \"RemoteControlID\": 0, \"AllowAPIWhenDisconnected\": true } } } Finally, simple_flight uses steppable clock by default which means clock advances when simulator tells it to advance (unlike wall clock which advances strictly according to passage of time). This means clock can be paused, for example, if code hits the break point and there is zero variance in clock (clock APIs provides by operating system might have significant variance unless its \"real time\" OS). If you want simple_flight to use wall clock instead than use following settings: \"ClockType\": \"ScalableClock\"","title":"Configuration"},{"location":"skid_steer_vehicle/","text":"Unreal Skid Steering Vehicle Model For vehicles that can't use the normal WheeledVehicle setup with normal steering wheels and non-steering wheels, but that use the skid-steering/differential steering (like a tank) an alternative vehicle model was created. It is build using the NVIDIA PhysX Tank vehicle model. It is setup the same way as a WheeledVehicle and so can be easily be customized. http://www.robotplatform.com/knowledge/Classification_of_Robots/wheel_control_theory.html Creating a new skid steer vehicle The steps to setup the vehicle are largely the same as a WheeledVehicle with some slight adjustments. 1. Follow this guide to create the skeletal mesh, physics asset, wheel blueprint(s) and tire config data asset(s). 2. The animation blueprint is the exact same as in that tutorial however as class one should use the SkidVehicleAnimInstance sub-class. 3. For the vehicle blueprint to create the pawn it is also largely the same as in that tutorial however as class one should use the SkidVehicle sub-class. The vehicle setup parameters are more simplified. 4. For input relies on setting an X and Y( SetXJoy() and SetYJoy() functions, blueprint accessible!) value for a joystick. Hence it's always ideal to use a controller to steer the vehicle. The steering model is based on this guide with the following image as steering translation. Hence a X/Y input model is required. https://www.impulseadventure.com/elec/robot-differential-steering.html Skid steer model within AirSim The skid steer model is a separate SimMode within AirSim. It is fully implemented in similar fashion as the normal Car SimMode. There are already two vehicle types implemented, the ClearPath Husky and Pioneer P3DX robots. To configure the SimMode and vehicle type see the settings.json file documentation . If you create a new vehicle using the Unreal skid steering vehicle model as described above, one can use the PawnPaths setting in the Common Vehicle Settings in the settings.json file to link the custom vehicle pawn.","title":"Skid Steer Vehicles"},{"location":"skid_steer_vehicle/#unreal-skid-steering-vehicle-model","text":"For vehicles that can't use the normal WheeledVehicle setup with normal steering wheels and non-steering wheels, but that use the skid-steering/differential steering (like a tank) an alternative vehicle model was created. It is build using the NVIDIA PhysX Tank vehicle model. It is setup the same way as a WheeledVehicle and so can be easily be customized. http://www.robotplatform.com/knowledge/Classification_of_Robots/wheel_control_theory.html","title":"Unreal Skid Steering Vehicle Model"},{"location":"skid_steer_vehicle/#creating-a-new-skid-steer-vehicle","text":"The steps to setup the vehicle are largely the same as a WheeledVehicle with some slight adjustments. 1. Follow this guide to create the skeletal mesh, physics asset, wheel blueprint(s) and tire config data asset(s). 2. The animation blueprint is the exact same as in that tutorial however as class one should use the SkidVehicleAnimInstance sub-class. 3. For the vehicle blueprint to create the pawn it is also largely the same as in that tutorial however as class one should use the SkidVehicle sub-class. The vehicle setup parameters are more simplified. 4. For input relies on setting an X and Y( SetXJoy() and SetYJoy() functions, blueprint accessible!) value for a joystick. Hence it's always ideal to use a controller to steer the vehicle. The steering model is based on this guide with the following image as steering translation. Hence a X/Y input model is required. https://www.impulseadventure.com/elec/robot-differential-steering.html","title":"Creating a new skid steer vehicle"},{"location":"skid_steer_vehicle/#skid-steer-model-within-airsim","text":"The skid steer model is a separate SimMode within AirSim. It is fully implemented in similar fashion as the normal Car SimMode. There are already two vehicle types implemented, the ClearPath Husky and Pioneer P3DX robots. To configure the SimMode and vehicle type see the settings.json file documentation . If you create a new vehicle using the Unreal skid steering vehicle model as described above, one can use the PawnPaths setting in the Common Vehicle Settings in the settings.json file to link the custom vehicle pawn.","title":"Skid steer model within AirSim"},{"location":"steering_wheel_installation/","text":"Logitech G920 Steering Wheel Installation To use Logitech G920 steering wheel with AirSim follow these steps: Connect the steering wheel to the computer and wait until drivers installation complete. Install Logitech Gaming Software from here Before debug, you\u2019ll have to normalize the values in AirSim code. Perform this changes in CarPawn.cpp (according to the current update in the git): In line 382, change \u201cVal\u201d to \u201c1 \u2013 Val\u201d. (the complementary value in the range [0.0,1.0]). In line 388, change \u201cVal\u201d to \u201c5Val - 2.5\u201d (Change the range of the given input from [0.0,1.0] to [-1.0,1.0]). In line 404, change \u201cVal\u201d to \u201c4(1 \u2013 Val)\u201d. (the complementary value in the range [0.0,1.0]). Debug AirSim project (while the steering wheel is connected \u2013 it\u2019s important). On Unreal Editor, go to Edit->plugins->input devices and enable \u201cWindows RawInput\u201d. Go to Edit->Project Settings->Raw Input, and add new device configuration: Vendor ID: 0x046d (In case of Logitech G920, otherwise you might need to check it). Product ID: 0xc261 (In case of Logitech G920, otherwise you might need to check it). Under \u201cAxis Properties\u201d, make sure that \u201cGenericUSBController Axis 2\u201d, \u201cGenericUSBController Axis 4\u201d and \u201cGenericUSBController Axis 5\u201d are all enabled with an offset of 1.0. Explanation: axis 2 is responsible for steering movement, axis 4 is for brake and axis 5 is for gas. If you need to configure the clutch, it\u2019s on axis 3. Go to Edit->Project Settings->Input, Under Bindings in \u201cAxis Mappings\u201d: Remove existing mappings from the groups \u201cMoveRight\u201d and \u201cMoveForward\u201d. Add new axis mapping to the group \u201cMoveRight\u201d, use GenericUSBController axis 2 with a scale of 1.0. Add new axis mapping to the group \u201cMoveForward\u201d, use GenericUSBController axis 5 with a scale of 1.0. Add a new group of axis mappings, name it \u201cFootBrake\u201d and add new axis mapping to this group, use GenericUSBController axis 4 with a scale of 1.0. Play and drive ! Pay Attention Notice that in the first time we \"play\" after debug, we need to touch the wheel to \u201creset\u201d the values. Tip In the gaming software, you can configure buttons as keyboard shortcuts, we used it to configure a shortcut to record dataset or to play in full screen.","title":"Steering Wheel"},{"location":"steering_wheel_installation/#logitech-g920-steering-wheel-installation","text":"To use Logitech G920 steering wheel with AirSim follow these steps: Connect the steering wheel to the computer and wait until drivers installation complete. Install Logitech Gaming Software from here Before debug, you\u2019ll have to normalize the values in AirSim code. Perform this changes in CarPawn.cpp (according to the current update in the git): In line 382, change \u201cVal\u201d to \u201c1 \u2013 Val\u201d. (the complementary value in the range [0.0,1.0]). In line 388, change \u201cVal\u201d to \u201c5Val - 2.5\u201d (Change the range of the given input from [0.0,1.0] to [-1.0,1.0]). In line 404, change \u201cVal\u201d to \u201c4(1 \u2013 Val)\u201d. (the complementary value in the range [0.0,1.0]). Debug AirSim project (while the steering wheel is connected \u2013 it\u2019s important). On Unreal Editor, go to Edit->plugins->input devices and enable \u201cWindows RawInput\u201d. Go to Edit->Project Settings->Raw Input, and add new device configuration: Vendor ID: 0x046d (In case of Logitech G920, otherwise you might need to check it). Product ID: 0xc261 (In case of Logitech G920, otherwise you might need to check it). Under \u201cAxis Properties\u201d, make sure that \u201cGenericUSBController Axis 2\u201d, \u201cGenericUSBController Axis 4\u201d and \u201cGenericUSBController Axis 5\u201d are all enabled with an offset of 1.0. Explanation: axis 2 is responsible for steering movement, axis 4 is for brake and axis 5 is for gas. If you need to configure the clutch, it\u2019s on axis 3. Go to Edit->Project Settings->Input, Under Bindings in \u201cAxis Mappings\u201d: Remove existing mappings from the groups \u201cMoveRight\u201d and \u201cMoveForward\u201d. Add new axis mapping to the group \u201cMoveRight\u201d, use GenericUSBController axis 2 with a scale of 1.0. Add new axis mapping to the group \u201cMoveForward\u201d, use GenericUSBController axis 5 with a scale of 1.0. Add a new group of axis mappings, name it \u201cFootBrake\u201d and add new axis mapping to this group, use GenericUSBController axis 4 with a scale of 1.0. Play and drive !","title":"Logitech G920 Steering Wheel Installation"},{"location":"steering_wheel_installation/#pay-attention","text":"Notice that in the first time we \"play\" after debug, we need to touch the wheel to \u201creset\u201d the values.","title":"Pay Attention"},{"location":"steering_wheel_installation/#tip","text":"In the gaming software, you can configure buttons as keyboard shortcuts, we used it to configure a shortcut to record dataset or to play in full screen.","title":"Tip"},{"location":"unreal_blocks/","text":"Setup Blocks Environment for AirSim Blocks environment is available in repo in folder Unreal/Environments/Blocks and is designed to be lightweight in size. That means its very basic but fast. Here are quick steps to get Blocks environment up and running: Windows Make sure you have installed Unreal and built AirSim . Navigate to folder AirSim\\Unreal\\Environments\\Blocks and run update_from_git.bat . Double click on generated .sln file to open in Visual Studio. Make sure Blocks project is the startup project, build configuration is set to DebugGame_Editor and Win64 . Hit F5 to run. Press the Play button in Unreal Editor and you will see something like in below video. Also see how to use AirSim . Changing Code and Rebuilding For Windows, you can just change the code in Visual Studio, press F5 and re-run. There are few batch files available in folder AirSim\\Unreal\\Environments\\Blocks that lets you sync code, clean etc. Linux Make sure you have built the Unreal Engine and AirSim . Navigate to your UnrealEngine repo folder and run Engine/Binaries/Linux/UE4Editor which will start Unreal Editor. On first start you might not see any projects in UE4 editor. Click on Projects tab, Browse button and then navigate to AirSim/Unreal/Environments/Blocks/Blocks.uproject . If you get prompted for incompatible version and conversion, select In-place conversion which is usually under \"More\" options. If you get prompted for missing modules, make sure to select No so you don't exit. Finally, when prompted with building AirSim, select Yes. Now it might take a while so go get some coffee :). Press the Play button in Unreal Editor and you will see something like in below video. Also see how to use AirSim . Changing Code and Rebuilding For Linux, make code changes in AirLib or Unreal/Plugins folder and then run ./build.sh to rebuild. This step also copies the build output to Blocks sample project. You can then follow above steps again to re-run. Chosing Your Vehicle: Car or Multirotor By default AirSim spawns multirotor. You can easily change this to car and use all of AirSim goodies. Please see using car guide. FAQ I see warnings about like \"_BuitData\" file is missing. These are intermediate files and you can safely ignore it.","title":"Blocks Environment"},{"location":"unreal_blocks/#setup-blocks-environment-for-airsim","text":"Blocks environment is available in repo in folder Unreal/Environments/Blocks and is designed to be lightweight in size. That means its very basic but fast. Here are quick steps to get Blocks environment up and running:","title":"Setup Blocks Environment for AirSim"},{"location":"unreal_blocks/#windows","text":"Make sure you have installed Unreal and built AirSim . Navigate to folder AirSim\\Unreal\\Environments\\Blocks and run update_from_git.bat . Double click on generated .sln file to open in Visual Studio. Make sure Blocks project is the startup project, build configuration is set to DebugGame_Editor and Win64 . Hit F5 to run. Press the Play button in Unreal Editor and you will see something like in below video. Also see how to use AirSim .","title":"Windows"},{"location":"unreal_blocks/#changing-code-and-rebuilding","text":"For Windows, you can just change the code in Visual Studio, press F5 and re-run. There are few batch files available in folder AirSim\\Unreal\\Environments\\Blocks that lets you sync code, clean etc.","title":"Changing Code and Rebuilding"},{"location":"unreal_blocks/#linux","text":"Make sure you have built the Unreal Engine and AirSim . Navigate to your UnrealEngine repo folder and run Engine/Binaries/Linux/UE4Editor which will start Unreal Editor. On first start you might not see any projects in UE4 editor. Click on Projects tab, Browse button and then navigate to AirSim/Unreal/Environments/Blocks/Blocks.uproject . If you get prompted for incompatible version and conversion, select In-place conversion which is usually under \"More\" options. If you get prompted for missing modules, make sure to select No so you don't exit. Finally, when prompted with building AirSim, select Yes. Now it might take a while so go get some coffee :). Press the Play button in Unreal Editor and you will see something like in below video. Also see how to use AirSim .","title":"Linux"},{"location":"unreal_blocks/#changing-code-and-rebuilding_1","text":"For Linux, make code changes in AirLib or Unreal/Plugins folder and then run ./build.sh to rebuild. This step also copies the build output to Blocks sample project. You can then follow above steps again to re-run.","title":"Changing Code and Rebuilding"},{"location":"unreal_blocks/#chosing-your-vehicle-car-or-multirotor","text":"By default AirSim spawns multirotor. You can easily change this to car and use all of AirSim goodies. Please see using car guide.","title":"Chosing Your Vehicle: Car or Multirotor"},{"location":"unreal_blocks/#faq","text":"","title":"FAQ"},{"location":"unreal_blocks/#i-see-warnings-about-like-_buitdata-file-is-missing","text":"These are intermediate files and you can safely ignore it.","title":"I see warnings about like \"_BuitData\" file is missing."},{"location":"unreal_custenv/","text":"Creating and Setting Up Unreal Environment This page contains the complete instructions start to finish for setting up Unreal environment with AirSim. The Unreal Marketplace has several environment available that you can start using in just few minutes. It is also possible to use environments available on websites such as turbosquid.com or cgitrader.com with bit more effort (here's tutorial video ). In addition there also several free environments available. Below we will use a freely downloadable environment from Unreal Marketplace called Landscape Mountain but the steps are same for any other environments. You can also view these steps performed in Unreal AirSim Setup Video . Note for Linux Users There is no Epic Games Launcher for Linux which means that if you need to create custom environment, you will need Windows machine to do that. Once you have Unreal project folder, just copy it over to your Linux machine. Step by Step Instructions Make sure AirSim is built and Unreal 4.24.4 (Cosys-Lab fork) is installed as described in build instructions . In Epic Games Launcher click the Learn tab then scroll down and find Landscape Mountains . Click the Create Project and download this content (~2GB download). Open LandscapeMountains.uproject , it should launch the Unreal Editor. From the File menu select New C++ class , leave default None on the type of class, click Next , leave default name MyClass , and click Create Class . We need to do this because Unreal requires at least one source file in project. It should trigger compile and open up Visual Studio solution LandscapeMountains.sln . Go to your folder for AirSim repo and copy Unreal\\Plugins folder in to your LandscapeMountains folder. This way now your own Unreal project has AirSim plugin. Edit the LandscapeMountains.uproject so that it looks like this { \"FileVersion\": 3, \"EngineAssociation\": \"4.24.4\", \"Category\": \"Samples\", \"Description\": \"\", \"Modules\": [ { \"Name\": \"LandscapeMountains\", \"Type\": \"Runtime\", \"LoadingPhase\": \"Default\", \"AdditionalDependencies\": [ \"AirSim\" ] } ], \"TargetPlatforms\": [ \"MacNoEditor\", \"WindowsNoEditor\" ], \"Plugins\": [ { \"Name\": \"AirSim\", \"Enabled\": true } ] } Close Visual Studio and the Unreal Editor and right click the LandscapeMountains.uproject in Windows Explorer and select Generate Visual Studio Project Files . This step detects all plugins and source files in your Unreal project and generates .sln file for Visual Studio. Tip: If the Generate Visual Studio Project Files option is missing you may need to reboot your machine for the Unreal Shell extensions to take effect. If it is still missing then open the LandscapeMountains.uproject in the Unreal Editor and select Refresh Visual Studio Project from the File menu. Reopen LandscapeMountains.sln in Visual Studio, and make sure \"DebugGame Editor\" and \"Win64\" build configuration is the active build configuration. Press F5 to run . This will start the Unreal Editor. The Unreal Editor allows you to edit the environment, assets and other game related settings. First thing you want to do in your environment is set up PlayerStart object. In Landscape Mountains environment, PlayerStart object already exist and you can find it in the World Outliner . Make sure its location is setup as shown. This is where AirSim plugin will create and place the vehicle. If its too high up then vehicle will fall down as soon as you press play giving potentially random behavior In Window/World Settings as shown below, set the GameMode Override to AirSimGameMode : Go to 'Edit->Editor Preferences' in Unreal Editor, in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. If you don't do this then UE will be slowed down dramatically when UE window loses focus. Be sure to Save these edits. Hit the Play button in the Unreal Editor. See how to use AirSim . Congratulations! You are now running AirSim in your own Unreal environment. Choosing Your Vehicle: Car or Multirotor By default AirSim prompts user for which vehicle to use. You can easily change this by setting SimMode . Please see using car guide. Updating Your Environment to Latest Version of AirSim Once you have your environment using above instructions, you should frequently update your local AirSim code to latest version from GitHub. Below are the instructions to do this: First put clean.bat (or clean.sh for Linux users) in the root folder of your environment. Run this file to clean up all intermediate files in your Unreal project. Do git pull in your AirSim repo followed by build.cmd (or ./build.sh for Linux users). Replace [your project]/Plugins folder with AirSim/Unreal/Plugins folder. Right click on your .uproject file and chose \"Generate Visual Studio project files\" option. This is not required for Linux. FAQ What are other cool environments? Unreal Marketplace has dozens of prebuilt extra-ordinarily detailed environments ranging from Moon to Mars and everything in between. The one we have used for testing is called Modular Neighborhood Pack but you can use any environment. Another free environment is Infinity Blade series . Alternatively, if you look under the Learn tab in Epic Game Launcher, you will find many free samples that you can use. One of our favorites is \"A Boy and His Kite\" which is a 100 square miles of highly detailed environment (caution: you will need very beefy PC to run it!). When I press Play button some kind of video starts instead of my vehicle. If the environment comes with MatineeActor, delete it to avoid any startup demo sequences. There might be other ways to remove it as well, for example, click on Blueprints button, then Level Blueprint and then look at Begin Play event in Event Graph. You might want to disconnect any connections that may be starting \"matinee\". Is there easy way to sync code in my Unreal project with code in AirSim repo? Sure, there is! You can find bunch of .bat files (for linux, .sh ) in AirSim\\Unreal\\Environments\\Blocks . Just copy them over to your own Unreal project. Most of these are quite simple and self explanatory. I get some error about map. You might have to set default map for your project. For example, if you are using Modular Neighborhood Pack, set the Editor Starter Map as well as Game Default Map to Demo_Map in Project Settings > Maps & Modes. I see \"Add to project\" option for environment but not \"Create project\" option. In this case, create a new blank C++ project with no Starter Content and add your environment in to it. I already have my own Unreal project. How do I use AirSim with it? Copy the Unreal\\Plugins folder from the build you did in the above section into the root of your Unreal project's folder. In your Unreal project's .uproject file, add the key AdditionalDependencies to the \"Modules\" object as we showed in the LandscapeMountains.uproject above. \"AdditionalDependencies\": [ \"AirSim\" ] and the Plugins section to the top level object: \"Plugins\": [ { \"Name\": \"AirSim\", \"Enabled\": true } ]","title":"Custom Unreal Environment"},{"location":"unreal_custenv/#creating-and-setting-up-unreal-environment","text":"This page contains the complete instructions start to finish for setting up Unreal environment with AirSim. The Unreal Marketplace has several environment available that you can start using in just few minutes. It is also possible to use environments available on websites such as turbosquid.com or cgitrader.com with bit more effort (here's tutorial video ). In addition there also several free environments available. Below we will use a freely downloadable environment from Unreal Marketplace called Landscape Mountain but the steps are same for any other environments. You can also view these steps performed in Unreal AirSim Setup Video .","title":"Creating and Setting Up Unreal Environment"},{"location":"unreal_custenv/#note-for-linux-users","text":"There is no Epic Games Launcher for Linux which means that if you need to create custom environment, you will need Windows machine to do that. Once you have Unreal project folder, just copy it over to your Linux machine.","title":"Note for Linux Users"},{"location":"unreal_custenv/#step-by-step-instructions","text":"Make sure AirSim is built and Unreal 4.24.4 (Cosys-Lab fork) is installed as described in build instructions . In Epic Games Launcher click the Learn tab then scroll down and find Landscape Mountains . Click the Create Project and download this content (~2GB download). Open LandscapeMountains.uproject , it should launch the Unreal Editor. From the File menu select New C++ class , leave default None on the type of class, click Next , leave default name MyClass , and click Create Class . We need to do this because Unreal requires at least one source file in project. It should trigger compile and open up Visual Studio solution LandscapeMountains.sln . Go to your folder for AirSim repo and copy Unreal\\Plugins folder in to your LandscapeMountains folder. This way now your own Unreal project has AirSim plugin. Edit the LandscapeMountains.uproject so that it looks like this { \"FileVersion\": 3, \"EngineAssociation\": \"4.24.4\", \"Category\": \"Samples\", \"Description\": \"\", \"Modules\": [ { \"Name\": \"LandscapeMountains\", \"Type\": \"Runtime\", \"LoadingPhase\": \"Default\", \"AdditionalDependencies\": [ \"AirSim\" ] } ], \"TargetPlatforms\": [ \"MacNoEditor\", \"WindowsNoEditor\" ], \"Plugins\": [ { \"Name\": \"AirSim\", \"Enabled\": true } ] } Close Visual Studio and the Unreal Editor and right click the LandscapeMountains.uproject in Windows Explorer and select Generate Visual Studio Project Files . This step detects all plugins and source files in your Unreal project and generates .sln file for Visual Studio. Tip: If the Generate Visual Studio Project Files option is missing you may need to reboot your machine for the Unreal Shell extensions to take effect. If it is still missing then open the LandscapeMountains.uproject in the Unreal Editor and select Refresh Visual Studio Project from the File menu. Reopen LandscapeMountains.sln in Visual Studio, and make sure \"DebugGame Editor\" and \"Win64\" build configuration is the active build configuration. Press F5 to run . This will start the Unreal Editor. The Unreal Editor allows you to edit the environment, assets and other game related settings. First thing you want to do in your environment is set up PlayerStart object. In Landscape Mountains environment, PlayerStart object already exist and you can find it in the World Outliner . Make sure its location is setup as shown. This is where AirSim plugin will create and place the vehicle. If its too high up then vehicle will fall down as soon as you press play giving potentially random behavior In Window/World Settings as shown below, set the GameMode Override to AirSimGameMode : Go to 'Edit->Editor Preferences' in Unreal Editor, in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. If you don't do this then UE will be slowed down dramatically when UE window loses focus. Be sure to Save these edits. Hit the Play button in the Unreal Editor. See how to use AirSim . Congratulations! You are now running AirSim in your own Unreal environment.","title":"Step by Step Instructions"},{"location":"unreal_custenv/#choosing-your-vehicle-car-or-multirotor","text":"By default AirSim prompts user for which vehicle to use. You can easily change this by setting SimMode . Please see using car guide.","title":"Choosing Your Vehicle: Car or Multirotor"},{"location":"unreal_custenv/#updating-your-environment-to-latest-version-of-airsim","text":"Once you have your environment using above instructions, you should frequently update your local AirSim code to latest version from GitHub. Below are the instructions to do this: First put clean.bat (or clean.sh for Linux users) in the root folder of your environment. Run this file to clean up all intermediate files in your Unreal project. Do git pull in your AirSim repo followed by build.cmd (or ./build.sh for Linux users). Replace [your project]/Plugins folder with AirSim/Unreal/Plugins folder. Right click on your .uproject file and chose \"Generate Visual Studio project files\" option. This is not required for Linux.","title":"Updating Your Environment to Latest Version of AirSim"},{"location":"unreal_custenv/#faq","text":"","title":"FAQ"},{"location":"unreal_custenv/#what-are-other-cool-environments","text":"Unreal Marketplace has dozens of prebuilt extra-ordinarily detailed environments ranging from Moon to Mars and everything in between. The one we have used for testing is called Modular Neighborhood Pack but you can use any environment. Another free environment is Infinity Blade series . Alternatively, if you look under the Learn tab in Epic Game Launcher, you will find many free samples that you can use. One of our favorites is \"A Boy and His Kite\" which is a 100 square miles of highly detailed environment (caution: you will need very beefy PC to run it!).","title":"What are other cool environments?"},{"location":"unreal_custenv/#when-i-press-play-button-some-kind-of-video-starts-instead-of-my-vehicle","text":"If the environment comes with MatineeActor, delete it to avoid any startup demo sequences. There might be other ways to remove it as well, for example, click on Blueprints button, then Level Blueprint and then look at Begin Play event in Event Graph. You might want to disconnect any connections that may be starting \"matinee\".","title":"When I press Play button some kind of video starts instead of my vehicle."},{"location":"unreal_custenv/#is-there-easy-way-to-sync-code-in-my-unreal-project-with-code-in-airsim-repo","text":"Sure, there is! You can find bunch of .bat files (for linux, .sh ) in AirSim\\Unreal\\Environments\\Blocks . Just copy them over to your own Unreal project. Most of these are quite simple and self explanatory.","title":"Is there easy way to sync code in my Unreal project with code in AirSim repo?"},{"location":"unreal_custenv/#i-get-some-error-about-map","text":"You might have to set default map for your project. For example, if you are using Modular Neighborhood Pack, set the Editor Starter Map as well as Game Default Map to Demo_Map in Project Settings > Maps & Modes.","title":"I get some error about map."},{"location":"unreal_custenv/#i-see-add-to-project-option-for-environment-but-not-create-project-option","text":"In this case, create a new blank C++ project with no Starter Content and add your environment in to it.","title":"I see \"Add to project\" option for environment but not \"Create project\" option."},{"location":"unreal_custenv/#i-already-have-my-own-unreal-project-how-do-i-use-airsim-with-it","text":"Copy the Unreal\\Plugins folder from the build you did in the above section into the root of your Unreal project's folder. In your Unreal project's .uproject file, add the key AdditionalDependencies to the \"Modules\" object as we showed in the LandscapeMountains.uproject above. \"AdditionalDependencies\": [ \"AirSim\" ] and the Plugins section to the top level object: \"Plugins\": [ { \"Name\": \"AirSim\", \"Enabled\": true } ]","title":"I already have my own Unreal project. How do I use AirSim with it?"},{"location":"unreal_proj/","text":"Unreal Environment Setting Up the Unreal Project Option 1: Built-in Blocks Environment To get up and running fast, you can use the Blocks project that already comes with AirSim. This is not very highly detailed environment to keep the repo size reasonable but we use it for various testing all the times and it is the easiest way to get your feet wet in this strange land. Follow these quick steps . Option 2: Create Your Own Unreal Environment If you want to setup photo-realistic high quality environments, then you will need to create your own Unreal project. This is little bit more involved but worthwhile! Follow this step-by-step guide . Changing Code and Development Workflow To see how you can change and test AirSim code, please read our recommended development workflow .","title":"Setting up Unreal Environment"},{"location":"unreal_proj/#unreal-environment","text":"","title":"Unreal Environment"},{"location":"unreal_proj/#setting-up-the-unreal-project","text":"","title":"Setting Up the Unreal Project"},{"location":"unreal_proj/#option-1-built-in-blocks-environment","text":"To get up and running fast, you can use the Blocks project that already comes with AirSim. This is not very highly detailed environment to keep the repo size reasonable but we use it for various testing all the times and it is the easiest way to get your feet wet in this strange land. Follow these quick steps .","title":"Option 1: Built-in Blocks Environment"},{"location":"unreal_proj/#option-2-create-your-own-unreal-environment","text":"If you want to setup photo-realistic high quality environments, then you will need to create your own Unreal project. This is little bit more involved but worthwhile! Follow this step-by-step guide .","title":"Option 2: Create Your Own Unreal Environment"},{"location":"unreal_proj/#changing-code-and-development-workflow","text":"To see how you can change and test AirSim code, please read our recommended development workflow .","title":"Changing Code and Development Workflow"},{"location":"using_car/","text":"How to Use Car in AirSim By default AirSim prompts user for which vehicle to use. You can easily change this by setting SimMode . For example, if you want to use car instead then just set the SimMode in your settings.json which you can find in your ~/Documents/AirSim folder, like this: { \"SettingsVersion\": 1.2, \"SimMode\": \"Car\" } Now when you restart AirSim, you should see the car spawned automatically. Manual Driving Please use the keyboard arrow keys to drive manually. Spacebar for the handbrake. In manual drive mode, gears are set in \"auto\". Using APIs You can control the car, get state and images by calling APIs in variety of client languages including C++ and Python. Please see APIs doc for more details. Changing Views By default camera will chase the car from the back. You can get the FPV view by pressing F key and switch back to chasing from back view by pressing / key. More keyboard shortcuts can be seen by pressing F1. Cameras By default car is installed with 5 cameras: center, left and right, driver and reverse. You can chose the images from these camera by specifying the name .","title":"Car Mode"},{"location":"using_car/#how-to-use-car-in-airsim","text":"By default AirSim prompts user for which vehicle to use. You can easily change this by setting SimMode . For example, if you want to use car instead then just set the SimMode in your settings.json which you can find in your ~/Documents/AirSim folder, like this: { \"SettingsVersion\": 1.2, \"SimMode\": \"Car\" } Now when you restart AirSim, you should see the car spawned automatically.","title":"How to Use Car in AirSim"},{"location":"using_car/#manual-driving","text":"Please use the keyboard arrow keys to drive manually. Spacebar for the handbrake. In manual drive mode, gears are set in \"auto\".","title":"Manual Driving"},{"location":"using_car/#using-apis","text":"You can control the car, get state and images by calling APIs in variety of client languages including C++ and Python. Please see APIs doc for more details.","title":"Using APIs"},{"location":"using_car/#changing-views","text":"By default camera will chase the car from the back. You can get the FPV view by pressing F key and switch back to chasing from back view by pressing / key. More keyboard shortcuts can be seen by pressing F1.","title":"Changing Views"},{"location":"using_car/#cameras","text":"By default car is installed with 5 cameras: center, left and right, driver and reverse. You can chose the images from these camera by specifying the name .","title":"Cameras"},{"location":"who_is_using/","text":"Who is Using AirSim? Would you like to see your own group or project here? Just add a GitHub issue with quick details and link to your website or paper or send us email at msrair at microsoft.com. NASA Ames Research Center \u2013 Systems Analysis Office Astrobotic GRASP Lab, Univ of Pennsylvania Department of Aeronautics and Astronautics, Stanford University Formula Technion Ghent University ICARUS UC, Santa Barbara WISE Lab, Univ of Waterloo HAMS project, MSR India Washington and Lee University University of Oklahoma Robotics Institute, Carnegie Mellon University Texas A&M Robotics and Perception Group, University of Zurich National University of Ireland, Galway (NUIG) Soda Mobility Technologies","title":"Who is Using AirSim"},{"location":"who_is_using/#who-is-using-airsim","text":"","title":"Who is Using AirSim?"},{"location":"who_is_using/#would-you-like-to-see-your-own-group-or-project-here","text":"Just add a GitHub issue with quick details and link to your website or paper or send us email at msrair at microsoft.com. NASA Ames Research Center \u2013 Systems Analysis Office Astrobotic GRASP Lab, Univ of Pennsylvania Department of Aeronautics and Astronautics, Stanford University Formula Technion Ghent University ICARUS UC, Santa Barbara WISE Lab, Univ of Waterloo HAMS project, MSR India Washington and Lee University University of Oklahoma Robotics Institute, Carnegie Mellon University Texas A&M Robotics and Perception Group, University of Zurich National University of Ireland, Galway (NUIG) Soda Mobility Technologies","title":"Would you like to see your own group or project here?"},{"location":"working_with_plugin_contents/","text":"How to use plugin contents Plugin contents are not shown in Unreal projects by default. To view plugin content, you need to click on few semi-hidden buttons: Causion Changes you make in content folder are changes to binary files so be careful.","title":"Working with UE Plugin Contents"},{"location":"working_with_plugin_contents/#how-to-use-plugin-contents","text":"Plugin contents are not shown in Unreal projects by default. To view plugin content, you need to click on few semi-hidden buttons: Causion Changes you make in content folder are changes to binary files so be careful.","title":"How to use plugin contents"},{"location":"xbox_controller/","text":"XBox Controller To use an XBox controller with AirSim follow these steps: Connect XBox controller so it shows up in your PC Game Controllers: Launch QGroundControl and you should see a new Joystick tab under stettings: Now calibrate the radio, and setup some handy button actions. For example, I set mine so that the 'A' button arms the drone, 'B' put it in manual flight mode, 'X' puts it in altitude hold mode and 'Y' puts it in position hold mode. I also prefer the feel of the controller when I check the box labelled \"Use exponential curve on roll,pitch, yaw\" because this gives me more sensitivity for small movements.] QGroundControl will find your Pixhawk via the UDP proxy port 14550 setup by MavLinkTest above. AirSim will find your Pixhawk via the other UDP server port 14570 also setup by MavLinkTest above. You can also use all the QGroundControl controls for autonomous flying at this point too. Connect to Pixhawk serial port using MavLinkTest.exe like this: MavLinkTest.exe -serial:*,115200 -proxy:127.0.0.1:14550 -server:127.0.0.1:14570 Run AirSim Unreal simulator with these ~/Documents/AirSim/settings.json settings: \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", \"SitlIp\": \"\", \"SitlPort\": 14560, \"UdpIp\": \"127.0.0.1\", \"UdpPort\": 14570, \"UseSerial\": false } } Advanced If the Joystick tab doesn't show up in QGroundControl then Click on the purple \"Q\" icon on left in tool bar to reveal the Preferences panel. Go to General tab and check the Virtual Joystick checkbox. Go back to settings screen (gears icon), click on Parameters tab, type COM_RC_IN_MODE in search box and change its value to either Joystick/No RC Checks or Virtual RC by Joystick . Other Options See remote controller options","title":"XBox Controller"},{"location":"xbox_controller/#xbox-controller","text":"To use an XBox controller with AirSim follow these steps: Connect XBox controller so it shows up in your PC Game Controllers: Launch QGroundControl and you should see a new Joystick tab under stettings: Now calibrate the radio, and setup some handy button actions. For example, I set mine so that the 'A' button arms the drone, 'B' put it in manual flight mode, 'X' puts it in altitude hold mode and 'Y' puts it in position hold mode. I also prefer the feel of the controller when I check the box labelled \"Use exponential curve on roll,pitch, yaw\" because this gives me more sensitivity for small movements.] QGroundControl will find your Pixhawk via the UDP proxy port 14550 setup by MavLinkTest above. AirSim will find your Pixhawk via the other UDP server port 14570 also setup by MavLinkTest above. You can also use all the QGroundControl controls for autonomous flying at this point too. Connect to Pixhawk serial port using MavLinkTest.exe like this: MavLinkTest.exe -serial:*,115200 -proxy:127.0.0.1:14550 -server:127.0.0.1:14570 Run AirSim Unreal simulator with these ~/Documents/AirSim/settings.json settings: \"Vehicles\": { \"PX4\": { \"VehicleType\": \"PX4Multirotor\", \"SitlIp\": \"\", \"SitlPort\": 14560, \"UdpIp\": \"127.0.0.1\", \"UdpPort\": 14570, \"UseSerial\": false } }","title":"XBox Controller"},{"location":"xbox_controller/#advanced","text":"If the Joystick tab doesn't show up in QGroundControl then Click on the purple \"Q\" icon on left in tool bar to reveal the Preferences panel. Go to General tab and check the Virtual Joystick checkbox. Go back to settings screen (gears icon), click on Parameters tab, type COM_RC_IN_MODE in search box and change its value to either Joystick/No RC Checks or Virtual RC by Joystick .","title":"Advanced"},{"location":"xbox_controller/#other-options","text":"See remote controller options","title":"Other Options"}]}